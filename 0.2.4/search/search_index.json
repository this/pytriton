{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pytriton","title":"PyTriton","text":"<p>PyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. The library allows serving Machine Learning models directly from Python through NVIDIA's Triton Inference Server.</p>"},{"location":"#how-it-works","title":"How it works?","text":"<p>In PyTriton, as in Flask or FastAPI, you can define any Python function that executes a machine learning model prediction and exposes it through an HTTP/gRPC API. PyTriton installs Triton Inference Server in your environment and uses it for handling HTTP/gRPC requests and responses. Our library provides a Python API that allows attaching a Python function to Triton and a communication layer to send/receive data between Triton and the function. This solution helps utilize the performance features of Triton Inference Server, such as dynamic batching or response cache, without changing your model environment. Thus, it improves the performance of running inference on GPU for models implemented in Python. The solution is framework-agnostic and can be used along with frameworks like PyTorch, TensorFlow, or JAX.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The diagram below presents the schema of how the Python models are served through Triton Inference Server using PyTriton. The solution consists of two main components:</p> <ul> <li>Triton Inference Server: for exposing the HTTP/gRPC API and benefiting from performance features like dynamic batching or response cache.</li> <li>Python Model Environment: your environment where the Python model is executed.</li> </ul> <p>The Triton Inference Server binaries are provided as part of the PyTriton installation. The Triton Server is installed in your current environment (system or container). The PyTriton controls the Triton Server process through the <code>Triton Controller</code>.</p> <p>Exposing the model through PyTriton requires the definition of an <code>Inference Callable</code> - a Python function that is connected to Triton Inference Server and executes the model or ensemble for predictions. The integration layer binds the <code>Inference Callable</code> to Triton Server and exposes it through the Triton HTTP/gRPC API under a provided <code>&lt;model name&gt;</code>. Once the integration is done, the defined <code>Inference Callable</code> receives data sent to the HTTP/gRPC API endpoint <code>v2/models/&lt;model name&gt;/infer</code>. Read more about HTTP/gRPC interface in Triton Inference Server documentation.</p> <p>The HTTP/gRPC requests sent to <code>v2/models/&lt;model name&gt;/infer</code> are handled by Triton Inference Server. The server batches requests and passes them to the <code>Proxy Backend</code>, which sends the batched requests to the appropriate <code>Inference Callable</code>. The data is sent as a <code>numpy</code> array. Once the <code>Inference Callable</code> finishes execution of the model prediction, the result is returned to the <code>Proxy Backend</code>, and a response is created by Triton Server.</p> <p></p>"},{"location":"#serving-the-models","title":"Serving the models","text":"<p>PyTriton provides an option to serve your Python model using Triton Inference Server to handle HTTP/gRPC requests and pass the input/output tensors to and from the model. We use a blocking mode where the application is a long-lived process deployed in your cluster to serve the requests from clients.</p> <p>Before you run the model for serving the inference callback function, it has to be defined. The inference callback receives the inputs and should return the model outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n@batch\ndef infer_fn(**inputs: np.ndarray):\ninput1, input2 = inputs.values()\noutputs = model(input1, input2)\nreturn [outputs]\n</code></pre> <p>The <code>infer_fn</code> receives the batched input data for the model and should return the batched outputs.</p> <p>In the next step, you need to create a connection between Triton and the model. For that purpose, the <code>Triton</code> class has to be used, and the <code>bind</code> method is required to be called to create a dedicated connection between Triton Inference Server and the defined <code>infer_fn</code>.</p> <p>In the blocking mode, we suggest using the <code>Triton</code> object as a context manager where multiple models can be loaded in the way presented below:</p> <pre><code>from pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"MyModel\",\ninfer_func=infer_fn,\ninputs=[\nTensor(dtype=bytes, shape=(1,)),  # sample containing single bytes value\nTensor(dtype=bytes, shape=(-1,)),  # sample containing vector of bytes\n],\noutputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\nconfig=ModelConfig(max_batch_size=16),\n)\n</code></pre> <p>At this point, you have defined how the model has to be handled by Triton and where the HTTP/gRPC requests for the model have to be directed. The last part for serving the model is to call the <code>serve</code> method on the Triton object:</p> <pre><code>with Triton() as triton:\n# ...\ntriton.serve()\n</code></pre> <p>When the <code>.serve()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p>"},{"location":"#working-in-the-jupyter-notebook","title":"Working in the Jupyter Notebook","text":"<p>The package provides an option to work with your model inside the Jupyter Notebook. We call it a background mode where the model is deployed on Triton Inference Server for handling HTTP/gRPC requests, but there are other actions that you want to perform after loading and starting serving the model.</p> <p>Having the <code>infer_fn</code> defined in the same way as described in the serving the models section, you can use the <code>Triton</code> object without a context:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n</code></pre> <p>In the next step, the model has to be loaded for serving in Triton Inference Server (which is also the same as in the serving example):</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\n@batch\ndef infer_fn(**inputs: np.ndarray):\ninput1, input2 = inputs.values()\noutputs = input1 + input2\nreturn [outputs]\ntriton.bind(\nmodel_name=\"MyModel\",\ninfer_func=infer_fn,\ninputs=[\nTensor(shape=(1,), dtype=np.float32),\nTensor(shape=(-1,), dtype=np.float32),\n],\noutputs=[Tensor(shape=(-1,), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=16),\n)\n</code></pre> <p>Finally, to run the model in background mode, use the <code>run</code> method:</p> <pre><code>triton.run()\n</code></pre> <p>When the <code>.run()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p> <p>The Triton server can be stopped at any time using the <code>stop</code> method:</p> <pre><code>triton.stop()\n</code></pre>"},{"location":"#what-next","title":"What next?","text":"<p>Read more about using PyTriton in the Quick Start, Examples and find more options on how to configure Triton, models, and deployment on a cluster in the Deploying Models section.</p> <p>The details about classes and methods can be found in the API Reference page.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#024-2023-08-10","title":"0.2.4 (2023-08-10)","text":"<ul> <li>Introduced <code>strict</code> flag in <code>Triton.bind</code> which enables data types and shapes validation of inference callable outputs   against model config</li> <li>Added <code>AsyncioModelClient</code> which works in FastAPI and other async frameworks</li> <li> <p>Fixed <code>FuturesModelClient</code> to prevent raising <code>gevent.exceptions.InvalidThreadUseError</code></p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#023-2023-07-21","title":"0.2.3 (2023-07-21)","text":"<ul> <li>Improved verification of Proxy Backend environment when running under same Python interpreter</li> <li> <p>Fixed pytriton.version to represent currently installed version</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#022-2023-07-19","title":"0.2.2 (2023-07-19)","text":"<ul> <li>Added <code>inference_timeout_s</code> parameters to client classes</li> <li>Renamed <code>PyTritonClientUrlParseError</code> to <code>PyTritonClientInvalidUrlError</code></li> <li><code>ModelClient</code> and <code>FuturesModelClient</code> methods raise <code>PyTritonClientClosedError</code> when used after client is closed</li> <li>Pinned tritonclient dependency due to issues with tritonclient &gt;= 2.34 on systems with glibc version lower than 2.34</li> <li> <p>Added warning after Triton Server setup and teardown while using too verbose logging level as it may cause a significant performance drop in model inference</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#021-2023-06-28","title":"0.2.1 (2023-06-28)","text":"<ul> <li>Fixed handling <code>TritonConfig.cache_directory</code> option - the directory was always overwritten with the default value.</li> <li>Fixed tritonclient dependency - PyTriton need tritonclient supporting http headers and parameters</li> <li> <p>Improved shared memory usage to match 64MB limit (default value for Docker, Kubernetes) reducing the initial size for PyTriton Proxy Backend.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#020-2023-05-30","title":"0.2.0 (2023-05-30)","text":"<ul> <li>Added support for using custom HTTP/gRPC request headers and parameters.</li> </ul> <p>This change breaks backward compatibility of the inference function signature.   The undecorated inference function now accepts a list of <code>Request</code> instances instead   of a list of dictionaries. The <code>Request</code> class contains data for inputs and parameters   for combined parameters and headers.</p> <p>See docs/custom_params.md for further information</p> <ul> <li>Added <code>FuturesModelClient</code> which enables sending inference requests in a parallel manner.</li> <li> <p>Added displaying documentation link after models are loaded.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#015-2023-05-12","title":"0.1.5 (2023-05-12)","text":"<ul> <li>Improved <code>pytriton.decorators.group_by_values</code> function</li> <li>Modified the function to avoid calling the inference callable on each individual sample when grouping by string/bytes input</li> <li>Added <code>pad_fn</code> argument for easy padding and combining of the inference results</li> <li>Fixed Triton binaries search</li> <li> <p>Improved Workspace management (remove workspace on shutdown)</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#014-2023-03-16","title":"0.1.4 (2023-03-16)","text":"<ul> <li>Add validation of the model name passed to Triton bind method.</li> <li> <p>Add monkey patching of <code>InferenceServerClient.__del__</code> method to prevent unhandled exceptions.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#013-2023-02-20","title":"0.1.3 (2023-02-20)","text":"<ul> <li> <p>Fixed getting model config in <code>fill_optionals</code> decorator.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#012-2023-02-14","title":"0.1.2 (2023-02-14)","text":"<ul> <li>Fixed wheel build to support installations on operating systems with glibc version 2.31 or higher.</li> <li>Updated the documentation on custom builds of the package.</li> <li>Change: TritonContext instance is shared across bound models and contains model_configs dictionary.</li> <li> <p>Fixed support of binding multiple models that uses methods of the same class.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#011-2023-01-31","title":"0.1.1 (2023-01-31)","text":"<ul> <li>Change: The <code>@first_value</code> decorator has been updated with new features:</li> <li>Renamed from <code>@first_values</code> to <code>@first_value</code></li> <li>Added a <code>strict</code> flag to toggle the checking of equality of values on a single selected input of the request. Default is True</li> <li>Added a <code>squeeze_single_values</code> flag to toggle the squeezing of single value ND arrays to scalars. Default is True</li> <li>Fix: <code>@fill_optionals</code> now supports non-batching models</li> <li>Fix: <code>@first_value</code> fixed to work with optional inputs</li> <li>Fix: <code>@group_by_values</code> fixed to work with string inputs</li> <li> <p>Fix: <code>@group_by_values</code> fixed to work per sample-wise</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#010-2023-01-12","title":"0.1.0 (2023-01-12)","text":"<ul> <li> <p>Initial release of PyTriton</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/pytriton/issues.</p> <p>If you are reporting a bug, include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The PyTriton could always use more documentation, whether as part of the official PyTriton docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/pytriton/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, you simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes: <pre><code>$ git commit -s -m \"Add cool feature.\n</code></pre></p> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the below:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up the <code>PyTriton</code> for local development.</p> <ol> <li>Fork the <code>PyTriton</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/pytriton.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development:</li> </ol> <pre><code>$ mkvirtualenv pytriton\n$ cd pytriton/\n</code></pre> <p>If you do not use virtualenvwrapper package, you can initialize virtual environment using pure python command:</p> <p><pre><code>$ python -m venv pytriton\n$ cd pytriton/\n$ source bin/activate\n</code></pre> Once the virtualenv has been activated install the development dependencies:</p> <pre><code>$ make install-dev\n</code></pre> <ol> <li>Extract Triton Server to your environment so you are able to debug PyTriton while serving some models on Triton:</li> </ol> <pre><code>$ make extract-triton\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ make lint  # will run i.a. flake8 and pytype linters\n$ make test  # will run a test with on your current virtualenv\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#tips","title":"Tips","text":"<p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_subset\n</code></pre>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in Google Style Guide.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>PyTriton requires that all contributors (or their corporate entity) send a signed copy of the Contributor License Agreement to triton-cla@nvidia.com.</p> <p>NOTE: Contributors with no company affiliation can fill <code>N/A</code> in the <code>Corporation Name</code> and <code>Corporation Address</code> fields.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#api-reference","title":"API Reference","text":""},{"location":"api/#pytriton.triton.TritonConfig","title":"<code>pytriton.triton.TritonConfig</code>  <code>dataclass</code>","text":"<p>Triton Inference Server configuration class for customization of server execution.</p> <p>The arguments are optional. If value is not provided the defaults for Triton Inference Server are used. Please, refer to https://github.com/triton-inference-server/server/ for more details.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[str]</code> <p>Identifier for this server.</p> <code>None</code> <code>log_verbose</code> <code>Optional[int]</code> <p>Set verbose logging level. Zero (0) disables verbose logging and values &gt;= 1 enable verbose logging.</p> <code>None</code> <code>log_file</code> <code>Optional[pathlib.Path]</code> <p>Set the name of the log output file.</p> <code>None</code> <code>exit_timeout_secs</code> <code>Optional[int]</code> <p>Timeout (in seconds) when exiting to wait for in-flight inferences to finish.</p> <code>None</code> <code>exit_on_error</code> <code>Optional[bool]</code> <p>Exit the inference server if an error occurs during initialization.</p> <code>None</code> <code>strict_readiness</code> <code>Optional[bool]</code> <p>If true /v2/health/ready endpoint indicates ready if the server is responsive and all models are available.</p> <code>None</code> <code>allow_http</code> <code>Optional[bool]</code> <p>Allow the server to listen for HTTP requests.</p> <code>None</code> <code>http_address</code> <code>Optional[str]</code> <p>The address for the http server to bind to. Default is 0.0.0.0.</p> <code>None</code> <code>http_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for HTTP requests. Default is 8000.</p> <code>None</code> <code>http_header_forward_pattern</code> <code>Optional[str]</code> <p>The regular expression pattern that will be used for forwarding HTTP headers as inference request parameters.</p> <code>None</code> <code>http_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling HTTP requests.</p> <code>None</code> <code>allow_grpc</code> <code>Optional[bool]</code> <p>Allow the server to listen for GRPC requests.</p> <code>None</code> <code>grpc_address</code> <code>Optional[str]</code> <p>The address for the grpc server to binds to. Default is 0.0.0.0.</p> <code>None</code> <code>grpc_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for GRPC requests. Default is 8001.</p> <code>None</code> <code>grpc_header_forward_pattern</code> <code>Optional[str]</code> <p>The regular expression pattern that will be used for forwarding GRPC headers as inference request parameters.</p> <code>None</code> <code>grpc_infer_allocation_pool_size</code> <code>Optional[int]</code> <p>The maximum number of inference request/response objects that remain allocated for reuse. As long as the number of in-flight requests doesn't exceed this value there will be no allocation/deallocation of request/response objects.</p> <code>None</code> <code>grpc_use_ssl</code> <code>Optional[bool]</code> <p>Use SSL authentication for GRPC requests. Default is false.</p> <code>None</code> <code>grpc_use_ssl_mutual</code> <code>Optional[bool]</code> <p>Use mututal SSL authentication for GRPC requests. This option will preempt grpc_use_ssl if it is also specified. Default is false.</p> <code>None</code> <code>grpc_server_cert</code> <code>Optional[pathlib.Path]</code> <p>File holding PEM-encoded server certificate. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_server_key</code> <code>Optional[pathlib.Path]</code> <p>Path to file holding PEM-encoded server key. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_root_cert</code> <code>Optional[pathlib.Path]</code> <p>Path to file holding PEM-encoded root certificate. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_infer_response_compression_level</code> <code>Optional[str]</code> <p>The compression level to be used while returning the inference response to the peer. Allowed values are none, low, medium and high. Default is none.</p> <code>None</code> <code>grpc_keepalive_time</code> <code>Optional[int]</code> <p>The period (in milliseconds) after which a keepalive ping is sent on the transport.</p> <code>None</code> <code>grpc_keepalive_timeout</code> <code>Optional[int]</code> <p>The period (in milliseconds) the sender of the keepalive ping waits for an acknowledgement.</p> <code>None</code> <code>grpc_keepalive_permit_without_calls</code> <code>Optional[bool]</code> <p>Allows keepalive pings to be sent even if there are no calls in flight</p> <code>None</code> <code>grpc_http2_max_pings_without_data</code> <code>Optional[int]</code> <p>The maximum number of pings that can be sent when there is no data/header frame to be sent.</p> <code>None</code> <code>grpc_http2_min_recv_ping_interval_without_data</code> <code>Optional[int]</code> <p>If there are no data/header frames being sent on the transport, this channel argument on the server side controls the minimum time (in milliseconds) that gRPC Core would expect between receiving successive pings.</p> <code>None</code> <code>grpc_http2_max_ping_strikes</code> <code>Optional[int]</code> <p>Maximum number of bad pings that the server will tolerate before sending an HTTP2 GOAWAY frame and closing the transport.</p> <code>None</code> <code>grpc_restricted_protocol</code> <p>Specify restricted GRPC protocol setting. The format of this flag is ,=. Where  is a comma-separated list of protocols to be restricted.  will be additional header key to be checked when a GRPC request is received, and  is the value expected to be matched. required <code>allow_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide prometheus metrics.</p> <code>None</code> <code>allow_gpu_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide GPU metrics.</p> <code>None</code> <code>allow_cpu_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide CPU metrics.</p> <code>None</code> <code>metrics_interval_ms</code> <code>Optional[int]</code> <p>Metrics will be collected once every  milliseconds. <code>None</code> <code>metrics_port</code> <code>Optional[int]</code> <p>The port reporting prometheus metrics.</p> <code>None</code> <code>metrics_address</code> <code>Optional[str]</code> <p>The address for the metrics server to bind to. Default is the same as http_address.</p> <code>None</code> <code>allow_sagemaker</code> <code>Optional[bool]</code> <p>Allow the server to listen for Sagemaker requests.</p> <code>None</code> <code>sagemaker_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for Sagemaker requests.</p> <code>None</code> <code>sagemaker_safe_port_range</code> <code>Optional[str]</code> <p>Set the allowed port range for endpoints other than the SageMaker endpoints.</p> <code>None</code> <code>sagemaker_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling Sagemaker requests.</p> <code>None</code> <code>allow_vertex_ai</code> <code>Optional[bool]</code> <p>Allow the server to listen for Vertex AI requests.</p> <code>None</code> <code>vertex_ai_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for Vertex AI requests.</p> <code>None</code> <code>vertex_ai_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling Vertex AI requests.</p> <code>None</code> <code>vertex_ai_default_model</code> <code>Optional[str]</code> <p>The name of the model to use for single-model inference requests.</p> <code>None</code> <code>metrics_config</code> <code>Optional[List[str]]</code> <p>Specify a metrics-specific configuration setting. The format of this flag is =. It can be specified multiple times <code>None</code> <code>trace_config</code> <code>Optional[List[str]]</code> <p>Specify global or trace mode specific configuration setting. The format of this flag is ,=. Where  is either 'triton' or 'opentelemetry'. The default is 'triton'. To specify global trace settings (level, rate, count, or mode), the format would be =. For 'triton' mode, the server will use Triton's Trace APIs. For 'opentelemetry' mode, the server will use OpenTelemetry's APIs to generate, collect and export traces for individual inference requests. <code>None</code> <code>cache_config</code> <code>Optional[List[str]]</code> <p>Specify a cache-specific configuration setting. The format of this flag is ,=. Where  is the name of the cache, such as 'local' or 'redis'. Example: local,size=1048576 will configure a 'local' cache implementation with a fixed buffer pool of size 1048576 bytes. <code>None</code> <code>cache_directory</code> <code>Optional[str]</code> <p>The global directory searched for cache shared libraries. Default is '/opt/tritonserver/caches'. This directory is expected to contain a cache implementation as a shared library with the name 'libtritoncache.so'.</p> <code>None</code> <code>buffer_manager_thread_count</code> <code>Optional[int]</code> <p>The number of threads used to accelerate copies and other operations required to manage input and output tensor contents.</p> <code>None</code>"},{"location":"api/#pytriton.triton.TritonConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration for early error handling.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate configuration for early error handling.\"\"\"\nif self.allow_http not in [True, None] and self.allow_grpc not in [True, None]:\nraise PyTritonValidationError(\"The `http` or `grpc` endpoint has to be allowed.\")\n</code></pre>"},{"location":"api/#pytriton.triton.TritonConfig.from_env","title":"<code>from_env()</code>  <code>classmethod</code>","text":"<p>Creates TritonConfig from environment variables.</p> <p>Environment variables should start with <code>PYTRITON_TRITON_CONFIG_</code> prefix. For example:</p> <pre><code>PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\nPYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n</code></pre> Typical use <p>triton_config = TritonConfig.from_env()</p> <p>Returns:</p> Type Description <code>TritonConfig</code> <p>TritonConfig class instantiated from environment variables.</p> Source code in <code>pytriton/triton.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; \"TritonConfig\":\n\"\"\"Creates TritonConfig from environment variables.\n    Environment variables should start with `PYTRITON_TRITON_CONFIG_` prefix. For example:\n        PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\n        PYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n    Typical use:\n        triton_config = TritonConfig.from_env()\n    Returns:\n        TritonConfig class instantiated from environment variables.\n    \"\"\"\nprefix = \"PYTRITON_TRITON_CONFIG_\"\nconfig = {name[len(prefix) :].lower(): value for name, value in os.environ.items() if name.startswith(prefix)}\nfields: Dict[str, dataclasses.Field] = {field.name: field for field in dataclasses.fields(cls)}\nunknown_config_parameters = {name: value for name, value in config.items() if name not in fields}\nfor name, value in unknown_config_parameters.items():\nLOGGER.warning(\nf\"Ignoring {name}={value} as could not find matching config field. \"\nf\"Available fields: {', '.join(map(str, fields))}\"\n)\ndef _cast_value(_field, _value):\nfield_type = _field.type\nis_optional = typing_inspect.is_optional_type(field_type)\nif is_optional:\nfield_type = field_type.__args__[0]\nreturn field_type(_value)\nconfig_with_casted_values = {\nname: _cast_value(fields[name], value) for name, value in config.items() if name in fields\n}\nreturn cls(**config_with_casted_values)\n</code></pre>"},{"location":"api/#pytriton.triton.TritonConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Map config object to dictionary.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def to_dict(self):\n\"\"\"Map config object to dictionary.\"\"\"\nreturn dataclasses.asdict(self)\n</code></pre>"},{"location":"api/#pytriton.decorators","title":"<code>pytriton.decorators</code>","text":"<p>Inference callable decorators.</p>"},{"location":"api/#pytriton.decorators.ConstantPadder","title":"<code>ConstantPadder(pad_value=0)</code>","text":"<p>Padder that pads the given batches with a constant value.</p> <p>Initialize the padder.</p> <p>Parameters:</p> Name Type Description Default <code>pad_value</code> <code>int</code> <p>Padding value. Defaults to 0.</p> <code>0</code> Source code in <code>pytriton/decorators.py</code> <pre><code>def __init__(self, pad_value=0):\n\"\"\"Initialize the padder.\n    Args:\n        pad_value (int, optional): Padding value. Defaults to 0.\n    \"\"\"\nself.pad_value = pad_value\n</code></pre>"},{"location":"api/#pytriton.decorators.ConstantPadder.__call__","title":"<code>__call__(batches_list)</code>","text":"<p>Pad the given batches with the specified value to pad size enabling further batching to single arrays.</p> <p>Parameters:</p> Name Type Description Default <code>batches_list</code> <code>List[Dict[str, np.ndarray]]</code> <p>List of batches to pad.</p> required <p>Returns:</p> Type Description <code>InferenceResults</code> <p>List[Dict[str, np.ndarray]]: List of padded batches.</p> <p>Raises:</p> Type Description <code>PyTritonRuntimeError</code> <p>If the input arrays for a given input name have different dtypes.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __call__(self, batches_list: InferenceResults) -&gt; InferenceResults:\n\"\"\"Pad the given batches with the specified value to pad size enabling further batching to single arrays.\n    Args:\n        batches_list (List[Dict[str, np.ndarray]]): List of batches to pad.\n    Returns:\n        List[Dict[str, np.ndarray]]: List of padded batches.\n    Raises:\n        PyTritonRuntimeError: If the input arrays for a given input name have different dtypes.\n    \"\"\"\ndef _get_padded_shape(_batches: List[np.ndarray]) -&gt; Tuple[int, ...]:\n\"\"\"Get the shape of the padded array without batch axis.\"\"\"\nreturn tuple(np.max([batch.shape[1:] for batch in _batches if batch is not None], axis=0))\ndef _get_padded_dtype(_batches: List[np.ndarray]) -&gt; np.dtype:\ndtypes = [batch.dtype for batch in _batches if batch is not None]\nresult_dtype = dtypes[0]\nif not all(dtype.kind == result_dtype.kind for dtype in dtypes):\nraise PyTritonRuntimeError(\"All input arrays for given input name must have the same dtype.\")\n# for bytes (encoded string) or unicode string need to obtain the max length\nif result_dtype.kind in \"SU\":\norder_and_kind = result_dtype.str[:2]\nmax_len = max([int(dtype.str[2:]) for dtype in dtypes])\nresult_dtype = f\"{order_and_kind}{max_len}\"\nelse:\nif not all(dtype == result_dtype for dtype in dtypes):\nraise PyTritonRuntimeError(\"All input arrays for given input name must have the same dtype.\")\nreturn np.dtype(result_dtype)\ninput_names = list(\ncollections.OrderedDict.fromkeys(input_name for batch in batches_list for input_name in batch.keys())\n)\nbatches_by_name = {input_name: [batch.get(input_name) for batch in batches_list] for input_name in input_names}\nfor input_batches in batches_by_name.values():\nresult_shape, result_dtype = _get_padded_shape(input_batches), _get_padded_dtype(input_batches)\nfor batch_idx, batch in enumerate(input_batches):\nif batch is not None:\ninput_batches[batch_idx] = np.pad(\nbatch,\n[(0, 0)] + [(0, b - a) for a, b in zip(batch.shape[1:], result_shape)],\nmode=\"constant\",\nconstant_values=self.pad_value if result_dtype.kind not in [\"S\", \"U\", \"O\"] else b\"\",\n).astype(result_dtype)\nreturn [\n{name: batches[batch_idx] for name, batches in batches_by_name.items() if batches[batch_idx] is not None}\nfor batch_idx in range(len(batches_list))\n]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict","title":"<code>ModelConfigDict()</code>","text":"<p>             Bases: <code>MutableMapping</code></p> <p>Dictionary for storing model configs for inference callable.</p> <p>Create ModelConfigDict object.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __init__(self):\n\"\"\"Create ModelConfigDict object.\"\"\"\nself._data: Dict[str, TritonModelConfig] = {}\nself._keys: List[Callable] = []\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__delitem__","title":"<code>__delitem__(infer_callable)</code>","text":"<p>Delete model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __delitem__(self, infer_callable: Callable):\n\"\"\"Delete model config for inference callable.\"\"\"\nkey = self._get_model_config_key(infer_callable)\ndel self._data[key]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__getitem__","title":"<code>__getitem__(infer_callable)</code>","text":"<p>Get model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __getitem__(self, infer_callable: Callable) -&gt; TritonModelConfig:\n\"\"\"Get model config for inference callable.\"\"\"\nkey = self._get_model_config_key(infer_callable)\nreturn self._data[key]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterate over inference callable keys.\"\"\"\nreturn iter(self._keys)\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__len__","title":"<code>__len__()</code>","text":"<p>Get number of inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __len__(self):\n\"\"\"Get number of inference callable keys.\"\"\"\nreturn len(self._data)\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__setitem__","title":"<code>__setitem__(infer_callable, item)</code>","text":"<p>Set model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __setitem__(self, infer_callable: Callable, item: TritonModelConfig):\n\"\"\"Set model config for inference callable.\"\"\"\nself._keys.append(infer_callable)\nkey = self._get_model_config_key(infer_callable)\nself._data[key] = item\n</code></pre>"},{"location":"api/#pytriton.decorators.TritonContext","title":"<code>TritonContext</code>  <code>dataclass</code>","text":"<p>Triton context definition class.</p>"},{"location":"api/#pytriton.decorators.batch","title":"<code>batch(wrapped, instance, args, kwargs)</code>","text":"<p>Decorator for converting list of request dicts to dict of input batches.</p> <p>Converts list of request dicts to dict of input batches. It passes **kwargs to inference callable where each named input contains numpy array with batch of requests received by Triton server. We assume that each request has the same set of keys (you can use group_by_keys decorator before using @batch decorator if your requests may have different set of keys).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef batch(wrapped, instance, args, kwargs):\n\"\"\"Decorator for converting list of request dicts to dict of input batches.\n    Converts list of request dicts to dict of input batches.\n    It passes **kwargs to inference callable where each named input contains numpy array with batch of requests\n    received by Triton server.\n    We assume that each request has the same set of keys (you can use group_by_keys decorator before\n    using @batch decorator if your requests may have different set of keys).\n    \"\"\"\nreq_list = args[0]\ninput_names = req_list[0].keys()\nfor req_dict2 in req_list[1:]:\nif input_names != req_dict2.keys():\nraise PyTritonValidationError(\"Cannot batch requests with different set of inputs keys\")\ninputs = {}\nfor model_input in input_names:\nconcatenated_input_data = np.concatenate([req[model_input] for req in req_list])\ninputs[model_input] = concatenated_input_data\nargs = args[1:]\nnew_kwargs = dict(kwargs)\nnew_kwargs.update(inputs)\noutputs = wrapped(*args, **new_kwargs)\noutputs = convert_output(outputs, wrapped, instance)\noutput_names = outputs.keys()\nout_list = []\nstart_idx = 0\nfor request in req_list:\n# get batch_size of first input for each request - assume that all inputs have same batch_size\nfirst_input = next(iter(request.values()))\nrequest_batch_size = first_input.shape[0]\nreq_output_dict = {}\nfor _output_ind, output_name in enumerate(output_names):\nreq_output = outputs[output_name][start_idx : start_idx + request_batch_size, ...]\nreq_output_dict[output_name] = req_output\nout_list.append(req_output_dict)\nstart_idx += request_batch_size\nreturn out_list\n</code></pre>"},{"location":"api/#pytriton.decorators.convert_output","title":"<code>convert_output(outputs, wrapped=None, instance=None, model_config=None)</code>","text":"<p>Converts output from tuple ot list to dictionary.</p> <p>It is utility function useful for mapping output list into dictionary of outputs. Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs instead of dictionary if this list matches output list in model config (size and order).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def convert_output(\noutputs: Union[Dict, List, Tuple], wrapped=None, instance=None, model_config: Optional[TritonModelConfig] = None\n):\n\"\"\"Converts output from tuple ot list to dictionary.\n    It is utility function useful for mapping output list into dictionary of outputs.\n    Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs\n    instead of dictionary if this list matches output list in model config (size and order).\n    \"\"\"\nif isinstance(outputs, dict):\nreturn outputs\nelif isinstance(outputs, (list, tuple)):\nif model_config is None:\nmodel_config = get_model_config(wrapped, instance)\nif len(outputs) != len(model_config.outputs):\nraise PyTritonValidationError(\"Outputs length different than config outputs length\")\noutputs = {config_output.name: output for config_output, output in zip(model_config.outputs, outputs)}\nreturn outputs\nelse:\nraise PyTritonValidationError(f\"Unsupported output type {type(outputs)}.\")\n</code></pre>"},{"location":"api/#pytriton.decorators.fill_optionals","title":"<code>fill_optionals(**defaults)</code>","text":"<p>This decorator ensures that any missing inputs in requests are filled with default values specified by the user.</p> <p>Default values should be NumPy arrays without batch axis.</p> <p>If you plan to group requests ex. with @group_by_keys or @group_by_vales decorators provide default values for optional parameters at the beginning of decorators stack. The other decorators can then group requests into bigger batches resulting in a better model performance.</p> Typical use <p>@fill_optionals() @group_by_keys() @batch def infer_fun(**inputs):     ...     return outputs</p> <p>Parameters:</p> Name Type Description Default <code>defaults</code> <p>keyword arguments containing default values for missing inputs</p> <code>{}</code> <p>If you have default values for some optional parameter it is good idea to provide them at the very beginning, so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def fill_optionals(**defaults):\n\"\"\"This decorator ensures that any missing inputs in requests are filled with default values specified by the user.\n    Default values should be NumPy arrays without batch axis.\n    If you plan to group requests ex. with\n    [@group_by_keys][pytriton.decorators.group_by_keys] or\n    [@group_by_vales][pytriton.decorators.group_by_values] decorators\n    provide default values for optional parameters at the beginning of decorators stack.\n    The other decorators can then group requests into bigger batches resulting in a better model performance.\n    Typical use:\n        @fill_optionals()\n        @group_by_keys()\n        @batch\n        def infer_fun(**inputs):\n            ...\n            return outputs\n    Args:\n        defaults: keyword arguments containing default values for missing inputs\n    If you have default values for some optional parameter it is good idea to provide them at the very beginning,\n    so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.\n    \"\"\"\ndef _verify_defaults(model_config: TritonModelConfig):\ninputs = {spec.name: spec for spec in model_config.inputs}\nnot_matching_default_names = sorted(set(defaults) - set(inputs))\nif not_matching_default_names:\nraise PyTritonBadParameterError(f\"Could not found {', '.join(not_matching_default_names)} inputs\")\nnon_numpy_items = {k: v for k, v in defaults.items() if not isinstance(v, np.ndarray)}\nif non_numpy_items:\nraise PyTritonBadParameterError(\nf\"Could not use {', '.join([f'{k}={v}' for k, v in non_numpy_items.items()])} defaults \"\n\"as they are not NumPy arrays\"\n)\nnot_matching_dtypes = {k: (v.dtype, inputs[k].dtype) for k, v in defaults.items() if v.dtype != inputs[k].dtype}\nif not_matching_dtypes:\nnon_matching_dtypes_str_list = [\nf\"{name}: dtype={have_dtype} expected_dtype={expected_dtype}\"\nfor name, (have_dtype, expected_dtype) in not_matching_dtypes.items()\n]\nraise PyTritonBadParameterError(\nf\"Could not use {', '.join(non_matching_dtypes_str_list)} \"\nf\"defaults as they have different than input signature dtypes\"\n)\ndef _shape_match(_have_shape, _expected_shape):\nreturn len(_have_shape) == len(_expected_shape) and all(\ne == -1 or h == e for h, e in zip(_have_shape, _expected_shape)\n)\nnot_matching_shapes = {\nk: (v.shape, inputs[k].shape) for k, v in defaults.items() if not _shape_match(v.shape, inputs[k].shape)\n}\nif not_matching_shapes:\nnon_matching_shapes_str_list = [\nf\"{name}: shape={have_shape} expected_shape={expected_shape}\"\nfor name, (have_shape, expected_shape) in not_matching_shapes.items()\n]\nraise PyTritonBadParameterError(\nf\"Could not use {', '.join(non_matching_shapes_str_list)} \"\nf\"defaults as they have different than input signature shapes\"\n)\n@wrapt.decorator\ndef _wrapper(wrapped, instance, args, kwargs):\nmodel_config = get_model_config(wrapped, instance)\n_verify_defaults(model_config)\n# verification if not after group wrappers is in group wrappers\n(requests,) = args\nmodel_supports_batching = model_config.batching\nfor request in requests:\nbatch_size = get_inference_request_batch_size(request) if model_supports_batching else None\nfor default_key, default_value in defaults.items():\nif default_key in request:\ncontinue\nif model_supports_batching:\nones_reps = (1,) * default_value.ndim  # repeat once default_value on each axis\naxis_reps = (batch_size,) + ones_reps  # ... except on batch axis. we repeat it batch_size times\ndefault_value = np.tile(default_value, axis_reps)\nrequest[default_key] = default_value\nreturn wrapped(*args, **kwargs)\nreturn _wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.first_value","title":"<code>first_value(*keys, squeeze_single_values=True, strict=True)</code>","text":"<p>This decorator overwrites selected inputs with first element of the given input.</p> <p>It can be used in two ways:</p> <ol> <li> <p>Wrapping a single request inference callable by chaining with @batch decorator:     @batch     @first_value(\"temperature\")     def infer_fn(**inputs):         ...         return result</p> </li> <li> <p>Wrapping a multiple requests inference callable:     @first_value(\"temperature\")     def infer_fn(requests):         ...         return results</p> </li> </ol> <p>By default, the decorator squeezes single value arrays to scalars. This behavior can be disabled by setting the <code>squeeze_single_values</code> flag to False.</p> <p>By default, the decorator checks the equality of the values on selected values. This behavior can be disabled by setting the <code>strict</code> flag to False.</p> <p>Wrapper can only be used with models that support batching.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str</code> <p>The input keys selected for conversion.</p> <code>()</code> <code>squeeze_single_values</code> <p>squeeze single value ND array to scalar values. Defaults to True.</p> <code>True</code> <code>strict</code> <code>bool</code> <p>enable checking if all values on single selected input of request are equal. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>PyTritonRuntimeError</code> <p>if not all values on a single selected input of the request are equal</p> <code>PyTritonBadParameterError</code> <p>if any of the keys passed to the decorator are not allowed.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def first_value(*keys: str, squeeze_single_values=True, strict: bool = True):\n\"\"\"This decorator overwrites selected inputs with first element of the given input.\n    It can be used in two ways:\n    1. Wrapping a single request inference callable by chaining with @batch decorator:\n        @batch\n        @first_value(\"temperature\")\n        def infer_fn(**inputs):\n            ...\n            return result\n    2. Wrapping a multiple requests inference callable:\n        @first_value(\"temperature\")\n        def infer_fn(requests):\n            ...\n            return results\n    By default, the decorator squeezes single value arrays to scalars.\n    This behavior can be disabled by setting the `squeeze_single_values` flag to False.\n    By default, the decorator checks the equality of the values on selected values.\n    This behavior can be disabled by setting the `strict` flag to False.\n    Wrapper can only be used with models that support batching.\n    Args:\n        keys: The input keys selected for conversion.\n        squeeze_single_values: squeeze single value ND array to scalar values. Defaults to True.\n        strict: enable checking if all values on single selected input of request are equal. Defaults to True.\n    Raises:\n        PyTritonRuntimeError: if not all values on a single selected input of the request are equal\n        and the strict flag is set to True. Additionally, if the decorator is used with a model that doesn't support batching,\n        PyTritonBadParameterError: if any of the keys passed to the decorator are not allowed.\n    \"\"\"\nif any(k in _SPECIAL_KEYS for k in keys):\nnot_allowed_keys = [key for key in keys if key in _SPECIAL_KEYS]\nraise PyTritonBadParameterError(\nf\"The keys {', '.join(not_allowed_keys)} are not allowed as keys for @first_value wrapper. \"\nf\"The set of not allowed keys are {', '.join(_SPECIAL_KEYS)}\"\n)\n@wrapt.decorator\ndef wrapper(wrapped, instance, args, kwargs):\nmodel_config = get_model_config(wrapped, instance)\nif not model_config.batching:\nraise PyTritonRuntimeError(\"The @first_value decorator can only be used with models that support batching.\")\ndef _replace_inputs_with_first_value(_request):\nfor input_name in keys:\nif input_name not in _request:\ncontinue\nvalues = _request[input_name]\nif strict:\n# do not set axis for arrays with strings (object) or models not supporting batching\naxis_of_uniqueness = None if values.dtype == object else 0\nunique_values = np.unique(values, axis=axis_of_uniqueness)\nif len(unique_values) &gt; 1:\nraise PyTritonRuntimeError(\nf\"The values on the {input_name!r} input are not equal. \"\n\"To proceed, either disable strict mode in @first_value wrapper \"\n\"or ensure that the values always are consistent. \"\nf\"The current values of {input_name!r} are {_request[input_name]!r}.\"\n)\n_first_value = values[0]\nif (\nsqueeze_single_values\nand not np.isscalar(_first_value)\nand all(dim == 1 for dim in _first_value.shape)\n):\n_dim_0_array = np.squeeze(_first_value)\n_first_value = _dim_0_array[()]  # obtain scalar from 0-dim array with numpy type\n_request[input_name] = _first_value\nreturn _request\ninputs_names = set(kwargs) - set(_SPECIAL_KEYS)\nif inputs_names:\nkwargs = _replace_inputs_with_first_value(kwargs)\nreturn wrapped(*args, **kwargs)\nelse:\nrequests, *other_args = args\nrequests = [_replace_inputs_with_first_value(request) for request in requests]\nreturn wrapped(requests, *other_args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.get_inference_request_batch_size","title":"<code>get_inference_request_batch_size(inference_request)</code>","text":"<p>Get batch size from triton request.</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>InferenceRequest</code> <p>Triton request.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Batch size.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_inference_request_batch_size(inference_request: InferenceRequest) -&gt; int:\n\"\"\"Get batch size from triton request.\n    Args:\n        inference_request (InferenceRequest): Triton request.\n    Returns:\n        int: Batch size.\n    \"\"\"\nfirst_input_value = next(iter(inference_request.values()))\nbatch_size, *dims = first_input_value.shape\nreturn batch_size\n</code></pre>"},{"location":"api/#pytriton.decorators.get_model_config","title":"<code>get_model_config(wrapped, instance)</code>","text":"<p>Retrieves instance of TritonModelConfig from callable.</p> <p>It is internally used in convert_output function to get output list from model. You can use this in custom decorators if you need access to model_config information. If you use @triton_context decorator you do not need this function (you can get model_config directly from triton_context passing function/callable to dictionary getter).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_model_config(wrapped, instance) -&gt; TritonModelConfig:\n\"\"\"Retrieves instance of TritonModelConfig from callable.\n    It is internally used in convert_output function to get output list from model.\n    You can use this in custom decorators if you need access to model_config information.\n    If you use @triton_context decorator you do not need this function (you can get model_config directly\n    from triton_context passing function/callable to dictionary getter).\n    \"\"\"\nreturn get_triton_context(wrapped, instance).model_configs[wrapped]\n</code></pre>"},{"location":"api/#pytriton.decorators.get_triton_context","title":"<code>get_triton_context(wrapped, instance)</code>","text":"<p>Retrieves triton context from callable.</p> <p>It is used in @triton_context to get triton context registered by triton binding in inference callable. If you use @triton_context decorator you do not need this function.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_triton_context(wrapped, instance) -&gt; TritonContext:\n\"\"\"Retrieves triton context from callable.\n    It is used in @triton_context to get triton context registered by triton binding in inference callable.\n    If you use @triton_context decorator you do not need this function.\n    \"\"\"\ncaller = instance or wrapped\nif not hasattr(caller, \"__triton_context__\"):\nraise PyTritonValidationError(\"Wrapped function or object must bound with triton to get  __triton_context__\")\nreturn caller.__triton_context__\n</code></pre>"},{"location":"api/#pytriton.decorators.group_by_keys","title":"<code>group_by_keys(wrapped, instance, args, kwargs)</code>","text":"<p>Group by keys.</p> <p>Decorator prepares groups of requests with the same set of keys and calls wrapped function for each group separately (it is convenient to use this decorator before batching, because the batching decorator requires consistent set of inputs as it stacks them into batches).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef group_by_keys(wrapped, instance, args, kwargs):\n\"\"\"Group by keys.\n    Decorator prepares groups of requests with the same set of keys and calls wrapped function\n    for each group separately (it is convenient to use this decorator before batching, because the batching decorator\n    requires consistent set of inputs as it stacks them into batches).\n    \"\"\"\ninputs = args[0]\nidx_inputs = [(idx, tuple(sorted(input.keys())), input) for idx, input in enumerate(inputs)]\nidx_inputs.sort(key=operator.itemgetter(1))\nidx_groups_res = []\nfor _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\nidx, _key, sample_list = zip(*group)\nargs = (list(sample_list),) + args[1:]\nout = wrapped(*args, **kwargs)\nidx_groups_res.extend(zip(idx, out))\nidx_groups_res.sort(key=operator.itemgetter(0))\nres_flat = [r[1] for r in idx_groups_res]\nreturn res_flat\n</code></pre>"},{"location":"api/#pytriton.decorators.group_by_values","title":"<code>group_by_values(*keys, pad_fn=None)</code>","text":"<p>Decorator for grouping requests by values of selected keys.</p> <p>This function splits a batch into multiple sub-batches based on the specified keys values and calls the decorated function with each sub-batch. This is particularly useful when working with models that require dynamic parameters sent by the user.</p> <p>For example, given an input of the form:</p> <pre><code>{\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n</code></pre> <p>Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:</p> <pre><code>[\n    {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n    {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n]\n</code></pre> <p>This decorator should be used after the @batch decorator.</p> Example usage <p>@batch @group_by_values(\"param1\", \"param2\") def infer_fun(**inputs):     ...     return outputs</p> <p>Parameters:</p> Name Type Description Default <code>*keys</code> <p>List of keys to group by.</p> <code>()</code> <code>pad_fn</code> <code>typing.Optional[typing.Callable[[InferenceRequests], InferenceRequests]]</code> <p>Optional function to pad the batch to the same size before merging again to a single batch.</p> <code>None</code> <p>Returns:</p> Type Description <p>The decorator function.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def group_by_values(*keys, pad_fn: typing.Optional[typing.Callable[[InferenceRequests], InferenceRequests]] = None):\n\"\"\"Decorator for grouping requests by values of selected keys.\n    This function splits a batch into multiple sub-batches based on the specified keys values and\n    calls the decorated function with each sub-batch. This is particularly useful when working with models\n    that require dynamic parameters sent by the user.\n    For example, given an input of the form:\n        {\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n    Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:\n        [\n            {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n            {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n        ]\n    This decorator should be used after the @batch decorator.\n    Example usage:\n        @batch\n        @group_by_values(\"param1\", \"param2\")\n        def infer_fun(**inputs):\n            ...\n            return outputs\n    Args:\n        *keys: List of keys to group by.\n        pad_fn: Optional function to pad the batch to the same size before merging again to a single batch.\n    Returns:\n        The decorator function.\n    \"\"\"\ndef value_to_key(value):\nif isinstance(value, np.ndarray):\nif value.dtype == np.object_ or value.dtype.type == np.bytes_:\nreturn _serialize_byte_tensor(value)\nelse:\nreturn value.tobytes()\nreturn value\ndef _get_sort_key_for_sample(_request, _sample_idx: int):\nreturn tuple(value_to_key(_request[_key][_sample_idx]) for _key in keys)\ndef _group_request(_request: InferenceRequest, _batch_size: int):\nidx_inputs = [(sample_idx, _get_sort_key_for_sample(_request, sample_idx)) for sample_idx in range(_batch_size)]\nidx_inputs.sort(key=operator.itemgetter(1))\nfor _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\n_samples_idxes, _ = zip(*group)\ngrouped_request = {input_name: value[_samples_idxes, ...] for input_name, value in _request.items()}\nyield _samples_idxes, grouped_request\n@wrapt.decorator\ndef _wrapper(wrapped, instance, args, kwargs):\nwrappers_stack = [\ncallable_with_wrapper.wrapper\nfor callable_with_wrapper in _get_wrapt_stack(wrapped)\nif callable_with_wrapper.wrapper is not None\n]\nif batch in wrappers_stack:\nraise PyTritonRuntimeError(\"The @group_by_values decorator must be used after the @batch decorator.\")\nrequest = {k: v for k, v in kwargs.items() if k not in _SPECIAL_KEYS}\nother_kwargs = {k: v for k, v in kwargs.items() if k in _SPECIAL_KEYS}\nbatch_size = get_inference_request_batch_size(request)\nsample_indices_with_interim_result = []\nfor sample_indices, _grouped_sub_request in _group_request(request, batch_size):\ninterim_result = wrapped(*args, **_grouped_sub_request, **other_kwargs)\nsample_indices_with_interim_result.append((sample_indices, interim_result))\nif pad_fn is not None:\nindices, results = tuple(map(tuple, zip(*sample_indices_with_interim_result)))\nresults = pad_fn(results)\nsample_indices_with_interim_result = tuple(zip(indices, results))\n_, first_result_data = sample_indices_with_interim_result[0]\nresult = {\noutput_name: np.zeros((batch_size,) + data.shape[1:], dtype=data.dtype)\nfor output_name, data in first_result_data.items()\n}\nfor indices, results in sample_indices_with_interim_result:\nfor output_name, data in results.items():\nresult[output_name][indices, ...] = data\nreturn result\nreturn _wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.pad_batch","title":"<code>pad_batch(wrapped, instance, args, kwargs)</code>","text":"<p>Add padding to the inputs batches.</p> <p>Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or max batch size from model config whatever is closer to current input size).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef pad_batch(wrapped, instance, args, kwargs):\n\"\"\"Add padding to the inputs batches.\n    Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or\n    max batch size from model config whatever is closer to current input size).\n    \"\"\"\ninputs = {k: v for k, v in kwargs.items() if k != \"__triton_context__\"}\nfirst_input = next(iter(inputs.values()))\nconfig = get_model_config(wrapped, instance)\nbatch_sizes = (\n[]\nif (config.batcher is None or config.batcher.preferred_batch_size is None)\nelse sorted(config.batcher.preferred_batch_size)\n)\nbatch_sizes.append(config.max_batch_size)\nbatch_size = batch_sizes[bisect_left(batch_sizes, first_input.shape[0])]\nnew_inputs = {\ninput_name: np.repeat(\ninput_array,\nnp.concatenate(\n[np.ones(input_array.shape[0] - 1), np.array([batch_size - input_array.shape[0] + 1])]\n).astype(np.int64),\naxis=0,\n)\nfor input_name, input_array in inputs.items()\n}\nkwargs.update(new_inputs)\nreturn wrapped(*args, **kwargs)\n</code></pre>"},{"location":"api/#pytriton.decorators.sample","title":"<code>sample(wrapped, instance, args, kwargs)</code>","text":"<p>Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.</p> <p>Decorator takes first request and convert it into named inputs. Useful with non-batching models - instead of one element list of request, we will get named inputs - <code>kwargs</code>.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef sample(wrapped, instance, args, kwargs):\n\"\"\"Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.\n    Decorator takes first request and convert it into named inputs.\n    Useful with non-batching models - instead of one element list of request, we will get named inputs - `kwargs`.\n    \"\"\"\nkwargs.update(args[0][0])\noutputs = wrapped(*args[1:], **kwargs)\noutputs = convert_output(outputs, wrapped, instance)\nreturn [outputs]\n</code></pre>"},{"location":"api/#pytriton.decorators.triton_context","title":"<code>triton_context(wrapped, instance, args, kwargs)</code>","text":"<p>Adds triton context.</p> <p>It gives you additional argument passed to the function in **kwargs called 'triton_context'. You can read model config from it and in the future possibly have some interaction with triton.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef triton_context(wrapped, instance, args, kwargs):\n\"\"\"Adds triton context.\n    It gives you additional argument passed to the function in **kwargs called 'triton_context'.\n    You can read model config from it and in the future possibly have some interaction with triton.\n    \"\"\"\nkwargs[TRITON_CONTEXT_FIELD_NAME] = get_triton_context(wrapped, instance)\nreturn wrapped(*args, **kwargs)\n</code></pre>"},{"location":"api/#pytriton.triton.Triton","title":"<code>pytriton.triton.Triton(*, config=None, workspace=None)</code>","text":"<p>Triton Inference Server for Python models.</p> <p>Initialize Triton Inference Server context for starting server and loading models.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[TritonConfig]</code> <p>TritonConfig object with optional customizations for Triton Inference Server. Configuration can be passed also through environment variables. See TritonConfig.from_env() class method for details.</p> <p>Order of precedence:</p> <ul> <li>config defined through <code>config</code> parameter of init method.</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul> <code>None</code> <code>workspace</code> <code>Union[Workspace, str, pathlib.Path, None]</code> <p>workspace or path where the Triton Model Store and files used by pytriton will be created. If workspace is <code>None</code> random workspace will be created. Workspace will be deleted in Triton.stop().</p> <code>None</code> Source code in <code>pytriton/triton.py</code> <pre><code>def __init__(\nself, *, config: Optional[TritonConfig] = None, workspace: Union[Workspace, str, pathlib.Path, None] = None\n):\n\"\"\"Initialize Triton Inference Server context for starting server and loading models.\n    Args:\n        config: TritonConfig object with optional customizations for Triton Inference Server.\n            Configuration can be passed also through environment variables.\n            See [TritonConfig.from_env()][pytriton.triton.TritonConfig.from_env] class method for details.\n            Order of precedence:\n              - config defined through `config` parameter of init method.\n              - config defined in environment variables\n              - default TritonConfig values\n        workspace: workspace or path where the Triton Model Store and files used by pytriton will be created.\n            If workspace is `None` random workspace will be created.\n            Workspace will be deleted in [Triton.stop()][pytriton.triton.Triton.stop].\n    \"\"\"\ndef _without_none_values(_d):\nreturn {name: value for name, value in _d.items() if value is not None}\ndefault_config_dict = _without_none_values(TritonConfig().to_dict())\nenv_config_dict = _without_none_values(TritonConfig.from_env().to_dict())\nexplicit_config_dict = _without_none_values(config.to_dict() if config else {})\nconfig_dict = {**default_config_dict, **env_config_dict, **explicit_config_dict}\nself._config = TritonConfig(**config_dict)\nself._workspace = workspace if isinstance(workspace, Workspace) else Workspace(workspace)\nmodel_repository = TritonModelRepository(path=self._config.model_repository, workspace=self._workspace)\nself._model_manager = ModelManager(model_repository)\nself._triton_server_config = TritonServerConfig()\nconfig_data = self._config.to_dict()\nself._python_backend_config = PythonBackendConfig()\nbackend_config_data = {\n\"shm_default_byte_size\": INITIAL_BACKEND_SHM_SIZE,\n\"shm-growth-byte-size\": GROWTH_BACKEND_SHM_SIZE,\n}\nfor name, value in backend_config_data.items():\nif name not in PythonBackendConfig.allowed_keys() or value is None:\ncontinue\nself._python_backend_config[name] = value\nfor name, value in config_data.items():\nif name not in TritonServerConfig.allowed_keys() or value is None:\ncontinue\nself._triton_server_config[name] = value\nself._triton_server_config[\"backend_config\"] = self._python_backend_config.to_cli_string()\nself._triton_server_config[\"model_repository\"] = model_repository.path.as_posix()\nself._triton_server_config[\"backend_directory\"] = (TRITONSERVER_DIST_DIR / \"backends\").as_posix()\nif \"cache_directory\" not in self._triton_server_config:\nself._triton_server_config[\"cache_directory\"] = (get_root_module_path() / \"tritonserver/caches\").as_posix()\nself._triton_server = TritonServer(\npath=(TRITONSERVER_DIST_DIR / \"bin/tritonserver\").as_posix(),\nlibs_path=get_libs_path(),\nconfig=self._triton_server_config,\n)\nself._cv = th.Condition()\nwith self._cv:\nself._stopped = True\nself.triton_context = TritonContext()\nurl = (\nself._triton_server.get_endpoint(\"http\")\nif (self._config.allow_http is None or self._config.allow_http)\nelse self._triton_server.get_endpoint(\"grpc\")\n)\nself._log_level_checker = _LogLevelChecker(url)\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context.</p> <p>Returns:</p> Type Description <code>Triton</code> <p>A Triton object</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __enter__(self) -&gt; \"Triton\":\n\"\"\"Enter the context.\n    Returns:\n        A Triton object\n    \"\"\"\nreturn self\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.__exit__","title":"<code>__exit__(*_)</code>","text":"<p>Exit the context stopping the process and cleaning the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>*_</code> <p>unused arguments</p> <code>()</code> Source code in <code>pytriton/triton.py</code> <pre><code>def __exit__(self, *_) -&gt; None:\n\"\"\"Exit the context stopping the process and cleaning the workspace.\n    Args:\n        *_: unused arguments\n    \"\"\"\nself.stop()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.bind","title":"<code>bind(model_name, infer_func, inputs, outputs, model_version=1, config=None, strict=False)</code>","text":"<p>Create a model with given name and inference callable binding into Triton Inference Server.</p> <p>More information about model configuration: https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md</p> <p>Parameters:</p> Name Type Description Default <code>infer_func</code> <code>Union[Callable, Sequence[Callable]]</code> <p>Inference callable to handle request/response from Triton Inference Server</p> required <code>inputs</code> <code>Sequence[Tensor]</code> <p>Definition of model inputs</p> required <code>outputs</code> <code>Sequence[Tensor]</code> <p>Definition of model outputs</p> required <code>model_name</code> <code>str</code> <p>Name under which model is available in Triton Inference Server. It can only contain</p> required <code>model_version</code> <code>int</code> <p>Version of model</p> <code>1</code> <code>config</code> <code>Optional[ModelConfig]</code> <p>Model configuration for Triton Inference Server deployment</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Enable strict validation between model config outputs and inference function result</p> <code>False</code> Source code in <code>pytriton/triton.py</code> <pre><code>def bind(\nself,\nmodel_name: str,\ninfer_func: Union[Callable, Sequence[Callable]],\ninputs: Sequence[Tensor],\noutputs: Sequence[Tensor],\nmodel_version: int = 1,\nconfig: Optional[ModelConfig] = None,\nstrict: bool = False,\n) -&gt; None:\n\"\"\"Create a model with given name and inference callable binding into Triton Inference Server.\n    More information about model configuration:\n    https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md\n    Args:\n        infer_func: Inference callable to handle request/response from Triton Inference Server\n        (or list of inference callable for multi instance model)\n        inputs: Definition of model inputs\n        outputs: Definition of model outputs\n        model_name: Name under which model is available in Triton Inference Server. It can only contain\n        alphanumeric characters, dots, underscores and dashes.\n        model_version: Version of model\n        config: Model configuration for Triton Inference Server deployment\n        strict: Enable strict validation between model config outputs and inference function result\n    \"\"\"\nself._validate_model_name(model_name)\nmodel = Model(\nmodel_name=model_name,\nmodel_version=model_version,\ninference_fn=infer_func,\ninputs=inputs,\noutputs=outputs,\nconfig=config if config else ModelConfig(),\nworkspace=self._workspace,\ntriton_context=self.triton_context,\nstrict=strict,\n)\nmodel.on_model_event(self._on_model_event)\nself._model_manager.add_model(model)\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.is_alive","title":"<code>is_alive()</code>","text":"<p>Verify is deployed models and server are alive.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if server and loaded models are alive, False otherwise.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def is_alive(self) -&gt; bool:\n\"\"\"Verify is deployed models and server are alive.\n    Returns:\n        True if server and loaded models are alive, False otherwise.\n    \"\"\"\nif not self._triton_server.is_alive():\nreturn False\nfor model in self._model_manager.models:\nif not model.is_alive():\nreturn False\nreturn True\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.run","title":"<code>run()</code>","text":"<p>Run Triton Inference Server.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def run(self) -&gt; None:\n\"\"\"Run Triton Inference Server.\"\"\"\nif not self._triton_server.is_alive():\nself._model_manager.create_models()\nwith self._cv:\nself._stopped = False\nLOGGER.debug(\"Starting Triton Inference\")\nself._triton_server.register_on_exit(self._on_tritonserver_exit)\natexit.register(self.stop)\nself._triton_server.start()\nself._wait_for_models()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.serve","title":"<code>serve(monitoring_period_sec=MONITORING_PERIOD_SEC)</code>","text":"<p>Run Triton Inference Server and lock thread for serving requests/response.</p> <p>Parameters:</p> Name Type Description Default <code>monitoring_period_sec</code> <code>int</code> <p>the timeout of monitoring if Triton and models are available. Every monitoring_period_sec seconds main thread wakes up and check if triton server and proxy backend are still alive and sleep again. If triton or proxy is not alive - method returns.</p> <code>MONITORING_PERIOD_SEC</code> Source code in <code>pytriton/triton.py</code> <pre><code>def serve(self, monitoring_period_sec: int = MONITORING_PERIOD_SEC) -&gt; None:\n\"\"\"Run Triton Inference Server and lock thread for serving requests/response.\n    Args:\n        monitoring_period_sec: the timeout of monitoring if Triton and models are available.\n            Every monitoring_period_sec seconds main thread wakes up and check if triton server and proxy backend\n            are still alive and sleep again. If triton or proxy is not alive - method returns.\n    \"\"\"\nself.run()\nwith self._cv:\nwhile self.is_alive():\nself._cv.wait(timeout=monitoring_period_sec)\nself.stop()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.stop","title":"<code>stop()</code>","text":"<p>Stop Triton Inference Server.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def stop(self) -&gt; None:\n\"\"\"Stop Triton Inference Server.\"\"\"\nLOGGER.debug(\"Stopping Triton Inference server and proxy backends\")\nwith self._cv:\nif self._stopped:\nLOGGER.debug(\"Triton Inference already stopped.\")\nreturn\nself._stopped = True\nself._triton_server.unregister_on_exit(self._on_tritonserver_exit)\natexit.unregister(self.stop)\nself._triton_server.stop()\nself._model_manager.clean()\nself._workspace.clean()\nwith self._cv:\nself._cv.notify_all()\nLOGGER.debug(\"Stopped Triton Inference server and proxy backends\")\nself._log_level_checker.check(skip_update=True)\n</code></pre>"},{"location":"api/#pytriton.model_config.tensor.Tensor","title":"<code>pytriton.model_config.tensor.Tensor</code>  <code>dataclass</code>","text":"<p>Model input and output definition for Triton deployment.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple</code> <p>Shape of the input/output tensor.</p> required <code>dtype</code> <code>Union[np.dtype, Type[np.dtype], Type[object]]</code> <p>Data type of the input/output tensor.</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the input/output of model.</p> <code>None</code> <code>optional</code> <code>Optional[bool]</code> <p>Flag to mark if input is optional.</p> <code>False</code>"},{"location":"api/#pytriton.model_config.tensor.Tensor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Override object values on post init or field override.</p> Source code in <code>pytriton/model_config/tensor.py</code> <pre><code>def __post_init__(self):\n\"\"\"Override object values on post init or field override.\"\"\"\nif isinstance(self.dtype, np.dtype):\nobject.__setattr__(self, \"dtype\", self.dtype.type)  # pytype: disable=attribute-error\n</code></pre>"},{"location":"api/#pytriton.model_config.common","title":"<code>pytriton.model_config.common</code>","text":"<p>Common structures for internal and external ModelConfig.</p>"},{"location":"api/#pytriton.model_config.common.DeviceKind","title":"<code>DeviceKind</code>","text":"<p>             Bases: <code>enum.Enum</code></p> <p>Device kind for model deployment.</p> <p>Parameters:</p> Name Type Description Default <code>KIND_AUTO</code> <p>Automatically select the device for model deployment.</p> required <code>KIND_CPU</code> <p>Model is deployed on CPU.</p> required <code>KIND_GPU</code> <p>Model is deployed on GPU.</p> required"},{"location":"api/#pytriton.model_config.common.DynamicBatcher","title":"<code>DynamicBatcher</code>  <code>dataclass</code>","text":"<p>Dynamic batcher configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> <code>0</code> <code>preferred_batch_size</code> <code>Optional[list]</code> <p>Preferred batch sizes for dynamic batching.</p> <code>None</code> <code>preserve_ordering</code> <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> <code>False</code> <code>priority_levels</code> <code>int</code> <p>The number of priority levels to be enabled for the model.</p> <code>0</code> <code>default_priority_level</code> <code>int</code> <p>The priority level used for requests that don't specify their priority.</p> <code>0</code> <code>default_queue_policy</code> <code>Optional[QueuePolicy]</code> <p>The default queue policy used for requests.</p> <code>None</code> <code>priority_queue_policy</code> <code>Optional[Dict[int, QueuePolicy]]</code> <p>Specify the queue policy for the priority level.</p> <code>None</code>"},{"location":"api/#pytriton.model_config.common.QueuePolicy","title":"<code>QueuePolicy</code>  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>timeout_action</code> <code>TimeoutAction</code> <p>The action applied to timed-out request.</p> <code>TimeoutAction.REJECT</code> <code>default_timeout_microseconds</code> <code>int</code> <p>The default timeout for every request, in microseconds.</p> <code>0</code> <code>allow_timeout_override</code> <code>bool</code> <p>Whether individual request can override the default timeout value.</p> <code>False</code> <code>max_queue_size</code> <code>int</code> <p>The maximum queue size for holding requests.</p> <code>0</code>"},{"location":"api/#pytriton.model_config.common.TimeoutAction","title":"<code>TimeoutAction</code>","text":"<p>             Bases: <code>enum.Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Parameters:</p> Name Type Description Default <code>REJECT</code> <p>Reject the request and return error message accordingly.</p> required <code>DELAY</code> <p>Delay the request until all other requests at the same (or higher) priority levels that have not reached their timeouts are processed.</p> required"},{"location":"api/#pytriton.model_config.model_config.ModelConfig","title":"<code>pytriton.model_config.model_config.ModelConfig</code>  <code>dataclass</code>","text":"<p>Additional model configuration for running model through Triton Inference Server.</p> <p>Parameters:</p> Name Type Description Default <code>batching</code> <code>bool</code> <p>Flag to enable/disable batching for model.</p> <code>True</code> <code>max_batch_size</code> <code>int</code> <p>The maximal batch size that would be handled by model.</p> <code>4</code> <code>batcher</code> <code>DynamicBatcher</code> <p>Configuration of Dynamic Batching for the model.</p> <code>dataclasses.field(default_factory=DynamicBatcher)</code> <code>response_cache</code> <code>bool</code> <p>Flag to enable/disable response cache for the model</p> <code>False</code>"},{"location":"api/#pytriton.client.client","title":"<code>pytriton.client.client</code>","text":"<p>Clients for easy interaction with models deployed on the Triton Inference Server.</p> Typical usage example <p>with ModelClient(\"localhost\", \"MyModel\") as client:     result_dict = client.infer_sample(input_a=a, input_b=b)</p> Inference inputs can be provided either as positional or keyword arguments <p>result_dict = client.infer_sample(input1, input2) result_dict = client.infer_sample(a=input1, b=input2)</p> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p>"},{"location":"api/#pytriton.client.client.AsyncioModelClient","title":"<code>AsyncioModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>             Bases: <code>BaseModelClient</code></p> <p>Asyncio client for model deployed on the Triton Inference Server.</p> <p>This client is based on Triton Inference Server Python clients and GRPC library: * <code>tritonclient.http.aio.InferenceServerClient</code> * <code>tritonclient.grpc.aio.InferenceServerClient</code></p> <p>It can wait for server to be ready with model loaded and then perform inference on it. <code>AsyncioModelClient</code> supports asyncio context manager protocol.</p> <p>Typical usage: <pre><code>from pytriton.client import AsyncioModelClient\nimport numpy as np\ninput1_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\ninput2_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\nasync with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\nresult_dict = await client.infer_sample(input1_sample, input2_sample)\nprint(result_dict[\"output_name\"])\n</code></pre></p> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> required <code>model_name</code> <code>str</code> <p>name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> <code>None</code> <code>lazy_init</code> <code>bool</code> <p>if initialization should be performed just before sending first request to inference server.</p> <code>True</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>timeout for server and model being ready.</p> <code>None</code> <p>Raises:</p> Type Description <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientTimeoutError</code> <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> <code>PyTritonClientUrlParseError</code> <p>In case of problems with parsing url.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\nself,\nurl: str,\nmodel_name: str,\nmodel_version: Optional[str] = None,\n*,\nlazy_init: bool = True,\ninit_timeout_s: Optional[float] = None,\ninference_timeout_s: Optional[float] = None,\n):\n\"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for server and model being ready.\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError: if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\nsuper().__init__(\nurl=url,\nmodel_name=model_name,\nmodel_version=model_version,\nlazy_init=lazy_init,\ninit_timeout_s=init_timeout_s,\ninference_timeout_s=inference_timeout_s,\n)\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.model_config","title":"<code>model_config</code>  <code>async</code> <code>property</code>","text":"<p>Obtain configuration of model deployed on the Triton Inference Server.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Create context for use AsyncioModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aenter__(self):\n\"\"\"Create context for use AsyncioModelClient as a context manager.\"\"\"\n_LOGGER.debug(\"Entering AsyncioModelClient context\")\ntry:\nif not self._lazy_init:\n_LOGGER.debug(\"Waiting in AsyncioModelClient context for model to be ready\")\nawait self._wait_and_init_model_config(self._init_timeout_s)\n_LOGGER.debug(\"Model is ready in AsyncioModelClient context\")\nreturn self\nexcept Exception as e:\n_LOGGER.error(\"Error occurred during AsyncioModelClient context initialization\")\nawait self.close()\nraise e\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.__aexit__","title":"<code>__aexit__(*_)</code>  <code>async</code>","text":"<p>Close resources used by AsyncioModelClient when exiting from context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aexit__(self, *_):\n\"\"\"Close resources used by AsyncioModelClient when exiting from context.\"\"\"\nawait self.close()\n_LOGGER.debug(\"Exiting AsyncioModelClient context\")\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close resources used by _ModelClientBase.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def close(self):\n\"\"\"Close resources used by _ModelClientBase.\"\"\"\n_LOGGER.debug(\"Closing InferenceServerClient\")\nawait self._general_client.close()\nawait self._infer_client.close()\n_LOGGER.debug(\"InferenceServerClient closed\")\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.get_lib","title":"<code>get_lib()</code>","text":"<p>Get Triton Inference Server Python client library.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n\"\"\"Get Triton Inference Server Python client library.\"\"\"\nreturn {\"grpc\": tritonclient.grpc.aio, \"http\": tritonclient.http.aio}[self._scheme.lower()]\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.infer_batch","title":"<code>infer_batch(*inputs, parameters=None, headers=None, **named_inputs)</code>  <code>async</code>","text":"<p>Run asynchronous inference on batched data.</p> <p>Typical usage:</p> <pre><code>async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\nresult_dict = await client.infer_batch(input1, input2)\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = await client.infer_batch(input1, input2)\nresult_dict = await client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> <code>PyTritonClientModelDoesntSupportBatchingError</code> <p>if model doesn't support batching.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_batch(\nself,\n*inputs,\nparameters: Optional[Dict[str, Union[str, int, bool]]] = None,\nheaders: Optional[Dict[str, Union[str, int, bool]]] = None,\n**named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n\"\"\"Run asynchronous inference on batched data.\n    Typical usage:\n    ```python\n    async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\n        result_dict = await client.infer_batch(input1, input2)\n    ```\n    Inference inputs can be provided either as positional or keyword arguments:\n    ```python\n    result_dict = await client.infer_batch(input1, input2)\n    result_dict = await client.infer_batch(a=input1, b=input2)\n    ```\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelDoesntSupportBatchingError: if model doesn't support batching.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n_verify_inputs_args(inputs, named_inputs)\n_verify_parameters(parameters)\n_verify_parameters(headers)\n_LOGGER.debug(f\"Running inference for {self._model_name}\")\nmodel_config = await self.model_config\n_LOGGER.debug(f\"Model config for {self._model_name} obtained\")\nmodel_supports_batching = model_config.max_batch_size &gt; 0\nif not model_supports_batching:\n_LOGGER.error(f\"Model {model_config.model_name} doesn't support batching\")\nraise PyTritonClientModelDoesntSupportBatchingError(\nf\"Model {model_config.model_name} doesn't support batching - use infer_sample method instead\"\n)\n_LOGGER.debug(f\"Running _infer for {self._model_name}\")\nresult = await self._infer(inputs or named_inputs, parameters, headers)\n_LOGGER.debug(f\"_infer for {self._model_name} finished\")\nreturn result\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.infer_sample","title":"<code>infer_sample(*inputs, parameters=None, headers=None, **named_inputs)</code>  <code>async</code>","text":"<p>Run asynchronous inference on single data sample.</p> <p>Typical usage:</p> <pre><code>async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\nresult_dict = await client.infer_sample(input1, input2)\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = await client.infer_sample(input1, input2)\nresult_dict = await client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_sample(\nself,\n*inputs,\nparameters: Optional[Dict[str, Union[str, int, bool]]] = None,\nheaders: Optional[Dict[str, Union[str, int, bool]]] = None,\n**named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n\"\"\"Run asynchronous inference on single data sample.\n    Typical usage:\n    ```python\n    async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\n        result_dict = await client.infer_sample(input1, input2)\n    ```\n    Inference inputs can be provided either as positional or keyword arguments:\n    ```python\n    result_dict = await client.infer_sample(input1, input2)\n    result_dict = await client.infer_sample(a=input1, b=input2)\n    ```\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n_verify_inputs_args(inputs, named_inputs)\n_verify_parameters(parameters)\n_verify_parameters(headers)\n_LOGGER.debug(f\"Running inference for {self._model_name}\")\nmodel_config = await self.model_config\n_LOGGER.debug(f\"Model config for {self._model_name} obtained\")\nmodel_supports_batching = model_config.max_batch_size &gt; 0\nif model_supports_batching:\nif inputs:\ninputs = tuple(data[np.newaxis, ...] for data in inputs)\nelif named_inputs:\nnamed_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\n_LOGGER.debug(f\"Running _infer for {self._model_name}\")\nresult = await self._infer(inputs or named_inputs, parameters, headers)\n_LOGGER.debug(f\"_infer for {self._model_name} finished\")\nif model_supports_batching:\nresult = {name: data[0] for name, data in result.items()}\nreturn result\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.wait_for_model","title":"<code>wait_for_model(timeout_s)</code>  <code>async</code>","text":"<p>Asynchronous wait for Triton Inference Server and deployed on it model readiness.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>timeout to server and model get into readiness state.</p> required <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If server and model are not in readiness state before given timeout.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>KeyboardInterrupt</code> <p>If hosting process receives SIGINT</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def wait_for_model(self, timeout_s: float):\n\"\"\"Asynchronous wait for Triton Inference Server and deployed on it model readiness.\n    Args:\n        timeout_s: timeout to server and model get into readiness state.\n    Raises:\n        PyTritonClientTimeoutError: If server and model are not in readiness state before given timeout.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\n_LOGGER.debug(f\"Waiting for model {self._model_name} to be ready\")\nasync with async_timeout.timeout(self._init_timeout_s):\nawait asyncio_wait_for_model_ready(\nself._general_client, self._model_name, self._model_version, timeout_s=timeout_s\n)\n_LOGGER.debug(f\"Model {self._model_name} is ready\")\n</code></pre>"},{"location":"api/#pytriton.client.client.BaseModelClient","title":"<code>BaseModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>Base client for model deployed on the Triton Inference Server.</p> <p>Inits BaseModelClient for given model deployed on the Triton Inference Server.</p> Common usage <pre><code>with ModelClient(\"localhost\", \"BERT\") as client\n    result_dict = client.infer_sample(input1_sample, input2_sample)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. <code>grpc://localhost:8001</code>. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> required <code>model_name</code> <code>str</code> <p>name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> <code>None</code> <code>lazy_init</code> <code>bool</code> <p>if initialization should be performed just before sending first request to inference server.</p> <code>True</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>timeout in seconds for the server and model to be ready. If not passed, the default timeout of 300 seconds will be used.</p> <code>None</code> <code>inference_timeout_s</code> <code>Optional[float]</code> <p>timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientTimeoutError</code> <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> <code>PyTritonClientInvalidUrlError</code> <p>If provided Triton Inference Server url is invalid.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\nself,\nurl: str,\nmodel_name: str,\nmodel_version: Optional[str] = None,\n*,\nlazy_init: bool = True,\ninit_timeout_s: Optional[float] = None,\ninference_timeout_s: Optional[float] = None,\n):\n\"\"\"Inits BaseModelClient for given model deployed on the Triton Inference Server.\n    Common usage:\n        ```\n        with ModelClient(\"localhost\", \"BERT\") as client\n            result_dict = client.infer_sample(input1_sample, input2_sample)\n        ```\n    Args:\n        url: The Triton Inference Server url, e.g. `grpc://localhost:8001`.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout in seconds for the server and model to be ready. If not passed, the default timeout of 300 seconds will be used.\n        inference_timeout_s: timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError:\n            if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\nself._init_timeout_s = _DEFAULT_SYNC_INIT_TIMEOUT_S if init_timeout_s is None else init_timeout_s\nself._inference_timeout_s = DEFAULT_INFERENCE_TIMEOUT_S if inference_timeout_s is None else inference_timeout_s\nself._network_timeout_s = min(_DEFAULT_NETWORK_TIMEOUT_S, self._init_timeout_s)\nself._general_client = self.create_client_from_url(url, network_timeout_s=self._network_timeout_s)\nself._infer_client = self.create_client_from_url(url, network_timeout_s=self._inference_timeout_s)\nself._model_name = model_name\nself._model_version = model_version\nself._request_id_generator = itertools.count(0)\n# Monkey patch __del__ method from client to catch error in client when instance is garbage collected.\n# This is needed because we are closing client in __exit__ method or in close method.\n# (InferenceClient uses gevent library which does not support closing twice from different threads)\nself._monkey_patch_client()\nself._model_config = None\nself._model_ready = None\nself._lazy_init: bool = lazy_init\nself._handle_lazy_init()\n</code></pre>"},{"location":"api/#pytriton.client.client.BaseModelClient.create_client_from_url","title":"<code>create_client_from_url(url, network_timeout_s=None)</code>","text":"<p>Create Triton Inference Server client.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url of the server to connect to. If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added. If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.</p> required <code>network_timeout_s</code> <code>Optional[float]</code> <p>timeout for client commands. Default value is 60.0 s.</p> <code>None</code> <p>Returns:</p> Type Description <p>Triton Inference Server client.</p> <p>Raises:</p> Type Description <code>PyTritonClientInvalidUrlError</code> <p>If provided Triton Inference Server url is invalid.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def create_client_from_url(self, url: str, network_timeout_s: Optional[float] = None):\n\"\"\"Create Triton Inference Server client.\n    Args:\n        url: url of the server to connect to.\n            If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added.\n            If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.\n        network_timeout_s: timeout for client commands. Default value is 60.0 s.\n    Returns:\n        Triton Inference Server client.\n    Raises:\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\nif not isinstance(url, str):\nraise PyTritonClientInvalidUrlError(f\"Invalid url {url}. Url must be a string.\")\ntry:\nparsed_url = urllib.parse.urlparse(url)\n# change in py3.9+\n# https://github.com/python/cpython/commit/5a88d50ff013a64fbdb25b877c87644a9034c969\nif sys.version_info &lt; (3, 9) and not parsed_url.scheme and \"://\" in parsed_url.path:\nraise ValueError(f\"Invalid url {url}. Only grpc and http are supported.\")\nif (sys.version_info &lt; (3, 9) and not parsed_url.scheme and \"://\" not in parsed_url.path) or (\nsys.version_info &gt;= (3, 9) and parsed_url.scheme and not parsed_url.netloc\n):\n_LOGGER.debug(f\"Adding http scheme to {url}\")\nparsed_url = urllib.parse.urlparse(f\"http://{url}\")\nself._scheme = parsed_url.scheme.lower()\nif self._scheme not in [\"grpc\", \"http\"]:\nraise ValueError(f\"Invalid scheme {self._scheme}. Only grpc and http are supported.\")\nport = parsed_url.port or {\"grpc\": DEFAULT_GRPC_PORT, \"http\": DEFAULT_HTTP_PORT}[self._scheme]\nexcept ValueError as e:\nraise PyTritonClientInvalidUrlError(f\"Invalid url {url}\") from e\nself._triton_client_lib = self.get_lib()\nself._url = f\"{parsed_url.hostname}:{port}\"\nself._monkey_patch_client()\nif self._scheme == \"grpc\":\n# by default grpc client has very large number of timeout, thus we want to make it equal to http client timeout\nnetwork_timeout_s = _DEFAULT_NETWORK_TIMEOUT_S if network_timeout_s is None else network_timeout_s\n_LOGGER.warning(\nf\"tritonclient.grpc doesn't support timeout for other commands than infer. Ignoring network_timeout: {network_timeout_s}.\"\n)\ntriton_client_init_kwargs = self._get_init_extra_args()\n_LOGGER.debug(\nf\"Creating InferenceServerClient for {parsed_url.scheme}://{url} with {triton_client_init_kwargs}\"\n)\nreturn self._triton_client_lib.InferenceServerClient(self._url, **triton_client_init_kwargs)\n</code></pre>"},{"location":"api/#pytriton.client.client.BaseModelClient.get_lib","title":"<code>get_lib()</code>","text":"<p>Returns tritonclient library for given scheme.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n\"\"\"Returns tritonclient library for given scheme.\"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient","title":"<code>FuturesModelClient(url, model_name, model_version=None, *, max_workers=None, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>A client for interacting with a model deployed on the Triton Inference Server using concurrent.futures.</p> <p>This client allows asynchronous inference requests using a thread pool executor. It can be used to perform inference on a model by providing input data and receiving the corresponding output data. The client can be used in a <code>with</code> statement to ensure proper resource management.</p> Example usage <pre><code>with FuturesModelClient(\"localhost\", \"MyModel\") as client:\nresult_future = client.infer_sample(input1=input1_data, input2=input2_data)\n# do something else\nprint(result_future.result())\n</code></pre> <p>Initializes the FuturesModelClient for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. <code>grpc://localhost:8001</code>.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>The version of the model to interact with. If None, the latest version will be used.</p> <code>None</code> <code>max_workers</code> <code>Optional[int]</code> <p>The maximum number of threads that can be used to execute the given calls. If None, the <code>min(32, os.cpu_count() + 4)</code> number of threads will be used.</p> <code>None</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>Timeout in seconds for server and model being ready. If non passed default 60 seconds timeout will be used.</p> <code>None</code> <code>inference_timeout_s</code> <code>Optional[float]</code> <p>Timeout in seconds for the single model inference request. If non passed default 60 seconds timeout will be used.</p> <code>None</code> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\nself,\nurl: str,\nmodel_name: str,\nmodel_version: Optional[str] = None,\n*,\nmax_workers: Optional[int] = None,\ninit_timeout_s: Optional[float] = None,\ninference_timeout_s: Optional[float] = None,\n):\n\"\"\"Initializes the FuturesModelClient for a given model.\n    Args:\n        url: The Triton Inference Server url, e.g. `grpc://localhost:8001`.\n        model_name: The name of the model to interact with.\n        model_version: The version of the model to interact with. If None, the latest version will be used.\n        max_workers: The maximum number of threads that can be used to execute the given calls. If None, the `min(32, os.cpu_count() + 4)` number of threads will be used.\n        init_timeout_s: Timeout in seconds for server and model being ready. If non passed default 60 seconds timeout will be used.\n        inference_timeout_s: Timeout in seconds for the single model inference request. If non passed default 60 seconds timeout will be used.\n    \"\"\"\nself._url = url\nself._model_name = model_name\nself._model_version = model_version\nself._threads = []\nself._max_workers = max_workers\nif self._max_workers is not None and self._max_workers &lt;= 0:\nraise ValueError(\"max_workers must be greater than 0\")\nkwargs = {}\nif self._max_workers is not None:\nkwargs[\"maxsize\"] = self._max_workers\nself._queue = Queue(**kwargs)\nself._queue.put((_INIT, None, None))\nself._init_timeout_s = _DEFAULT_FUTURES_INIT_TIMEOUT_S if init_timeout_s is None else init_timeout_s\nself._inference_timeout_s = inference_timeout_s\nself._closed = False\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Create context for using FuturesModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n\"\"\"Create context for using FuturesModelClient as a context manager.\"\"\"\nreturn self\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Close resources used by FuturesModelClient instance when exiting from the context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n\"\"\"Close resources used by FuturesModelClient instance when exiting from the context.\"\"\"\nself.close()\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.close","title":"<code>close(wait=True)</code>","text":"<p>Close resources used by FuturesModelClient.</p> <p>This method closes the resources used by the FuturesModelClient instance, including the Triton Inference Server connections. Once this method is called, the FuturesModelClient instance should not be used again.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <p>If True, then shutdown will not return until all running futures have finished executing.</p> <code>True</code> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self, wait=True):\n\"\"\"Close resources used by FuturesModelClient.\n    This method closes the resources used by the FuturesModelClient instance, including the Triton Inference Server connections.\n    Once this method is called, the FuturesModelClient instance should not be used again.\n    Args:\n        wait: If True, then shutdown will not return until all running futures have finished executing.\n    \"\"\"\nif self._closed:\n_LOGGER.warning(\"FuturesModelClient is already closed\")\nreturn\n_LOGGER.debug(\"Closing FuturesModelClient.\")\nself._closed = True\nfor _ in range(len(self._threads)):\nself._queue.put((_CLOSE, None, None))\nif wait:\n_LOGGER.debug(\"Waiting for futures to finish.\")\nfor thread in self._threads:\nthread.join()\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.infer_batch","title":"<code>infer_batch(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run asynchronous inference on batched data and return a Future object.</p> <p>This method allows the user to perform inference on batched data by providing input data and receiving the corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.</p> Example usage <pre><code>with FuturesModelClient(\"localhost\", \"BERT\") as client:\nfuture = client.infer_batch(input1_sample, input2_sample)\n# do something else\nprint(future.result())\n</code></pre> Inference inputs can be provided either as positional or keyword arguments <pre><code>future = client.infer_batch(input1, input2)\nfuture = client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of HTTP headers for the inference request.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Future</code> <p>A Future object wrapping a dictionary of inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientClosedError</code> <p>If the FuturesModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(\nself,\n*inputs,\nparameters: Optional[Dict[str, Union[str, int, bool]]] = None,\nheaders: Optional[Dict[str, Union[str, int, bool]]] = None,\n**named_inputs,\n) -&gt; Future:\n\"\"\"Run asynchronous inference on batched data and return a Future object.\n    This method allows the user to perform inference on batched data by providing input data and receiving the corresponding output data.\n    The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.\n    Example usage:\n        ```python\n        with FuturesModelClient(\"localhost\", \"BERT\") as client:\n            future = client.infer_batch(input1_sample, input2_sample)\n            # do something else\n            print(future.result())\n        ```\n    Inference inputs can be provided either as positional or keyword arguments:\n        ```python\n        future = client.infer_batch(input1, input2)\n        future = client.infer_batch(a=input1, b=input2)\n        ```\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Optional dictionary of inference parameters.\n        headers: Optional dictionary of HTTP headers for the inference request.\n        **named_inputs: Inference inputs provided as named arguments.\n    Returns:\n        A Future object wrapping a dictionary of inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\nreturn self._execute(name=_INFER_BATCH, request=(inputs, parameters, headers, named_inputs))\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.infer_sample","title":"<code>infer_sample(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run asynchronous inference on a single data sample and return a Future object.</p> <p>This method allows the user to perform inference on a single data sample by providing input data and receiving the corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.</p> Example usage <pre><code>with FuturesModelClient(\"localhost\", \"BERT\") as client:\nresult_future = client.infer_sample(input1=input1_data, input2=input2_data)\n# do something else\nprint(result_future.result())\n</code></pre> Inference inputs can be provided either as positional or keyword arguments <pre><code>future = client.infer_sample(input1, input2)\nfuture = client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of HTTP headers for the inference request.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Future</code> <p>A Future object wrapping a dictionary of inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientClosedError</code> <p>If the FuturesModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(\nself,\n*inputs,\nparameters: Optional[Dict[str, Union[str, int, bool]]] = None,\nheaders: Optional[Dict[str, Union[str, int, bool]]] = None,\n**named_inputs,\n) -&gt; Future:\n\"\"\"Run asynchronous inference on a single data sample and return a Future object.\n    This method allows the user to perform inference on a single data sample by providing input data and receiving the\n    corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.\n    Example usage:\n        ```python\n        with FuturesModelClient(\"localhost\", \"BERT\") as client:\n            result_future = client.infer_sample(input1=input1_data, input2=input2_data)\n            # do something else\n            print(result_future.result())\n        ```\n    Inference inputs can be provided either as positional or keyword arguments:\n        ```python\n        future = client.infer_sample(input1, input2)\n        future = client.infer_sample(a=input1, b=input2)\n        ```\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Optional dictionary of inference parameters.\n        headers: Optional dictionary of HTTP headers for the inference request.\n        **named_inputs: Inference inputs provided as named arguments.\n    Returns:\n        A Future object wrapping a dictionary of inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\nreturn self._execute(\nname=_INFER_SAMPLE,\nrequest=(inputs, parameters, headers, named_inputs),\n)\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.model_config","title":"<code>model_config()</code>","text":"<p>Obtain the configuration of the model deployed on the Triton Inference Server.</p> <p>This method returns a Future object that will contain the TritonModelConfig object when it is ready. Client will wait init_timeout_s for the server to get into readiness state before obtaining the model configuration.</p> <p>Returns:</p> Type Description <code>Future</code> <p>A Future object that will contain the TritonModelConfig object when it is ready.</p> <p>Raises:</p> Type Description <code>PyTritonClientClosedError</code> <p>If the FuturesModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def model_config(self) -&gt; Future:\n\"\"\"Obtain the configuration of the model deployed on the Triton Inference Server.\n    This method returns a Future object that will contain the TritonModelConfig object when it is ready.\n    Client will wait init_timeout_s for the server to get into readiness state before obtaining the model configuration.\n    Returns:\n        A Future object that will contain the TritonModelConfig object when it is ready.\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\nreturn self._execute(name=_MODEL_CONFIG)\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.wait_for_model","title":"<code>wait_for_model(timeout_s)</code>","text":"<p>Returns a Future object which result will be None when the model is ready.</p> Typical usage <pre><code>with FuturesModelClient(\"localhost\", \"BERT\") as client\nfuture = client.wait_for_model(300.)\n# do something else\nfuture.result()   # wait rest of timeout_s time\n# till return None if model is ready\n# or raise PyTritonClientTimeutError\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>The maximum amount of time to wait for the model to be ready, in seconds.</p> required <p>Returns:</p> Type Description <code>Future</code> <p>A Future object which result is None when the model is ready.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float) -&gt; Future:\n\"\"\"Returns a Future object which result will be None when the model is ready.\n    Typical usage:\n        ```python\n        with FuturesModelClient(\"localhost\", \"BERT\") as client\n            future = client.wait_for_model(300.)\n            # do something else\n            future.result()   # wait rest of timeout_s time\n                              # till return None if model is ready\n                              # or raise PyTritonClientTimeutError\n        ```\n    Args:\n        timeout_s: The maximum amount of time to wait for the model to be ready, in seconds.\n    Returns:\n        A Future object which result is None when the model is ready.\n    \"\"\"\nreturn self._execute(\nname=_WAIT_FOR_MODEL,\nrequest=timeout_s,\n)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient","title":"<code>ModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>             Bases: <code>BaseModelClient</code></p> <p>Synchronous client for model deployed on the Triton Inference Server.</p> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> <p>Common usage: <pre><code>with ModelClient(\"localhost\", \"BERT\") as client\n    result_dict = client.infer_sample(input1_sample, input2_sample)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> required <code>model_name</code> <code>str</code> <p>name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> <code>None</code> <code>lazy_init</code> <code>bool</code> <p>if initialization should be performed just before sending first request to inference server.</p> <code>True</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>timeout for maximum waiting time in loop, which sends retry requests ask if model is ready. It is applied at initialization time only when <code>lazy_init</code> argument is False. Default is to do retry loop at first inference.</p> <code>None</code> <code>inference_timeout_s</code> <code>Optional[float]</code> <p>timeout in seconds for the model inference process. If non passed default 60 seconds timeout will be used. For HTTP client it is not only inference timeout but any client request timeout - get model config, is model loaded. For GRPC client it is only inference timeout.</p> <code>None</code> <p>Raises:</p> Type Description <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientTimeoutError</code> <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> <code>PyTritonClientUrlParseError</code> <p>In case of problems with parsing url.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\nself,\nurl: str,\nmodel_name: str,\nmodel_version: Optional[str] = None,\n*,\nlazy_init: bool = True,\ninit_timeout_s: Optional[float] = None,\ninference_timeout_s: Optional[float] = None,\n):\n\"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n    Common usage:\n    ```\n    with ModelClient(\"localhost\", \"BERT\") as client\n        result_dict = client.infer_sample(input1_sample, input2_sample)\n    ```\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for maximum waiting time in loop, which sends retry requests ask if model is ready. It is applied at initialization time only when `lazy_init` argument is False. Default is to do retry loop at first inference.\n        inference_timeout_s: timeout in seconds for the model inference process.\n            If non passed default 60 seconds timeout will be used.\n            For HTTP client it is not only inference timeout but any client request timeout\n            - get model config, is model loaded. For GRPC client it is only inference timeout.\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError:\n            if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\nsuper().__init__(\nurl=url,\nmodel_name=model_name,\nmodel_version=model_version,\nlazy_init=lazy_init,\ninit_timeout_s=init_timeout_s,\ninference_timeout_s=inference_timeout_s,\n)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.is_batching_supported","title":"<code>is_batching_supported</code>  <code>property</code>","text":"<p>Checks if model supports batching.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"api/#pytriton.client.client.ModelClient.model_config","title":"<code>model_config: TritonModelConfig</code>  <code>property</code>","text":"<p>Obtain the configuration of the model deployed on the Triton Inference Server.</p> <p>This method waits for the server to get into readiness state before obtaining the model configuration.</p> <p>Returns:</p> Name Type Description <code>TritonModelConfig</code> <code>TritonModelConfig</code> <p>configuration of the model deployed on the Triton Inference Server.</p> <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If the server and model are not in readiness state before the given timeout.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>KeyboardInterrupt</code> <p>If the hosting process receives SIGINT.</p> <code>PyTritonClientClosedError</code> <p>If the ModelClient is closed.</p>"},{"location":"api/#pytriton.client.client.ModelClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Create context for using ModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n\"\"\"Create context for using ModelClient as a context manager.\"\"\"\nreturn self\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.__exit__","title":"<code>__exit__(*_)</code>","text":"<p>Close resources used by ModelClient instance when exiting from the context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, *_):\n\"\"\"Close resources used by ModelClient instance when exiting from the context.\"\"\"\nself.close()\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.close","title":"<code>close()</code>","text":"<p>Close resources used by ModelClient.</p> <p>This method closes the resources used by the ModelClient instance, including the Triton Inference Server connections. Once this method is called, the ModelClient instance should not be used again.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self):\n\"\"\"Close resources used by ModelClient.\n    This method closes the resources used by the ModelClient instance,\n    including the Triton Inference Server connections.\n    Once this method is called, the ModelClient instance should not be used again.\n    \"\"\"\n_LOGGER.debug(\"Closing ModelClient\")\ntry:\nif self._general_client is not None:\nself._general_client.close()\nif self._infer_client is not None:\nself._infer_client.close()\nself._general_client = None\nself._infer_client = None\nexcept Exception as e:\n_LOGGER.warning(\"Error while closing ModelClient resources: %s\", e)\nraise e\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.get_lib","title":"<code>get_lib()</code>","text":"<p>Returns tritonclient library for given scheme.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n\"\"\"Returns tritonclient library for given scheme.\"\"\"\nreturn {\"grpc\": tritonclient.grpc, \"http\": tritonclient.http}[self._scheme.lower()]\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.infer_batch","title":"<code>infer_batch(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run synchronous inference on batched data.</p> Typical usage <pre><code>with ModelClient(\"localhost\", \"MyModel\") as client:\nresult_dict = client.infer_batch(input1, input2)\n</code></pre> Inference inputs can be provided either as positional or keyword arguments <pre><code>result_dict = client.infer_batch(input1, input2)\nresult_dict = client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>Dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>If mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If an error occurred on the inference callable or Triton Inference Server side.</p> <code>PyTritonClientModelDoesntSupportBatchingError</code> <p>If the model doesn't support batching.</p> <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>inference_timeout_s</code> passed to <code>__init__</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side,</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(\nself,\n*inputs,\nparameters: Optional[Dict[str, Union[str, int, bool]]] = None,\nheaders: Optional[Dict[str, Union[str, int, bool]]] = None,\n**named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n\"\"\"Run synchronous inference on batched data.\n    Typical usage:\n        ```python\n        with ModelClient(\"localhost\", \"MyModel\") as client:\n            result_dict = client.infer_batch(input1, input2)\n        ```\n    Inference inputs can be provided either as positional or keyword arguments:\n        ```python\n        result_dict = client.infer_batch(input1, input2)\n        result_dict = client.infer_batch(a=input1, b=input2)\n        ```\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n        PyTritonClientModelDoesntSupportBatchingError: If the model doesn't support batching.\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s` or\n            inference time exceeds `inference_timeout_s` passed to `__init__`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side,\n    \"\"\"\n_verify_inputs_args(inputs, named_inputs)\n_verify_parameters(parameters)\n_verify_parameters(headers)\nif not self.is_batching_supported:\nraise PyTritonClientModelDoesntSupportBatchingError(\nf\"Model {self.model_config.model_name} doesn't support batching - use infer_sample method instead\"\n)\nreturn self._infer(inputs or named_inputs, parameters, headers)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.infer_sample","title":"<code>infer_sample(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run synchronous inference on a single data sample.</p> Typical usage <pre><code>with ModelClient(\"localhost\", \"MyModel\") as client:\nresult_dict = client.infer_sample(input1, input2)\n</code></pre> Inference inputs can be provided either as positional or keyword arguments <pre><code>result_dict = client.infer_sample(input1, input2)\nresult_dict = client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>Dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>If mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If an error occurred on the inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(\nself,\n*inputs,\nparameters: Optional[Dict[str, Union[str, int, bool]]] = None,\nheaders: Optional[Dict[str, Union[str, int, bool]]] = None,\n**named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n\"\"\"Run synchronous inference on a single data sample.\n    Typical usage:\n        ```python\n        with ModelClient(\"localhost\", \"MyModel\") as client:\n            result_dict = client.infer_sample(input1, input2)\n        ```\n    Inference inputs can be provided either as positional or keyword arguments:\n        ```python\n        result_dict = client.infer_sample(input1, input2)\n        result_dict = client.infer_sample(a=input1, b=input2)\n        ```\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n    \"\"\"\n_verify_inputs_args(inputs, named_inputs)\n_verify_parameters(parameters)\n_verify_parameters(headers)\nif self.is_batching_supported:\nif inputs:\ninputs = tuple(data[np.newaxis, ...] for data in inputs)\nelif named_inputs:\nnamed_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\nresult = self._infer(inputs or named_inputs, parameters, headers)\nif self.is_batching_supported:\nresult = {name: data[0] for name, data in result.items()}\nreturn result\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.wait_for_model","title":"<code>wait_for_model(timeout_s)</code>","text":"<p>Wait for the Triton Inference Server and the deployed model to be ready.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>timeout in seconds to wait for the server and model to be ready.</p> required <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If the server and model are not ready before the given timeout.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>KeyboardInterrupt</code> <p>If the hosting process receives SIGINT.</p> <code>PyTritonClientClosedError</code> <p>If the ModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float):\n\"\"\"Wait for the Triton Inference Server and the deployed model to be ready.\n    Args:\n        timeout_s: timeout in seconds to wait for the server and model to be ready.\n    Raises:\n        PyTritonClientTimeoutError: If the server and model are not ready before the given timeout.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        KeyboardInterrupt: If the hosting process receives SIGINT.\n        PyTritonClientClosedError: If the ModelClient is closed.\n    \"\"\"\nif self._general_client is None:\nraise PyTritonClientClosedError(\"ModelClient is closed\")\nwait_for_model_ready(self._general_client, self._model_name, self._model_version, timeout_s=timeout_s)\n</code></pre>"},{"location":"binding_configuration/","title":"Binding Configuration","text":""},{"location":"binding_configuration/#binding-configuration","title":"Binding Configuration","text":"<p>The additional configuration of binding the model for running a model through the Triton Inference Server can be provided in the <code>config</code> argument in the <code>bind</code> method. This section describes the possible configuration enhancements. The configuration of the model can be adjusted by overriding the defaults for the <code>ModelConfig</code> object.</p> <pre><code>from pytriton.model_config.common import DynamicBatcher\nclass ModelConfig:\nbatching: bool = True\nmax_batch_size: int = 4\nbatcher: DynamicBatcher = DynamicBatcher()\nresponse_cache: bool = False\n</code></pre>"},{"location":"binding_configuration/#batching","title":"Batching","text":"<p>The batching feature collects one or more samples and passes them to the model together. The model processes multiple samples at the same time and returns the output for all the samples processed together.</p> <p>Batching can significantly improve throughput. Processing multiple samples at the same time leverages the benefits of utilizing GPU performance for inference.</p> <p>The Triton Inference Server is responsible for collecting multiple incoming requests into a single batch. The batch is passed to the model, which improves the inference performance (throughput and latency). This feature is called <code>dynamic batching</code>, which collects samples from multiple clients into a single batch processed by the model.</p> <p>On the PyTriton side, the <code>infer_fn</code> obtain the fully created batch by Triton Inference Server so the only responsibility is to perform computation and return the output.</p> <p>By default, batching is enabled for the model. The default behavior for Triton is to have dynamic batching enabled. If your model does not support batching, use <code>batching=False</code> to disable it in Triton.</p>"},{"location":"binding_configuration/#maximal-batch-size","title":"Maximal batch size","text":"<p>The maximal batch size defines the number of samples that can be processed at the same time by the model. This configuration has an impact not only on throughput but also on memory usage, as a bigger batch means more data loaded to the memory at the same time.</p> <p>The <code>max_batch_size</code> has to be a value greater than or equal to 1.</p>"},{"location":"binding_configuration/#dynamic-batching","title":"Dynamic batching","text":"<p>The dynamic batching is a Triton Inference Server feature and can be configured by defining the <code>DynamicBatcher</code> object:</p> <pre><code>from typing import Dict, Optional\nfrom pytriton.model_config.common import QueuePolicy\nclass DynamicBatcher:\nmax_queue_delay_microseconds: int = 0\npreferred_batch_size: Optional[list] = None\npreserve_ordering: bool = False\npriority_levels: int = 0\ndefault_priority_level: int = 0\ndefault_queue_policy: Optional[QueuePolicy] = None\npriority_queue_policy: Optional[Dict[int, QueuePolicy]] = None\n</code></pre> <p>More about dynamic batching can be found in the Triton Inference Server documentation and API spec</p>"},{"location":"binding_configuration/#response-cache","title":"Response cache","text":"<p>The Triton Inference Server provides functionality to use a cached response for the model. To use the response cache:</p> <ul> <li>provide the <code>cache_config</code> in <code>TritonConfig</code></li> <li>set <code>response_cache=True</code> in <code>ModelConfig</code></li> </ul> <p>More about response cache can be found in the Triton Response Cache page.</p> <p>Example:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\ntriton_config = TritonConfig(\ncache_config=[f\"local,size={1024 * 1024}\"],  # 1MB\n)\n@batch\ndef _add_sub(**inputs):\na_batch, b_batch = inputs.values()\nadd_batch = a_batch + b_batch\nsub_batch = a_batch - b_batch\nreturn {\"add\": add_batch, \"sub\": sub_batch}\nwith Triton(config=triton_config) as triton:\ntriton.bind(\nmodel_name=\"AddSub\",\ninfer_func=_add_sub,\ninputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\noutputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=8, response_cache=True)\n)\n...\n</code></pre>"},{"location":"binding_models/","title":"Binding Model to Triton","text":""},{"location":"binding_models/#binding-models-to-triton","title":"Binding Models to Triton","text":"<p>The Triton class provides methods to bind one or multiple models to the Triton server in order to expose HTTP/gRPC endpoints for inference serving:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n@batch\ndef infer_fn(**inputs: np.ndarray):\ninput1, input2 = inputs.values()\noutputs = model(input1, input2)\nreturn [outputs]\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"ModelName\",\ninfer_func=infer_fn,\ninputs=[\nTensor(shape=(1,), dtype=np.bytes_),  # sample containing single bytes value\nTensor(shape=(-1,), dtype=np.bytes_)  # sample containing vector of bytes\n],\noutputs=[\nTensor(shape=(-1,), dtype=np.float32),\n],\nconfig=ModelConfig(max_batch_size=8),\nstrict=True,\n)\n</code></pre> <p>The <code>bind</code> method's mandatory arguments are:</p> <ul> <li><code>model_name</code>: defines under which name the model is available in Triton Inference Server</li> <li><code>infer_func</code>: function or Python <code>Callable</code> object which obtains the data passed in the request and returns the output</li> <li><code>inputs</code>: defines the number, types, and shapes for model inputs</li> <li><code>outputs</code>: defines the number, types, and shapes for model outputs</li> <li><code>config</code>: more customization for model deployment and behavior on the Triton server</li> <li><code>strict</code>: enable inference callable output validation of data types and shapes against provided model config</li> </ul> <p>Once the <code>bind</code> method is called, the model is created in the Triton Inference Server model store under the provided <code>model_name</code>.</p>"},{"location":"binding_models/#inference-callable","title":"Inference Callable","text":"<p>The inference callable is an entry point for inference. This can be any callable that receives the data for model inputs in the form of a list of request dictionaries where input names are mapped into ndarrays. Input can be also adapted to different more convenient forms using a set of decorators. More details about designing inference callable and using of decorators can be found in Inference Callable page.</p> <p>In the simplest implementation for functionality that passes input data on output, a lambda can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"Identity\",\ninfer_func=lambda requests: requests,\ninputs=[Tensor(dtype=np.float32, shape=(1,))],\noutputs=[Tensor(dtype=np.float32, shape=(1,))],\nconfig=ModelConfig(max_batch_size=8)\n)\n</code></pre>"},{"location":"binding_models/#multi-instance-model-inference","title":"Multi-instance model inference","text":"<p>Multi-instance model inference is a mechanism for loading multiple instances of the same model and calling them alternately (to hide transfer overhead).</p> <p>With the <code>Triton</code> class, it can be realized by providing the list of multiple inference callables to <code>Triton.bind</code> in the <code>infer_func</code> parameter.</p> <p>The example presents multiple instances of the Linear PyTorch model loaded on separate devices.</p> <p>First, define the wrapper class for the inference handler. The class initialization receives a model and device as arguments. The inference handling is done by method <code>__call__</code> where the <code>model</code> instance is called:</p> <pre><code>import torch\nfrom pytriton.decorators import batch\nclass _InferFuncWrapper:\ndef __init__(self, model: torch.nn.Module, device: str):\nself._model = model\nself._device = device\n@batch\ndef __call__(self, **inputs):\n(input1_batch,) = inputs.values()\ninput1_batch_tensor = torch.from_numpy(input1_batch).to(self._device)\noutput1_batch_tensor = self._model(input1_batch_tensor)\noutput1_batch = output1_batch_tensor.cpu().detach().numpy()\nreturn [output1_batch]\n</code></pre> <p>Next, create a factory function where a model and instances of <code>_InferFuncWrapper</code> are created - one per each device:</p> <pre><code>def _infer_function_factory(devices):\ninfer_fns = []\nfor device in devices:\nmodel = torch.nn.Linear(20, 30).to(device).eval()\ninfer_fns.append(_InferFuncWrapper(model=model, device=device))\nreturn infer_fns\n</code></pre> <p>Finally, the list of callable objects is passed to <code>infer_func</code> parameter of the <code>Triton.bind</code> function:</p> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"Linear\",\ninfer_func=_infer_function_factory(devices=[\"cuda\", \"cpu\"]),\ninputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\noutputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\nconfig=ModelConfig(max_batch_size=16),\n)\n...\n</code></pre> <p>Once the multiple callable objects are passed to <code>infer_func</code>, the Triton server gets information that multiple instances of the same model have been created. The incoming requests are distributed among created instances. In our case executing two instances of a <code>Linear</code> model loaded on CPU and GPU devices.</p>"},{"location":"binding_models/#defining-inputs-and-outputs","title":"Defining Inputs and Outputs","text":"<p>The integration of the Python model requires the inputs and outputs types of the model. This is required to correctly map the input and output data passed through the Triton Inference Server.</p> <p>The simplest definition of model inputs and outputs expects providing the type of data and the shape per input:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\ninputs = [\nTensor(dtype=np.float32, shape=(-1,)),\n]\noutput = [\nTensor(dtype=np.float32, shape=(-1,)),\nTensor(dtype=np.int32, shape=(-1,)),\n]\n</code></pre> <p>The provided configuration creates the following tensors:</p> <ul> <li>Single input:</li> <li>name: INPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>Two outputs:</li> <li>name: OUTPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>name: OUTPUT_2, data type: INT32, shape=(-1,)</li> </ul> <p>The <code>-1</code> means a dynamic shape of the input or output.</p> <p>To define the name of the input and its exact shape, the following definition can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\ninputs = [\nTensor(name=\"image\", dtype=np.float32, shape=(224, 224, 3)),\n]\noutputs = [\nTensor(name=\"class\", dtype=np.int32, shape=(1000,)),\n]\n</code></pre> <p>This definition describes that the model has:</p> <ul> <li>a single input named <code>image</code> of size 224x224x3 and 32-bit floating-point data type</li> <li>a single output named <code>class</code> of size 1000 and 32-bit integer data type.</li> </ul> <p>The <code>dtype</code> parameter can be either <code>numpy.dtype</code>, <code>numpy.dtype.type</code>, or <code>str</code>. For example:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\ntensor1 = Tensor(name=\"tensor1\", shape=(-1,), dtype=np.float32),\ntensor2 = Tensor(name=\"tensor2\", shape=(-1,), dtype=np.float32().dtype),\ntensor3 = Tensor(name=\"tensor3\", shape=(-1,), dtype=\"float32\"),\n</code></pre> <p>dtype for bytes and string inputs/outputs</p> <p>When using the <code>bytes</code> dtype, NumPy removes trailing <code>\\x00</code> bytes. Therefore, for arbitrary bytes, it is required to use <code>object</code> dtype.</p> <pre><code>&gt; np.array([b\"\\xff\\x00\"])\narray([b'\\xff'], dtype='|S2')\n\n&gt; np.array([b\"\\xff\\x00\"], dtype=object)\narray([b'\\xff\\x00'], dtype=object)\n</code></pre> <p>For ease of use, for encoded string values, users might use <code>bytes</code> dtype.</p>"},{"location":"binding_models/#throwing-unrecoverable-errors","title":"Throwing Unrecoverable errors","text":"<p>When the model gets into a state where further inference is impossible, you can throw PyTritonUnrecoverableError from the inference callable. This will cause NVIDIA Triton Inference Server to shut down. This might be useful when the model is deployed on a cluster in a multi-node setup. In that case to recover the model you need to restart all \"workers\" on the cluster.</p> <p>When the model gets into a state where further inference is impossible, you can throw the PyTritonUnrecoverableError from the inference callable. This will cause the NVIDIA Triton Inference Server to shut down. This might be useful when the model is deployed on a cluster in a multi-node setup. In that case, to recover the model, you need to restart all \"workers\" on the cluster.</p> <pre><code>from typing import Dict\nimport numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.exceptions import PyTritonUnrecoverableError\n@batch\ndef infer_fn(**inputs: np.ndarray) -&gt; Dict[str, np.ndarray]:\n...\ntry:\noutputs = model(**inputs)\nexcept Exception as e:\nraise PyTritonUnrecoverableError(\n\"Some unrecoverable error occurred, \"\n\"thus no further inferences are possible.\"\n) from e\n...\nreturn outputs\n</code></pre>"},{"location":"building/","title":"Building binary package","text":""},{"location":"building/#building-binary-package-from-source","title":"Building binary package from source","text":"<p>This guide provides an outline of the process for building the PyTriton binary package from source. It offers the flexibility to modify the PyTriton code and integrate it with various versions of the Triton Inference Server, including custom builds. Additionally, it allows you to incorporate hotfixes that have not yet been officially released.</p>"},{"location":"building/#prerequisites","title":"Prerequisites","text":"<p>Before building the PyTriton binary package, ensure the following:</p> <ul> <li>Docker is installed on the system. For more information, refer to the Docker documentation.</li> <li>Access to the Docker daemon is available from the system or container.</li> </ul>"},{"location":"building/#building-pytriton-binary-package","title":"Building PyTriton binary package","text":"<p>To build the wheel binary package, follow these steps from the root directory of the project:</p> <pre><code>make install-dev\nmake dist\n</code></pre> <p>The wheel package will be located in the <code>dist</code> directory. To install the library, run the following <code>pip</code> command:</p> <pre><code>pip install dist/nvidia_pytriton-*-py3-none-*_x86_64.whl\n</code></pre>"},{"location":"building/#building-for-a-specific-triton-inference-server-version","title":"Building for a specific Triton Inference Server version","text":"<p>Building for an unsupported OS or hardware platform is possible. PyTriton requires a Python backend and either an HTTP or gRPC endpoint. The build can be CPU-only, as inference is performed on Inference Handlers.</p> <p>For more information on the Triton Inference Server build process, refer to the building section of Triton Inference Server documentation.</p> <p>Untested Build</p> <p>The Triton Inference Server has only been rigorously tested on Ubuntu 20.04. Other OS and hardware platforms are not officially supported. You can test the build by following the steps outlined in the Triton Inference Server testing guide.</p> <p>Using the following docker method steps, you can create a <code>tritonserver:latest</code> Docker image that can be used to build PyTriton with the following command:</p> <p>By the following docker method steps you can create a <code>tritonserver:latest</code> Docker image that can be used to build PyTriton with the following command:</p> <pre><code>make TRITONSERVER_IMAGE_NAME=tritonserver:latest dist\n</code></pre>"},{"location":"clients/","title":"Clients","text":""},{"location":"clients/#triton-clients","title":"Triton clients","text":"<p>The prerequisite for this page is to install PyTriton. You also need <code>Linear</code> model described in quick_start. You should run it so client can connect to it.</p> <p>The clients section presents how to send requests to the Triton Inference Server using the PyTriton library.</p>"},{"location":"clients/#modelclient","title":"ModelClient","text":"<p>ModelClient is a simple client that can perform inference requests synchronously. You can use ModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the ModelClient object.</p> <p>For example, you can use ModelClient to send requests to a PyTorch model that performs linear regression:</p> <pre><code>import torch\nfrom pytriton.client import ModelClient\n# Create some input data as a numpy array\ninput1_data = torch.randn(128, 2).cpu().detach().numpy()\n# Create a ModelClient object with the server address and model name\nwith ModelClient(\"localhost:8000\", \"Linear\") as client:\n# Call the infer_batch method with the input data\nresult_dict = client.infer_batch(input1_data)\n# Print the result dictionary\nprint(result_dict)\n</code></pre> <p>You can also use ModelClient to send requests to a model that performs image classification. The example assumes that a model takes in an image and returns the top 5 predicted classes. This model is not included in the PyTriton library.</p> <p>You need to convert the image to a numpy array and resize it to the expected input shape. You can use Pillow package to do this.</p> <pre><code>import numpy as np\nfrom PIL import Image\nfrom pytriton.client import ModelClient\n# Create some input data as a numpy array of an image\nimg = Image.open(\"cat.jpg\")\nimg = img.resize((224, 224))\ninput_data = np.array(img)\n# Create a ModelClient object with the server address and model name\nwith ModelClient(\"localhost:8000\", \"ImageNet\") as client:\n# Call the infer_batch method with the input data\nresult_dict = client.infer_sample(input_data)\n# Print the result dictionary\nprint(result_dict)\n</code></pre> <p>You need to install Pillow package to run the above example: <pre><code>pip install Pillow\n</code></pre></p>"},{"location":"clients/#futuresmodelclient","title":"FuturesModelClient","text":"<p>FuturesModelClient is a concurrent.futures based client that can perform inference requests in a parallel way. You can use FuturesModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the FuturesModelClient object.</p> <p>For example, you can use FuturesModelClient to send multiple requests to a text generation model that takes in text prompts and returns generated texts. The TextGen model is not included in the PyTriton library. The example assumes that the model returns a single output tensor with the generated text. The example also assumes that the model takes in a list of text prompts and returns a list of generated texts.</p> <p>You need to convert the text prompts to numpy arrays of bytes using a tokenizer from transformers. You also need to detokenize the output texts using the same tokenizer:</p> <pre><code>import numpy as np\nfrom pytriton.client import FuturesModelClient\nfrom transformers import AutoTokenizer\n# Create some input data as a list of text prompts\ninput_data_list_text = [\"Write a haiku about winter.\", \"Summarize the article below in one sentence.\", \"Generate a catchy slogan for PyTriton.\"]\n# Create a tokenizer from transformers\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n# Convert the text prompts to numpy arrays of bytes using the tokenizer\ninput_data_list = [np.array(tokenizer.encode(prompt)) for prompt in input_data_list_text]\n# Create a FuturesModelClient object with the server address and model name\nwith FuturesModelClient(\"localhost:8000\", \"TextGen\") as client:\n# Call the infer_sample method for each input data in the list and store the returned futures\noutput_data_futures = [client.infer_sample(input_data) for input_data in input_data_list]\n# Wait for all the futures to complete and get the results\noutput_data_list = [output_data_future.result() for output_data_future in output_data_futures]\n# Print tokens\nprint(output_data_list)\n# Detokenize the output texts using the tokenizer and print them\noutput_texts = [tokenizer.decode(output_data[\"OUTPUT_1\"]) for output_data in output_data_list]\nfor output_text in output_texts:\nprint(output_text)\n</code></pre> <p>You need to install transformers package to run the above example: <pre><code>pip install transformers\n</code></pre></p> <p>You can also use FuturesModelClient to send multiple requests to an image classification model that takes in image data and returns class labels or probabilities. The ImageNet model is described above.</p> <p>In this case, you can use the infer_batch method to send a batch of images as input and get a batch of outputs. You need to stack the images along the first dimension to form a batch. You can also print the class names corresponding to the output labels:</p> <pre><code>import numpy as np\nfrom PIL import Image\nfrom pytriton.client import FuturesModelClient\n# Create some input data as a list of lists of image arrays\ninput_data_list = []\nfor batch in [[\"cat.jpg\", \"dog.jpg\", \"bird.jpg\"], [\"car.jpg\", \"bike.jpg\", \"bus.jpg\"], [\"apple.jpg\", \"banana.jpg\", \"orange.jpg\"]]:\nbatch_data = []\nfor filename in batch:\nimg = Image.open(filename)\nimg = img.resize((224, 224))\nimg = np.array(img)\nbatch_data.append(img)\n# Stack the images along the first dimension to form a batch\nbatch_data = np.stack(batch_data, axis=0)\ninput_data_list.append(batch_data)\n# Create a list of class names for ImageNet\nclass_names = [\"tench\", \"goldfish\", \"great white shark\", ...]\n# Create a FuturesModelClient object with the server address and model name\nwith FuturesModelClient(\"localhost:8000\", \"ImageNet\") as client:\n# Call the infer_batch method for each input data in the list and store the returned futures\noutput_data_futures = [client.infer_batch(input_data) for input_data in input_data_list]\n# Wait for all the futures to complete and get the results\noutput_data_list = [output_data_future.result() for output_data_future in output_data_futures]\n# Print the list of result dictionaries\nprint(output_data_list)\n# Print the class names corresponding to the output labels for each batch\nfor output_data in output_data_list:\noutput_labels = output_data[\"OUTPUT_1\"]\nfor output_label in output_labels:\nclass_name = class_names[output_label]\nprint(f\"The image is classified as {class_name}.\")\n</code></pre>"},{"location":"clients/#asynciomodelclient","title":"AsyncioModelClient","text":"<p>AsyncioModelClient is an asynchronous client that can perform inference requests using the asyncio library. You can use AsyncioModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the AsyncioModelClient object.</p> <p>For example, you can use AsyncioModelClient to send requests to a PyTorch model that performs linear regression:</p> <pre><code>import torch\nfrom pytriton.client import AsyncioModelClient\n# Create some input data as a numpy array\ninput1_data = torch.randn(2).cpu().detach().numpy()\n# Create an AsyncioModelClient object with the server address and model name\nasync with AsyncioModelClient(\"localhost:8000\", \"Linear\") as client:\n# Call the infer_sample method with the input data\nresult_dict = await client.infer_sample(input1_data)\n# Print the result dictionary\nprint(result_dict)\n</code></pre> <p>You can also use FastAPI to create a web application that exposes the results of inference at an HTTP endpoint. FastAPI is a modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.</p> <p>To use FastAPI, you need to install it with:</p> <pre><code>pip install fastapi\n</code></pre> <p>You also need an ASGI server, for production such as Uvicorn or Hypercorn.</p> <p>To install Uvicorn, run:</p> <pre><code>pip install uvicorn[standard]\n</code></pre> <p>The <code>uvicorn</code> uses port <code>8000</code> as default for web server. Triton server default port is also <code>8000</code> for HTTP protocol. You can change uvicorn port by using <code>--port</code> option. PyTriton also supports custom ports configuration for Triton server. The class <code>TritonConfig</code> contains parameters for ports configuration. You can pass it to <code>Triton</code> during initialization:</p> <pre><code>config = TritonConfig(http_port=8015)\ntriton_server = Triton(config=config)\n</code></pre> <p>You can use this <code>triton_server</code> object to bind your inference model and run HTTP endpoint from Triton Inference Server at port <code>8015</code>.</p> <p>Then you can create a FastAPI app that uses the AsyncioModelClient to perform inference and return the results as JSON:</p> <pre><code>from fastapi import FastAPI\nimport torch\nfrom pytriton.client import AsyncioModelClient\napp = FastAPI()\n@app.get(\"/predict\")\nasync def predict():\n# Create some input data as a numpy array\ninput1_data = torch.randn(2).cpu().detach().numpy()\n# Create an AsyncioModelClient object with the server address and model name\nasync with AsyncioModelClient(\"localhost:8000\", \"Linear\") as client:\n# Call the infer_sample method with the input data\nresult_dict = await client.infer_sample(input1_data)\n# Return the result dictionary as JSON\nreturn result_dict\n</code></pre> <p>Save this file as <code>main.py</code>.</p> <p>To run the app, use the command:</p> <pre><code>uvicorn main:app --reload --port 8015\n</code></pre> <p>You can then access the endpoint at <code>http://127.0.0.1:8015/predict</code> and see the JSON response.</p> <p>You can also check the interactive API documentation at <code>http://127.0.0.1:8015/docs</code>.</p> <p>You can test your server using curl:</p> <pre><code>curl -X 'GET' \\\n'http://127.0.0.1:8015/predict' \\\n-H 'accept: application/json'\n</code></pre> <p>Command will print three random numbers:</p> <pre><code>[-0.2608422636985779,-0.6435106992721558,-0.3492531180381775]\n</code></pre> <p>For more information about FastAPI and Uvicorn, check out these links:</p> <ul> <li>FastAPI documentation</li> <li>Uvicorn documentation</li> </ul>"},{"location":"clients/#client-timeouts","title":"Client timeouts","text":"<p>When creating a ModelClient or FuturesModelClient object, you can specify the timeout for waiting until the server and model are ready using the <code>init_timeout_s</code> parameter. By default, the timeout is set to 5 minutes (300 seconds).</p> <p>Example usage:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\ninput1_data = np.random.randn(128, 2)\nwith ModelClient(\"localhost\", \"MyModel\", init_timeout_s=120) as client:\n# Raises PyTritonClientTimeoutError if the server or model is not ready within the specified timeout\nresult_dict = client.infer_batch(input1_data)\nwith FuturesModelClient(\"localhost\", \"MyModel\", init_timeout_s=120) as client:\nfuture = client.infer_batch(input1_data)\n...\n# It will raise `PyTritonClientTimeoutError` if the server is not ready and the model is not loaded within 120 seconds\n# from the time `infer_batch` was called by a thread from `ThreadPoolExecutor`\nresult_dict = future.result()\n</code></pre> <p>You can disable the default behavior of waiting for the server and model to be ready during first inference request by setting <code>lazy_init</code> to <code>False</code>:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\ninput1_data = np.random.randn(128, 2)\n# will raise PyTritonClientTimeoutError if server is not ready and model loaded\n# within 120 seconds during intialization of client\nwith ModelClient(\"localhost\", \"MyModel\", init_timeout_s=120, lazy_init=False) as client:\nresult_dict = client.infer_batch(input1_data)\n</code></pre> <p>You can specify the timeout for the client to wait for the inference response from the server. The default timeout is 60 seconds. You can specify the timeout when creating the ModelClient or FuturesModelClient object:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\ninput1_data = np.random.randn(128, 2)\nwith ModelClient(\"localhost\", \"MyModel\", inference_timeout_s=240) as client:\n# Raises `PyTritonClientTimeoutError` if the server does not respond to inference request within 240 seconds\nresult_dict = client.infer_batch(input1_data)\nwith FuturesModelClient(\"localhost\", \"MyModel\", inference_timeout_s=240) as client:\nfuture = client.infer_batch(input1_data)\n...\n# Raises `PyTritonClientTimeoutError` if the server does not respond within 240 seconds\n# from the time `infer_batch` was called by a thread from `ThreadPoolExecutor`\nresult_dict = future.result()\n</code></pre> <p>gRPC client timeout not fully supported</p> <p>There are some missing features in the gRPC client that prevent it from working correctly with timeouts used during the wait for the server and model to be ready. This may cause the client to hang if the server doesn't respond with the current server or model state.</p> <p>Server side timeout not implemented</p> <p>Currently, there is no support for server-side timeout. The server will continue to process the request even if the client timeout is reached.</p>"},{"location":"custom_params/","title":"Custom parameters/headers","text":""},{"location":"custom_params/#custom-httpgrpc-headers-and-parameters","title":"Custom HTTP/gRPC headers and parameters","text":"<p>This document provides guidelines for using custom HTTP/gRPC headers and parameters with PyTriton. Original Triton documentation related to parameters can be found here. Now, undecorated inference function accepts list of Request instances. Request class contains following fields: - data - for inputs (stored as dictionary, but can be also accessed with request dict interface e.g. request[\"input_name\"]) - parameters - for combined parameters and HTTP/gRPC headers</p> <p>Parameters/headers usage limitations</p> <p>Currently, custom parameters and headers can be only accessed in undecorated inference function (they don't work with decorators). There is separate example how to use parameters/headers in preprocessing step (see here)</p>"},{"location":"custom_params/#parameters","title":"Parameters","text":"<p>Parameters are passed to the inference callable as a dictionary. The dictionary is stored in HTTP/gRPC request body payload.</p>"},{"location":"custom_params/#httpgrpc-headers","title":"HTTP/gRPC headers","text":"<p>Custom HTTP/gRPC headers are passed to the inference callable in the same dictionary as parameters, but they are stored in HTTP/gRPC request headers instead of the request body payload. For the headers it is also necessary to specify the header prefix in Triton config, which is used to distinguish  the custom headers from standard ones (only headers with specified prefix are passed to the inference callable).</p>"},{"location":"custom_params/#usage","title":"Usage","text":"<ol> <li>Define inference callable (that one uses one parameter and one header):</li> </ol> <pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\ndef _infer_with_params_and_headers(requests):\nresponses = []\nfor req in requests:\na_batch, b_batch = req.values()\nscaled_add_batch = (a_batch + b_batch) / float(req.parameters[\"header_divisor\"])\nscaled_sub_batch = (a_batch - b_batch) * float(req.parameters[\"parameter_multiplier\"])\nresponses.append({\"scaled_add\": scaled_add_batch, \"scaled_sub\": scaled_sub_batch})\nreturn responses\n</code></pre> <ol> <li>Bind inference callable to Triton (\"header\" is the prefix for custom headers):</li> </ol> <pre><code>with Triton(config=TritonConfig(http_header_forward_pattern=\"header.*\")) as triton:\ntriton.bind(\nmodel_name=\"ParamsAndHeaders\",\ninfer_func=_infer_with_params_and_headers,\ninputs=[\nTensor(dtype=np.float32, shape=(-1,)),\nTensor(dtype=np.float32, shape=(-1,)),\n],\noutputs=[\nTensor(name=\"scaled_add\", dtype=np.float32, shape=(-1,)),\nTensor(name=\"scaled_sub\", dtype=np.float32, shape=(-1,)),\n],\nconfig=ModelConfig(max_batch_size=128),\n)\n</code></pre> <pre><code>    triton.serve()\n</code></pre> <ol> <li>Call the model using ModelClient:</li> </ol> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient\nbatch_size = 2\na_batch = np.ones((batch_size, 1), dtype=np.float32) * 2\nb_batch = np.ones((batch_size, 1), dtype=np.float32)\n</code></pre> <pre><code>with ModelClient(\"localhost\", \"ParamsAndHeaders\") as client:\nresult_batch = client.infer_batch(a_batch, b_batch, parameters={\"parameter_multiplier\": 2}, headers={\"header_divisor\": 3})\n</code></pre>"},{"location":"decorators/","title":"Decorators","text":""},{"location":"decorators/#decorators","title":"Decorators","text":"<p>The PyTriton provide decorators for operations on input requests to simplify passing the requests to the model inputs. We have prepared several useful decorators for converting generic request input into common user needs. You can create custom decorators tailored to your requirements and chain them with other decorators.</p>"},{"location":"decorators/#batch","title":"Batch","text":"<p>In many cases, it is more convenient to receive input already batched in the form of a NumPy array instead of a list of separate requests. For such cases, we have prepared the <code>@batch</code> decorator that adapts generic input into a batched form. It passes kwargs to the inference function where each named input contains a NumPy array with a batch of requests received by the Triton server.</p> <p>Below, we show the difference between decorated and undecorated functions bound with Triton:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.proxy.types import Request\n# Sample input data with 2 requests - each with 2 inputs\ninput_data = [\nRequest({'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])}),\nRequest({'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])})\n]\ndef undecorated_identity_fn(requests):\nprint(requests)\n# As expected, requests = [\n#     Request({'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])}),\n#     Request({'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])}),\n# ]\nresults = requests\nreturn results\n@batch\ndef decorated_identity_fn(in1, in2):\nprint(in1, in2)\n# in1 = np.array([[1, 1], [1, 2]])\n# in2 = np.array([[2, 2], [2, 3]])\n# Inputs are batched by `@batch` decorator and passed to the function as kwargs, so they can be automatically mapped\n# with in1, in2 function parameters\n# Of course, we could get these params explicitly with **kwargs like this:\n# def decorated_infer_fn(**kwargs):\nreturn {\"out1\": in1, \"out2\": in2}\nundecorated_identity_fn(input_data)\ndecorated_identity_fn(input_data)\n</code></pre> <p>More examples using the <code>@batch</code> decorator with different frameworks are shown below.</p> <p>Example implementation for TensorFlow model:</p> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom pytriton.decorators import batch\n@batch\ndef infer_tf_fn(**inputs: np.ndarray):\n(images_batch,) = inputs.values()\nimages_batch_tensor = tf.convert_to_tensor(images_batch)\noutput1_batch = model.predict(images_batch_tensor)\nreturn [output1_batch]\n</code></pre> <p>Example implementation for PyTorch model:</p> <pre><code>import numpy as np\nimport torch\nfrom pytriton.decorators import batch\n@batch\ndef infer_pt_fn(**inputs: np.ndarray):\n(input1_batch,) = inputs.values()\ninput1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\noutput1_batch_tensor = model(input1_batch_tensor)\noutput1_batch = output1_batch_tensor.cpu().detach().numpy()\nreturn [output1_batch]\n</code></pre> <p>Example implementation with named inputs and outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n@batch\ndef add_subtract_fn(a: np.ndarray, b: np.ndarray):\nreturn {\"add\": a + b, \"sub\": a - b}\n@batch\ndef multiply_fn(**inputs: np.ndarray):\na = inputs[\"a\"]\nb = inputs[\"b\"]\nreturn [a * b]\n</code></pre> <p>Example implementation with strings:</p> <pre><code>import numpy as np\nfrom transformers import pipeline\nfrom pytriton.decorators import batch\nCLASSIFIER = pipeline(\"zero-shot-classification\", model=\"facebook/bart-base\", device=0)\n@batch\ndef classify_text_fn(text_array: np.ndarray):\ntext = text_array[0]  # text_array contains one string at index 0\ntext = text.decode(\"utf-8\")  # string is stored in byte array encoded in utf-8\nresult = CLASSIFIER(text)\nreturn [np.array(result)]  # return statistics generated by classifier\n</code></pre>"},{"location":"decorators/#sample","title":"Sample","text":"<p><code>@sample</code> - takes the first request and converts it into named inputs. This decorator is useful with non-batching models. Instead of a one-element list of requests, we get named inputs - <code>kwargs</code>.</p> <pre><code>from pytriton.decorators import sample\n@sample\ndef infer_fn(sequence):\npass\n</code></pre>"},{"location":"decorators/#group-by-keys","title":"Group by keys","text":"<p><code>@group_by_keys</code> - groups requests with the same set of keys and calls the wrapped function for each group separately. This decorator is convenient to use before batching because the batching decorator requires a consistent set of inputs as it stacks them into batches.</p> <pre><code>from pytriton.decorators import batch, group_by_keys\n@group_by_keys\n@batch\ndef infer_fn(mandatory_input, optional_input=None):\n# perform inference\npass\n</code></pre>"},{"location":"decorators/#group-by-values","title":"Group by values","text":"<p><code>@group_by_values(*keys)</code> - groups requests with the same input value (for selected keys) and calls the wrapped function for each group separately. This decorator is particularly useful with models requiring dynamic parameters sent by users, such as temperature. In this case, we want to run the model only for requests with the same temperature value.</p> <pre><code>from pytriton.decorators import batch, group_by_values\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n# perform inference\npass\n</code></pre>"},{"location":"decorators/#fill-optionals","title":"Fill optionals","text":"<p><code>@fill_optionals(**defaults)</code> - fills missing inputs in requests with default values provided by the user. If model owners have default values for some optional parameters, it's a good idea to provide them at the beginning, so other decorators can create larger consistent groups and send them to the inference callable.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, group_by_values\n@fill_optionals(temperature=np.array([10.0]))\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n# perform inference\npass\n</code></pre> <p>``</p>"},{"location":"decorators/#pad-batch","title":"Pad batch","text":"<p><code>@pad_batch</code> - appends the last row to the input multiple times to achieve the desired batch size (preferred batch size or max batch size from the model config, whichever is closer to the current input size).</p> <pre><code>from pytriton.decorators import batch, pad_batch\n@batch\n@pad_batch\ndef infer_fn(mandatory_input):\n# this model requires mandatory_input batch to be the size provided in the model config\npass\n</code></pre>"},{"location":"decorators/#first-value","title":"First value","text":"<p><code>@first_value</code> - this decorator takes the first elements from batches for selected inputs specified by the <code>keys</code> parameter. If the value is a one-element array, it is converted to a scalar value. This decorator is convenient to use with dynamic model parameters that users send in requests. You can use <code>@group_by_values</code> before to have batches with the same values in each batch.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_values\n@fill_optionals(temperature=np.array([10.0]))\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\ndef infer_fn(mandatory_input, temperature):\n# perform inference with scalar temperature=10\npass\n</code></pre>"},{"location":"decorators/#triton-context","title":"Triton context","text":"<p>The <code>@triton_context</code> decorator provides an additional argument called <code>triton_context</code>, from which you can read the model config.</p> <p>```python  from pytriton.decorators import triton_context</p> <p>@triton_context def infer_fn(input_list, **kwargs):     model_config = kwargs['triton_context'].model_config     # perform inference using some information from model_config     pass  ```</p>"},{"location":"decorators/#stacking-multiple-decorators","title":"Stacking multiple decorators","text":"<p>Here is an example of stacking multiple decorators together. We recommend starting with type 1 decorators, followed by types 2 and 3. Place the <code>@triton_context</code> decorator last in the chain.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_keys, group_by_values, triton_context\n@fill_optionals(temperature=np.array([10.0]))\n@group_by_keys\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\n@triton_context\ndef infer(triton_context, mandatory_input, temperature, opt1=None, opt2=None):\nmodel_config = triton_context.model_config\n# perform inference using:\n#   - some information from model_config\n#   - scalar temperature value\n#   - optional parameters opt1 and/or opt2\n</code></pre>"},{"location":"deploying_in_clusters/","title":"Deploying in Clusters","text":""},{"location":"deploying_in_clusters/#deploying-in-cluster","title":"Deploying in Cluster","text":"<p>The library can be used inside containers and deployed on Kubernetes clusters. There are certain prerequisites and information that would help deploy the library in your cluster.</p>"},{"location":"deploying_in_clusters/#health-checks","title":"Health checks","text":"<p>The library uses the Triton Inference Server to handle HTTP/gRPC requests. Triton Server provides endpoints to validate if the server is ready and in a healthy state. The following API endpoints can be used in your orchestrator to control the application ready and live states:</p> <ul> <li>Ready: <code>/v2/health/ready</code></li> <li>Live: <code>/v2/health/live</code></li> </ul>"},{"location":"deploying_in_clusters/#exposing-ports","title":"Exposing ports","text":"<p>The library uses the Triton Inference Server, which exposes the HTTP, gRPC, and metrics ports for communication. In the default configuration, the following ports have to be exposed:</p> <ul> <li>8000 for HTTP</li> <li>8001 for gRPC</li> <li>8002 for metrics</li> </ul> <p>If the library is inside a Docker container, the ports can be exposed by passing an extra argument to the <code>docker run</code> command. An example of passing ports configuration:</p> <pre><code>docker run -p 8000:8000 -p 8001:8001 -p 8002:8002 {image}\n</code></pre> <p>To deploy a container in Kubernetes, add a ports definition for the container in YAML deployment configuration:</p> <pre><code>containers:\n- name: pytriton\n...\nports:\n- containerPort: 8000\nname: http\n- containerPort: 8001\nname: grpc\n- containerPort: 8002\nname: metrics\n</code></pre>"},{"location":"deploying_in_clusters/#configuring-shared-memory","title":"Configuring shared memory","text":"<p>The connection between Python callbacks and the Triton Inference Server uses shared memory to pass data between the processes. In the Docker container, the default amount of shared memory is <code>64MB</code>, which may not be enough to pass input and output data of the model. The PyTriton initialize <code>16MB</code> of shared memory for <code>Proxy Backend</code> at start to pass input/output tensors between processes. The additional memory is allocated dynamically. In case of failure, the size of available shared memory might need to be increased.</p> <p>To increase the available shared memory size, pass an additional flag to the <code>docker run</code> command. An example of increasing the shared memory size to 8GB:</p> <p><pre><code>docker run --shm-size 8GB {image}\n</code></pre> To increase the shared memory size for Kubernetes, the following configuration can be used:</p> <pre><code>spec:\nvolumes:\n- name: shared-memory\nemptyDir:\nmedium: Memory\ncontainers:\n- name: pytriton\n...\nvolumeMounts:\n- mountPath: /dev/shm\nname: shared-memory\n</code></pre>"},{"location":"deploying_in_clusters/#specify-container-init-process","title":"Specify container init process","text":"<p>You can use the <code>--init</code> flag of the <code>docker run</code> command to indicate that an init process should be used as the PID 1 in the container. Specifying an init process ensures that reaping zombie processes are performed inside the container. The reaping zombie processes functionality is important in case of an unexpected error occurrence in scripts hosting PyTriton.</p>"},{"location":"downloaded_input_data/","title":"Example with downloaded input data","text":""},{"location":"downloaded_input_data/#example-with-downloaded-input-data","title":"Example with downloaded input data","text":"<p>In the following example, we will demonstrate how to effectively utilize PyTriton with downloaded input data. While the model itself does not possess any inputs, it utilize custom parameters or headers to extract a URL and download data from an external source, such as an S3 bucket.</p> <p>The corresponding function can leverage the batch decorator since it does not rely on any parameters or headers.</p>"},{"location":"downloaded_input_data/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\n@batch\ndef model_infer_function(**inputs):\n...\ndef request_infer_function(requests):\nfor request in requests:\nimage_url = request.parameters[\"custom_url\"]\nimage_jpeg = download(image_url)\nimage_data = decompress(image_jpeg)\nrequest['images_data'] = image_data\noutputs = model_infer_function(requests)\nreturn outputs\nwith Triton(config=TritonConfig(http_header_forward_pattern=\"custom.*\")) as triton:\ntriton.bind(\nmodel_name=\"ImgModel\",\ninfer_func=request_infer_function,\ninputs=[],\noutputs=[Tensor(name=\"out\", dtype=np.float32, shape=(-1,))],\nconfig=ModelConfig(max_batch_size=128),\n)\ntriton.serve()\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide simple examples on how to integrate PyTorch, TensorFlow2, JAX, and simple Python models with the Triton Inference Server using PyTriton. The examples are available in the GitHub repository.</p>"},{"location":"examples/#samples-models-deployment","title":"Samples Models Deployment","text":"<p>The list of example models deployments:</p> <ul> <li>Add-Sub Python model</li> <li>Add-Sub Python model Jupyter Notebook</li> <li>BART PyTorch from HuggingFace</li> <li>BERT JAX from HuggingFace</li> <li>Identity Python model</li> <li>Linear RAPIDS/CuPy model</li> <li>Linear RAPIDS/CuPy model Jupyter Notebook</li> <li>Linear PyTorch model</li> <li>Multi-Layer TensorFlow2</li> <li>Multi Instance deployment for Linear PyTorch model</li> <li>Multi Model deployment for Python models</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> <li>Stable Diffusion 1.5 from HuggingFace</li> <li>Using custom HTTP/gRPC headers and parameters</li> </ul>"},{"location":"examples/#profiling-models","title":"Profiling models","text":"<p>The Perf Analyzer can be used to profile the models served through PyTriton. We have prepared an example of using Perf Analyzer to profile BART PyTorch. See the example code in the GitHub repository.</p>"},{"location":"examples/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>The following examples contain a guide on how to deploy them on a Kubernetes cluster:</p> <ul> <li>BART PyTorch from HuggingFace</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> <li>Stable Diffusion 1.5 from HuggingFace</li> </ul>"},{"location":"inference_callable/","title":"Inference Callable","text":""},{"location":"inference_callable/#inference-callable","title":"Inference Callable","text":"<p>This document provides guidelines for creating an inference callable for PyTriton, which serves as the entry point for handling inference requests.</p> <p>The inference callable is an entry point for handling inference requests. The interface of the inference callable assumes it receives a list of requests with input dictionaries, where each dictionary represents one request mapping model input names to NumPy ndarrays. Requests contain also custom HTTP/gRPC headers and parameters in parameters dictionary.</p>"},{"location":"inference_callable/#function","title":"Function","text":"<p>The simples inference callable is a function that implement the interface to handle request and responses. Request class contains following fields: - data - for inputs (stored as dictionary, but can be also accessed with request dict interface e.g. request[\"input_name\"]) - parameters - for combined parameters and HTTP/gRPC headers For more information about parameters and headers see here.</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nfrom pytriton.proxy.types import Request\ndef infer_fn(requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n...\n</code></pre>"},{"location":"inference_callable/#class","title":"Class","text":"<p>In many cases is worth to use an object of given class as callable. This is especially useful when you want to have a control over the order of initialized objects or models.</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nfrom pytriton.proxy.types import Request\nclass InferCallable:\ndef __call__(self, requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n...\n</code></pre>"},{"location":"inference_callable/#binding-to-triton","title":"Binding to Triton","text":"<p>To use the inference callable with PyTriton, it must be bound to a Triton server instance using the <code>bind</code> method:</p> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"MyInferenceFn\",\ninfer_func=infer_fn,\ninputs=[Tensor(shape=(1,), dtype=np.float32)],\noutputs=[Tensor(shape=(1,), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=8)\n)\ninfer_callable = InferCallable()\ntriton.bind(\nmodel_name=\"MyInferenceCallable\",\ninfer_func=infer_callable,\ninputs=[Tensor(shape=(1,), dtype=np.float32)],\noutputs=[Tensor(shape=(1,), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=8)\n)\n</code></pre> <p>For more information on serving the inference callable, refer to the Loading models section on Deploying Models page.</p>"},{"location":"initialization/","title":"Triton Initialization","text":""},{"location":"initialization/#initialization","title":"Initialization","text":"<p>The following page provides more details about possible options for configuring the Triton Inference Server and working with block and non-blocking mode for tests and deployment.</p>"},{"location":"initialization/#configuring-triton","title":"Configuring Triton","text":"<p>Connecting Python models with Triton Inference Server working in the current environment requires creating a Triton object. This can be done by creating a context:</p> <pre><code>from pytriton.triton import Triton\nwith Triton() as triton:\n...\n</code></pre> <p>or simply creating an object:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n</code></pre> <p>The Triton Inference Server behavior can be configured by passing config parameter:</p> <pre><code>import pathlib\nfrom pytriton.triton import Triton, TritonConfig\ntriton_config = TritonConfig(log_file=pathlib.Path(\"/tmp/triton.log\"))\nwith Triton(config=triton_config) as triton:\n...\n</code></pre> <p>and through environment variables, for example, set as in the command below:</p> <pre><code>PYTRITON_TRITON_CONFIG_LOG_VERBOSITY=4 python my_script.py\n</code></pre> <p>The order of precedence of configuration methods is:</p> <ul> <li>config defined through <code>config</code> parameter of Triton class <code>__init__</code> method</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul>"},{"location":"initialization/#blocking-mode","title":"Blocking mode","text":"<p>The blocking mode will stop the execution of the current thread and wait for incoming HTTP/gRPC requests for inference execution. This mode makes your application behave as a pure server. The example of using blocking mode:</p> <pre><code>from pytriton.triton import Triton\nwith Triton() as triton:\n...  # Load models here\ntriton.serve()\n</code></pre>"},{"location":"initialization/#background-mode","title":"Background mode","text":"<p>The background mode runs Triton as a subprocess and does not block the execution of the current thread. In this mode, you can run Triton Inference Server and interact with it from the current context. The example of using background mode:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n...  # Load models here\ntriton.run()  # Triton Server started\nprint(\"This print will appear\")\ntriton.stop()  # Triton Server stopped\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This page explains how to install the library. We assume that you have a basic understanding of the Python programming language and are familiar with machine learning models. Using Docker is optional but not required.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the library, ensure that you meet the following requirements:</p> <ul> <li>An operating system with glibc &gt;= 2.31. Triton Inference Server and PyTriton have only been rigorously tested on Ubuntu 20.04.   Other supported operating systems include Ubuntu 20.04+, Debian 11+, Rocky Linux 9+, and Red Hat Universal Base Image 9+.</li> <li>to check your glibc version, run <code>ldd --version</code></li> <li>Python version &gt;= 3.8. If you are using Python 3.9+, see the section \"Installation on Python 3.9+\" for additional steps.</li> <li>pip &gt;= 20.3</li> </ul> <p>The library can be installed in the system environment, a virtual environment, or a Docker image. NVIDIA optimized Docker images for Python frameworks can be obtained from the NVIDIA NGC Catalog. If you want to use the Docker runtime, we recommend that you install NVIDIA Container Toolkit to enable running model inference on NVIDIA GPU.</p>"},{"location":"installation/#installing-using-pip","title":"Installing using pip","text":"<p>You can install the package from pypi.org by running the following command:</p> <pre><code>pip install -U nvidia-pytriton\n</code></pre> <p>pip version</p> <p>The <code>pip</code> version must be at least 20.3. This is the first version of pip that supports the PEP-600 compliant <code>manylinux</code> platform tags used by PyTriton.</p> <p>To upgrade an older version of pip, run:</p> <pre><code>pip install -U pip\n</code></pre> <p>pip might install the pip package in the user site directory. If it is your case, make sure that the <code>$HOME/.local/bin</code> is in your <code>$PATH</code> environment variable.</p> <p>Triton Inference Server binaries</p> <p>The Triton Inference Server binaries are installed as part of the PyTriton package.</p>"},{"location":"installation/#installation-on-python-39","title":"Installation on Python 3.9+","text":"<p>The Triton Inference Server Python backend is linked to a fixed Python 3.8. Therefore, if you want to install PyTriton on a different version of Python, you need to prepare the environment for the Triton Inference Server Python backend. The environment should be located in the <code>~/.cache/pytriton/python_backend_interpreter</code> directory and should contain the packages <code>numpy~=1.21</code> and <code>pyzmq~=23.0</code>.</p>"},{"location":"installation/#using-pyenv","title":"Using pyenv","text":"<pre><code>apt update\n# need git and build dependencies https://github.com/pyenv/pyenv/wiki\\#suggested-build-environment\nDEBIAN_FRONTEND=noninteractive apt install -y python3 python3-distutils python-is-python3 git \\\nbuild-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev curl \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n\n# install pyenv\ncurl https://pyenv.run | bash\nexport PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n# compile python 3.8\npyenv install 3.8\n\n# prepare venv\npyenv global 3.8\npip3 install virtualenv\nmkdir -p ~/.cache/pytriton/\npython -mvenv ~/.cache/pytriton/python_backend_interpreter --copies --clear\nsource ~/.cache/pytriton/python_backend_interpreter/bin/activate\npip3 install numpy~=1.21 pyzmq~=23.0\n\n# recover system python\ndeactivate\npyenv global system\n</code></pre>"},{"location":"installation/#using-miniconda","title":"Using miniconda","text":"<pre><code>apt update\napt install -y python3 python3-distutils python-is-python3 curl\n\nCONDA_VERSION=latest\nTARGET_MACHINE=x86_64\ncurl \"https://repo.anaconda.com/miniconda/Miniconda3-${CONDA_VERSION}-Linux-${TARGET_MACHINE}.sh\" --output miniconda.sh\n\nsh miniconda.sh -b -p ~/.cache/conda\nrm miniconda.sh\n~/.cache/conda/bin/conda create -y -p ~/.cache/pytriton/python_backend_interpreter python=3.8 numpy~=1.21 pyzmq~=23.0\n</code></pre>"},{"location":"installation/#building-binaries-from-source","title":"Building binaries from source","text":"<p>The binary package can be built from the source, allowing access to unreleased hotfixes, the ability to modify the PyTriton code, and compatibility with various Triton Inference Server versions, including custom server builds. For further information on building the PyTriton binary, refer to the Building page.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>There is no one-to-one match between our solution and Triton Inference Server features, especially in terms of supporting a user model store.</li> <li>Support is currently limited to the x86-64 instruction set architecture.</li> <li>Running multiple scripts hosting PyTriton on the same machine or container is not feasible.</li> <li>Deadlocks may occur in some models when employing the NCCL communication library and multiple Inference Callables are triggered concurrently. This issue can be observed when deploying multiple instances of the same model or multiple models within a single server script. Additional information about this issue can be found here.</li> <li>Enabling verbose logging may cause a significant performance drop in model inference.</li> </ul>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>The prerequisite for this page is to install PyTriton, which can be found in the installation page.</p> <p>The Quick Start presents how to run a Python model in the Triton Inference Server without needing to change the current working environment. In this example, we are using a simple <code>Linear</code> PyTorch model.</p> <p>The integration of the model requires providing the following elements:</p> <ul> <li>The model - a framework or Python model or function that handles inference requests</li> <li>Inference Callable - function or class with <code>__call__</code> method, that handles the input data coming from Triton and returns the result</li> <li>Python function connection with Triton Inference Server - a binding for communication between Triton and the Inference Callable</li> </ul> <p>The requirement for the example is to have PyTorch installed in your environment. You can do this by running:</p> <pre><code>pip install torch\n</code></pre> <p>In the next step, define the <code>Linear</code> model:</p> <pre><code>import torch\nmodel = torch.nn.Linear(2, 3).to(\"cuda\").eval()\n</code></pre> <p>In the second step, create an inference callable as a function. The function obtains the HTTP/gRPC request data as an argument, which should be in the form of a NumPy array. The expected return object should also be a NumPy array. You can define an inference callable as a function that uses the <code>@batch</code> decorator from PyTriton. This decorator converts the input request into a more suitable format that can be directly passed to the model. You can read more about decorators here.</p> <p>Example implementation:</p> <pre><code>import numpy as np\nimport torch\nfrom pytriton.decorators import batch\n@batch\ndef infer_fn(**inputs: np.ndarray):\n(input1_batch,) = inputs.values()\ninput1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\noutput1_batch_tensor = model(input1_batch_tensor) # Calling the Python model inference\noutput1_batch = output1_batch_tensor.cpu().detach().numpy()\nreturn [output1_batch]\n</code></pre> <p>In the next step, you can create the binding between the inference callable and Triton Inference Server using the <code>bind</code> method from PyTriton. This method takes the model name, the inference callable, the inputs and outputs tensors, and an optional model configuration object.</p> <pre><code>from pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n# Connecting inference callable with Triton Inference Server\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"Linear\",\ninfer_func=infer_fn,\ninputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\noutputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\nconfig=ModelConfig(max_batch_size=128)\n)\n...\n</code></pre> <p>Finally, serve the model with the Triton Inference Server:</p> <pre><code>from pytriton.triton import Triton\nwith Triton() as triton:\n...  # Load models here\ntriton.serve()\n</code></pre> <p>The <code>bind</code> method creates a connection between the Triton Inference Server and the <code>infer_fn</code>, which handles the inference queries. The <code>inputs</code> and <code>outputs</code> describe the model inputs and outputs that are exposed in Triton. The config field allows more parameters for model deployment.</p> <p>The <code>serve</code> method is blocking, and at this point, the application waits for incoming HTTP/gRPC requests. From that moment, the model is available under the name <code>Linear</code> in the Triton server. The inference queries can be sent to <code>localhost:8000/v2/models/Linear/infer</code>, which are passed to the <code>infer_fn</code> function.</p> <p>If you would like to use Triton in the background mode, use <code>run</code>. More about that can be found in the Deploying Models page.</p> <p>Once the <code>serve</code> or <code>run</code> method is called on the <code>Triton</code> object, the server status can be obtained using:</p> <pre><code>curl -v localhost:8000/v2/health/live\n</code></pre> <p>The model is loaded right after the server starts, and its status can be queried using:</p> <pre><code>curl -v localhost:8000/v2/models/Linear/ready\n</code></pre> <p>Finally, you can send an inference query to the model:</p> <pre><code>curl -X POST \\\n-H \"Content-Type: application/json\"  \\\n-d @input.json \\\nlocalhost:8000/v2/models/Linear/infer\n</code></pre> <p>The <code>input.json</code> with sample query:</p> <pre><code>{\n\"id\": \"0\",\n\"inputs\": [\n{\n\"name\": \"INPUT_1\",\n\"shape\": [1, 2],\n\"datatype\": \"FP32\",\n\"parameters\": {},\n\"data\": [[-0.04281254857778549, 0.6738349795341492]]\n}\n]\n}\n</code></pre> <p>Read more about the HTTP/gRPC interface in the Triton Inference Server documentation.</p> <p>You can also validate the deployed model using a simple client that can perform inference requests:</p> <pre><code>import torch\nfrom pytriton.client import ModelClient\ninput1_data = torch.randn(128, 2).cpu().detach().numpy()\nwith ModelClient(\"localhost:8000\", \"Linear\") as client:\nresult_dict = client.infer_batch(input1_data)\nprint(result_dict)\n</code></pre> <p>The full example code can be found in examples/linear_random_pytorch.</p> <p>More information about running the server and models can be found in Deploying Models page.</p>"}]}