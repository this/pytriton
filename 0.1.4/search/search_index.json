{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pytriton","title":"PyTriton","text":"<p>PyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. The library allows serving Machine Learning models directly from Python through NVIDIA's Triton Inference Server.</p>"},{"location":"#how-it-works","title":"How it works?","text":"<p>In PyTriton, as in Flask or FastAPI, you can define any Python function that executes a machine learning model prediction and exposes it through an HTTP/gRPC API. PyTriton installs Triton Inference Server in your environment and uses it for handling HTTP/gRPC requests and responses. Our library provides a Python API that allows attaching a Python function to Triton and a communication layer to send/receive data between Triton and the function. This solution helps utilize the performance features of Triton Inference Server, such as dynamic batching or response cache, without changing your model environment. Thus, it improves the performance of running inference on GPU for models implemented in Python. The solution is framework-agnostic and can be used along with frameworks like PyTorch, TensorFlow, or JAX.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The diagram below presents the schema of how the Python models are served through Triton Inference Server using PyTriton. The solution consists of two main components:</p> <ul> <li>Triton Inference Server: for exposing the HTTP/gRPC API and benefiting from performance features like dynamic batching or response cache.</li> <li>Python Model Environment: your environment where the Python model is executed.</li> </ul> <p>The Triton Inference Server binaries are provided as part of the PyTriton installation. The Triton Server is installed in your current environment (system or container). The PyTriton controls the Triton Server process through the <code>Triton Controller</code>.</p> <p>Exposing the model through PyTriton requires the definition of an <code>Inference Callable</code> - a Python function that is connected to Triton Inference Server and executes the model or ensemble for predictions. The integration layer binds the <code>Inference Callable</code> to Triton Server and exposes it through the Triton HTTP/gRPC API under a provided <code>&lt;model name&gt;</code>. Once the integration is done, the defined <code>Inference Callable</code> receives data sent to the HTTP/gRPC API endpoint <code>v2/models/&lt;model name&gt;/infer</code>. Read more about HTTP/gRPC interface in Triton Inference Server documentation.</p> <p>The HTTP/gRPC requests sent to <code>v2/models/&lt;model name&gt;/infer</code> are handled by Triton Inference Server. The server batches requests and passes them to the <code>Proxy Backend</code>, which sends the batched requests to the appropriate <code>Inference Callable</code>. The data is sent as a <code>numpy</code> array. Once the <code>Inference Callable</code> finishes execution of the model prediction, the result is returned to the <code>Proxy Backend</code>, and a response is created by Triton Server.</p> <p></p>"},{"location":"#serving-the-models","title":"Serving the models","text":"<p>PyTriton provides an option to serve your Python model using Triton Inference Server to handle HTTP/gRPC requests and pass the input/output tensors to and from the model. We use a blocking mode where the application is a long-lived process deployed in your cluster to serve the requests from clients.</p> <p>Before you run the model for serving the inference callback function, it has to be defined. The inference callback receives the inputs and should return the model outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n@batch\ndef infer_fn(**inputs: np.ndarray):\ninput1, input2 = inputs.values()\noutputs = model(input1, input2)\nreturn [outputs]\n</code></pre> <p>The <code>infer_fn</code> receives the batched input data for the model and should return the batched outputs.</p> <p>In the next step, you need to create a connection between Triton and the model. For that purpose, the <code>Triton</code> class has to be used, and the <code>bind</code> method is required to be called to create a dedicated connection between Triton Inference Server and the defined <code>infer_fn</code>.</p> <p>In the blocking mode, we suggest using the <code>Triton</code> object as a context manager where multiple models can be loaded in the way presented below:</p> <pre><code>from pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"MyModel\",\ninfer_func=infer_fn,\ninputs=[\nTensor(dtype=bytes, shape=(1,)),  # sample containing single bytes value\nTensor(dtype=bytes, shape=(-1,)),  # sample containing vector of bytes\n],\noutputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\nconfig=ModelConfig(max_batch_size=16),\n)\n</code></pre> <p>At this point, you have defined how the model has to be handled by Triton and where the HTTP/gRPC requests for the model have to be directed. The last part for serving the model is to call the <code>serve</code> method on the Triton object:</p> <pre><code>with Triton() as triton:\n# ...\ntriton.serve()\n</code></pre> <p>When the <code>.serve()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p>"},{"location":"#working-in-the-jupyter-notebook","title":"Working in the Jupyter Notebook","text":"<p>The package provides an option to work with your model inside the Jupyter Notebook. We call it a background mode where the model is deployed on Triton Inference Server for handling HTTP/gRPC requests, but there are other actions that you want to perform after loading and starting serving the model.</p> <p>Having the <code>infer_fn</code> defined in the same way as described in the serving the models section, you can use the <code>Triton</code> object without a context:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n</code></pre> <p>In the next step, the model has to be loaded for serving in Triton Inference Server (which is also the same as in the serving example):</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\n@batch\ndef infer_fn(**inputs: np.ndarray):\ninput1, input2 = inputs.values()\noutputs = input1 + input2\nreturn [outputs]\ntriton.bind(\nmodel_name=\"MyModel\",\ninfer_func=infer_fn,\ninputs=[\nTensor(shape=(1,), dtype=np.float32),\nTensor(shape=(-1,), dtype=np.float32),\n],\noutputs=[Tensor(shape=(-1,), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=16),\n)\n</code></pre> <p>Finally, to run the model in background mode, use the <code>run</code> method:</p> <pre><code>triton.run()\n</code></pre> <p>When the <code>.run()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p> <p>The Triton server can be stopped at any time using the <code>stop</code> method:</p> <pre><code>triton.stop()\n</code></pre>"},{"location":"#what-next","title":"What next?","text":"<p>Read more about using PyTriton in the Quick Start, Examples and find more options on how to configure Triton, models, and deployment on a cluster in the Deploying Models section.</p> <p>The details about classes and methods can be found in the API Reference page.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#014-2023-03-16","title":"0.1.4 (2023-03-16)","text":"<ul> <li>Add validation of the model name passed to Triton bind method.</li> <li> <p>Add monkey patching of <code>InferenceServerClient.__del__</code> method to prevent unhandled exceptions.</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions. Refer to its support matrix for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#013-2023-02-20","title":"0.1.3 (2023-02-20)","text":"<ul> <li> <p>Fixed getting model config in <code>fill_optionals</code> decorator.</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions. Refer to its support matrix for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#012-2023-02-14","title":"0.1.2 (2023-02-14)","text":"<ul> <li>Fixed wheel build to support installations on operating systems with glibc version 2.31 or higher.</li> <li>Updated the documentation on custom builds of the package.</li> <li>Change: TritonContext instance is shared across bound models and contains model_configs dictionary.</li> <li> <p>Fixed support of binding multiple models that uses methods of the same class.</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions. Refer to its support matrix for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#011-2023-01-31","title":"0.1.1 (2023-01-31)","text":"<ul> <li>Change: The <code>@first_value</code> decorator has been updated with new features:</li> <li>Renamed from <code>@first_values</code> to <code>@first_value</code></li> <li>Added a <code>strict</code> flag to toggle the checking of equality of values on a single selected input of the request. Default is True</li> <li>Added a <code>squeeze_single_values</code> flag to toggle the squeezing of single value ND arrays to scalars. Default is True</li> <li>Fix: <code>@fill_optionals</code> now supports non-batching models</li> <li>Fix: <code>@first_value</code> fixed to work with optional inputs</li> <li>Fix: <code>@group_by_values</code> fixed to work with string inputs</li> <li> <p>Fix: <code>@group_by_values</code> fixed to work per sample-wise</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions. Refer to its support matrix for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#010-2023-01-12","title":"0.1.0 (2023-01-12)","text":"<ul> <li> <p>Initial release of PyTriton</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions. Refer to its support matrix for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/pytriton/issues.</p> <p>If you are reporting a bug, include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The pyTriton could always use more documentation, whether as part of the official pyTriton docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/pytriton/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, you simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes: <pre><code>$ git commit -s -m \"Add cool feature.\n</code></pre></p> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the below:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up the <code>PyTriton</code> for local development.</p> <ol> <li>Fork the <code>PyTriton</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/triton-inference-server/pytriton.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development:</li> </ol> <pre><code>$ mkvirtualenv pytriton\n$ cd pytriton/\n$ make install-dev\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ make lint  # will run i.a. flake8 and pytype linters\n$ make test  # will run a test with on your current virtualenv\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#tips","title":"Tips","text":"<p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_subset\n</code></pre>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in Google Style Guide.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#api-reference","title":"API Reference","text":""},{"location":"api/#pytriton.triton.TritonConfig","title":"<code>pytriton.triton.TritonConfig</code>  <code>dataclass</code>","text":"<p>Triton Inference Server configuration class for customization of server execution.</p> <p>The arguments are optional. If value is not provided the defaults for Triton Inference Server are used. Please, refer to https://github.com/triton-inference-server/server/ for more details.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[str]</code> <p>Identifier for this server.</p> <code>None</code> <code>log_verbose</code> <code>Optional[int]</code> <p>Set verbose logging level. Zero (0) disables verbose logging and values &gt;= 1 enable verbose logging.</p> <code>None</code> <code>log_file</code> <code>Optional[pathlib.Path]</code> <p>Set the name of the log output file.</p> <code>None</code> <code>exit_timeout_secs</code> <code>Optional[int]</code> <p>Timeout (in seconds) when exiting to wait for in-flight inferences to finish.</p> <code>None</code> <code>exit_on_error</code> <code>Optional[bool]</code> <p>Exit the inference server if an error occurs during initialization.</p> <code>None</code> <code>strict_readiness</code> <code>Optional[bool]</code> <p>If true /v2/health/ready endpoint indicates ready if the server is responsive and all models are available.</p> <code>None</code> <code>allow_http</code> <code>Optional[bool]</code> <p>Allow the server to listen for HTTP requests.</p> <code>None</code> <code>http_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for HTTP requests.</p> <code>None</code> <code>http_address</code> <code>Optional[str]</code> <p>The address for the http server to binds to.</p> <code>None</code> <code>http_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling HTTP requests.</p> <code>None</code> <code>allow_grpc</code> <code>Optional[bool]</code> <p>Allow the server to listen for GRPC requests.</p> <code>None</code> <code>grpc_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for GRPC requests.</p> <code>None</code> <code>grpc_address</code> <code>Optional[str]</code> <p>The address for the grpc server to binds to.</p> <code>None</code> <code>grpc_infer_allocation_pool_size</code> <code>Optional[int]</code> <p>The maximum number of inference request/response objects that remain allocated for reuse</p> <code>None</code> <code>grpc_use_ssl</code> <code>Optional[bool]</code> <p>Use SSL authentication for GRPC requests.</p> <code>None</code> <code>grpc_use_ssl_mutual</code> <code>Optional[bool]</code> <p>Use mutual SSL authentication for GRPC requests.</p> <code>None</code> <code>grpc_server_cert</code> <code>Optional[pathlib.Path]</code> <p>Path to file holding PEM-encoded server certificate. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_server_key</code> <code>Optional[pathlib.Path]</code> <p>Path to file holding PEM-encoded server key. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_root_cert</code> <code>Optional[pathlib.Path]</code> <p>Path to file holding PEM-encoded root certificate. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_infer_response_compression_level</code> <code>Optional[str]</code> <p>The compression level to be used while returning the inference response to the peer. Allowed values are none, low, medium and high. Default is none.</p> <code>None</code> <code>grpc_keepalive_time</code> <code>Optional[int]</code> <p>The period (in milliseconds) after which a keepalive ping is sent on the transport.</p> <code>None</code> <code>grpc_keepalive_timeout</code> <code>Optional[int]</code> <p>The period (in milliseconds) the sender of the keepalive ping waits for an acknowledgement.</p> <code>None</code> <code>grpc_keepalive_permit_without_calls</code> <code>Optional[bool]</code> <p>Allows keepalive pings to be sent even if there are no calls in flight</p> <code>None</code> <code>grpc_http2_max_pings_without_data</code> <code>Optional[int]</code> <p>The maximum number of pings that can be sent when there is no data/header frame to be sent.</p> <code>None</code> <code>grpc_http2_min_recv_ping_interval_without_data</code> <code>Optional[int]</code> <p>If there are no data/header frames being sent on the transport, this channel argument on the server side controls the minimum time (in milliseconds) that gRPC Core would expect between receiving successive pings.</p> <code>None</code> <code>grpc_http2_max_ping_strikes</code> <code>Optional[int]</code> <p>Maximum number of bad pings that the server will tolerate before sending an HTTP2 GOAWAY frame and closing the transport.</p> <code>None</code> <code>allow_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide prometheus metrics.</p> <code>None</code> <code>allow_gpu_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide GPU metrics.</p> <code>None</code> <code>allow_cpu_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide CPU metrics.</p> <code>None</code> <code>metrics_interval_ms</code> <code>Optional[int]</code> <p>Metrics will be collected once every  milliseconds. <code>None</code> <code>metrics_port</code> <code>Optional[int]</code> <p>The port reporting prometheus metrics.</p> <code>None</code> <code>allow_sagemaker</code> <code>Optional[bool]</code> <p>Allow the server to listen for Sagemaker requests.</p> <code>None</code> <code>sagemaker_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for Sagemaker requests.</p> <code>None</code> <code>sagemaker_safe_port_range</code> <code>Optional[str]</code> <p>Set the allowed port range for endpoints other than the SageMaker endpoints.</p> <code>None</code> <code>sagemaker_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling Sagemaker requests.</p> <code>None</code> <code>allow_vertex_ai</code> <code>Optional[bool]</code> <p>Allow the server to listen for Vertex AI requests.</p> <code>None</code> <code>vertex_ai_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for Vertex AI requests.</p> <code>None</code> <code>vertex_ai_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling Vertex AI requests.</p> <code>None</code> <code>vertex_ai_default_model</code> <code>Optional[str]</code> <p>The name of the model to use for single-model inference requests.</p> <code>None</code> <code>trace_file</code> <code>Optional[pathlib.Path]</code> <p>Set the file where trace output will be saved.</p> <code>None</code> <code>trace_level</code> <code>Optional[str]</code> <p>Specify a trace level.</p> <code>None</code> <code>trace_rate</code> <code>Optional[int]</code> <p>Set the trace sampling rate.</p> <code>None</code> <code>trace_count</code> <code>Optional[int]</code> <p>Set the number of traces to be sampled.</p> <code>None</code> <code>trace_log_frequency</code> <code>Optional[int]</code> <p>Set the trace log frequency.</p> <code>None</code> <code>response_cache_byte_size</code> <code>Optional[int]</code> <p>The size in bytes to allocate for a request/response cache.</p> <code>None</code> <code>buffer_manager_thread_count</code> <code>Optional[int]</code> <p>The number of threads used to accelerate copies and other operations required to manage input and output tensor contents.</p> <code>None</code>"},{"location":"api/#pytriton.triton.TritonConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration for early error handling.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate configuration for early error handling.\"\"\"\nif self.allow_http not in [True, None] and self.allow_grpc not in [True, None]:\nraise PyTritonValidationError(\"The `http` or `grpc` endpoint has to be allowed.\")\n</code></pre>"},{"location":"api/#pytriton.triton.TritonConfig.from_env","title":"<code>from_env()</code>  <code>classmethod</code>","text":"<p>Creates TritonConfig from environment variables.</p> <p>Environment variables should start with <code>PYTRITON_TRITON_CONFIG_</code> prefix. For example:</p> <pre><code>PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\nPYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n</code></pre> Typical use <p>triton_config = TritonConfig.from_env()</p> <p>Returns:</p> Type Description <code>TritonConfig</code> <p>TritonConfig class instantiated from environment variables.</p> Source code in <code>pytriton/triton.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; \"TritonConfig\":\n\"\"\"Creates TritonConfig from environment variables.\n    Environment variables should start with `PYTRITON_TRITON_CONFIG_` prefix. For example:\n        PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\n        PYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n    Typical use:\n        triton_config = TritonConfig.from_env()\n    Returns:\n        TritonConfig class instantiated from environment variables.\n    \"\"\"\nprefix = \"PYTRITON_TRITON_CONFIG_\"\nconfig = {name[len(prefix) :].lower(): value for name, value in os.environ.items() if name.startswith(prefix)}\nfields: Dict[str, dataclasses.Field] = {field.name: field for field in dataclasses.fields(cls)}\nunknown_config_parameters = {name: value for name, value in config.items() if name not in fields}\nfor name, value in unknown_config_parameters.items():\nLOGGER.warning(\nf\"Ignoring {name}={value} as could not find matching config field. \"\nf\"Available fields: {', '.join(map(str, fields))}\"\n)\ndef _cast_value(_field, _value):\nfield_type = _field.type\nis_optional = typing_inspect.is_optional_type(field_type)\nif is_optional:\nfield_type = field_type.__args__[0]\nreturn field_type(_value)\nconfig_with_casted_values = {\nname: _cast_value(fields[name], value) for name, value in config.items() if name in fields\n}\nreturn cls(**config_with_casted_values)\n</code></pre>"},{"location":"api/#pytriton.triton.TritonConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Map config object to dictionary.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def to_dict(self):\n\"\"\"Map config object to dictionary.\"\"\"\nreturn dataclasses.asdict(self)\n</code></pre>"},{"location":"api/#pytriton.decorators","title":"<code>pytriton.decorators</code>","text":"<p>Inference callable decorators.</p>"},{"location":"api/#pytriton.decorators.ModelConfigDict","title":"<code>ModelConfigDict()</code>","text":"<p>         Bases: <code>MutableMapping</code></p> <p>Dictionary for storing model configs for inference callable.</p> <p>Create ModelConfigDict object.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __init__(self):\n\"\"\"Create ModelConfigDict object.\"\"\"\nself._data: Dict[str, TritonModelConfig] = {}\nself._keys: List[Callable] = []\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__delitem__","title":"<code>__delitem__(infer_callable)</code>","text":"<p>Delete model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __delitem__(self, infer_callable: Callable):\n\"\"\"Delete model config for inference callable.\"\"\"\nkey = self._get_model_config_key(infer_callable)\ndel self._data[key]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__getitem__","title":"<code>__getitem__(infer_callable)</code>","text":"<p>Get model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __getitem__(self, infer_callable: Callable) -&gt; TritonModelConfig:\n\"\"\"Get model config for inference callable.\"\"\"\nkey = self._get_model_config_key(infer_callable)\nreturn self._data[key]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterate over inference callable keys.\"\"\"\nreturn iter(self._keys)\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__len__","title":"<code>__len__()</code>","text":"<p>Get number of inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __len__(self):\n\"\"\"Get number of inference callable keys.\"\"\"\nreturn len(self._data)\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__setitem__","title":"<code>__setitem__(infer_callable, item)</code>","text":"<p>Set model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __setitem__(self, infer_callable: Callable, item: TritonModelConfig):\n\"\"\"Set model config for inference callable.\"\"\"\nself._keys.append(infer_callable)\nkey = self._get_model_config_key(infer_callable)\nself._data[key] = item\n</code></pre>"},{"location":"api/#pytriton.decorators.TritonContext","title":"<code>TritonContext</code>  <code>dataclass</code>","text":"<p>Triton context definition class.</p>"},{"location":"api/#pytriton.decorators.batch","title":"<code>batch(wrapped, instance, args, kwargs)</code>","text":"<p>Decorator for converting list of request dicts to dict of input batches.</p> <p>Converts list of request dicts to dict of input batches. It passes **kwargs to inference callable where each named input contains numpy array with batch of requests received by Triton server. We assume that each request has the same set of keys (you can use group_by_keys decorator before using @batch decorator if your requests may have different set of keys).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef batch(wrapped, instance, args, kwargs):\n\"\"\"Decorator for converting list of request dicts to dict of input batches.\n    Converts list of request dicts to dict of input batches.\n    It passes **kwargs to inference callable where each named input contains numpy array with batch of requests\n    received by Triton server.\n    We assume that each request has the same set of keys (you can use group_by_keys decorator before\n    using @batch decorator if your requests may have different set of keys).\n    \"\"\"\nreq_list = args[0]\ninput_names = req_list[0].keys()\nfor req_dict2 in req_list[1:]:\nif input_names != req_dict2.keys():\nraise PyTritonValidationError(\"Cannot batch requests with different set of inputs keys\")\ninputs = {}\nfor model_input in input_names:\nconcatenated_input_data = np.concatenate([req_dict[model_input] for req_dict in req_list])\ninputs[model_input] = concatenated_input_data\nargs = args[1:]\nnew_kwargs = dict(kwargs)\nnew_kwargs.update(inputs)\noutputs = wrapped(*args, **new_kwargs)\noutputs = convert_output(outputs, wrapped, instance)\noutput_names = outputs.keys()\nout_list = []\nstart_idx = 0\nfor request in req_list:\n# get batch_size of first input for each request - assume that all inputs have same batch_size\nfirst_input = next(iter(request.values()))\nrequest_batch_size = first_input.shape[0]\nreq_output_dict = {}\nfor _output_ind, output_name in enumerate(output_names):\nreq_output = outputs[output_name][start_idx : start_idx + request_batch_size, ...]\nreq_output_dict[output_name] = req_output\nout_list.append(req_output_dict)\nstart_idx += request_batch_size\nreturn out_list\n</code></pre>"},{"location":"api/#pytriton.decorators.convert_output","title":"<code>convert_output(outputs, wrapped=None, instance=None, model_config=None)</code>","text":"<p>Converts output from tuple ot list to dictionary.</p> <p>It is utility function useful for mapping output list into dictionary of outputs. Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs instead of dictionary if this list matches output list in model config (size and order).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def convert_output(\noutputs: Union[Dict, List, Tuple], wrapped=None, instance=None, model_config: Optional[TritonModelConfig] = None\n):\n\"\"\"Converts output from tuple ot list to dictionary.\n    It is utility function useful for mapping output list into dictionary of outputs.\n    Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs\n    instead of dictionary if this list matches output list in model config (size and order).\n    \"\"\"\nif isinstance(outputs, dict):\nreturn outputs\nelif isinstance(outputs, (list, tuple)):\nif model_config is None:\nmodel_config = get_model_config(wrapped, instance)\nif len(outputs) != len(model_config.outputs):\nraise PyTritonValidationError(\"Outputs length different than config outputs length\")\noutputs = {config_output.name: output for config_output, output in zip(model_config.outputs, outputs)}\nreturn outputs\nelse:\nraise PyTritonValidationError(f\"Unsupported output type {type(outputs)}.\")\n</code></pre>"},{"location":"api/#pytriton.decorators.fill_optionals","title":"<code>fill_optionals(**defaults)</code>","text":"<p>This decorator ensures that any missing inputs in requests are filled with default values specified by the user.</p> <p>Default values should be NumPy arrays without batch axis.</p> <p>If you plan to group requests ex. with @group_by_keys or @group_by_vales decorators provide default values for optional parameters at the beginning of decorators stack. The other decorators can then group requests into bigger batches resulting in a better model performance.</p> Typical use <p>@fill_optionals() @group_by_keys() @batch def infer_fun(**inputs):     ...     return outputs</p> <p>Parameters:</p> Name Type Description Default <code>defaults</code> <p>keyword arguments containing default values for missing inputs</p> <code>{}</code> <p>If you have default values for some optional parameter it is good idea to provide them at the very beginning, so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def fill_optionals(**defaults):\n\"\"\"This decorator ensures that any missing inputs in requests are filled with default values specified by the user.\n    Default values should be NumPy arrays without batch axis.\n    If you plan to group requests ex. with\n    [@group_by_keys][pytriton.decorators.group_by_keys] or\n    [@group_by_vales][pytriton.decorators.group_by_values] decorators\n    provide default values for optional parameters at the beginning of decorators stack.\n    The other decorators can then group requests into bigger batches resulting in a better model performance.\n    Typical use:\n        @fill_optionals()\n        @group_by_keys()\n        @batch\n        def infer_fun(**inputs):\n            ...\n            return outputs\n    Args:\n        defaults: keyword arguments containing default values for missing inputs\n    If you have default values for some optional parameter it is good idea to provide them at the very beginning,\n    so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.\n    \"\"\"\ndef _verify_defaults(model_config: TritonModelConfig):\ninputs = {spec.name: spec for spec in model_config.inputs}\nnot_matching_default_names = sorted(set(defaults) - set(inputs))\nif not_matching_default_names:\nraise PyTritonBadParameterError(f\"Could not found {', '.join(not_matching_default_names)} inputs\")\nnon_numpy_items = {k: v for k, v in defaults.items() if not isinstance(v, np.ndarray)}\nif non_numpy_items:\nraise PyTritonBadParameterError(\nf\"Could not use {', '.join([f'{k}={v}' for k, v in non_numpy_items.items()])} defaults \"\n\"as they are not NumPy arrays\"\n)\nnot_matching_dtypes = {k: (v.dtype, inputs[k].dtype) for k, v in defaults.items() if v.dtype != inputs[k].dtype}\nif not_matching_dtypes:\nnon_matching_dtypes_str_list = [\nf\"{name}: dtype={have_dtype} expected_dtype={expected_dtype}\"\nfor name, (have_dtype, expected_dtype) in not_matching_dtypes.items()\n]\nraise PyTritonBadParameterError(\nf\"Could not use {', '.join(non_matching_dtypes_str_list)} \"\nf\"defaults as they have different than input signature dtypes\"\n)\ndef _shape_match(_have_shape, _expected_shape):\nreturn len(_have_shape) == len(_expected_shape) and all(\n[e == -1 or h == e for h, e in zip(_have_shape, _expected_shape)]\n)\nnot_matching_shapes = {\nk: (v.shape, inputs[k].shape) for k, v in defaults.items() if not _shape_match(v.shape, inputs[k].shape)\n}\nif not_matching_shapes:\nnon_matching_shapes_str_list = [\nf\"{name}: shape={have_shape} expected_shape={expected_shape}\"\nfor name, (have_shape, expected_shape) in not_matching_shapes.items()\n]\nraise PyTritonBadParameterError(\nf\"Could not use {', '.join(non_matching_shapes_str_list)} \"\nf\"defaults as they have different than input signature shapes\"\n)\n@wrapt.decorator\ndef _wrapper(wrapped, instance, args, kwargs):\nmodel_config = get_model_config(wrapped, instance)\n_verify_defaults(model_config)\n# verification if not after group wrappers is in group wrappers\n(requests,) = args\nmodel_supports_batching = model_config.batching\nfor request in requests:\nbatch_size = get_inference_request_batch_size(request) if model_supports_batching else None\nfor default_key, default_value in defaults.items():\nif default_key in request:\ncontinue\nif model_supports_batching:\nones_reps = (1,) * default_value.ndim  # repeat once default_value on each axis\naxis_reps = (batch_size,) + ones_reps  # ... except on batch axis. we repeat it batch_size times\ndefault_value = np.tile(default_value, axis_reps)\nrequest[default_key] = default_value\nreturn wrapped(*args, **kwargs)\nreturn _wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.first_value","title":"<code>first_value(*keys, squeeze_single_values=True, strict=True)</code>","text":"<p>This decorator overwrites selected inputs with first element of the given input.</p> <p>It can be used in two ways:</p> <ol> <li> <p>Wrapping a single request inference callable by chaining with @batch decorator:     @batch     @first_value(\"temperature\")     def infer_fn(**inputs):         ...         return result</p> </li> <li> <p>Wrapping a multiple requests inference callable:     @first_value(\"temperature\")     def infer_fn(requests):         ...         return results</p> </li> </ol> <p>By default, the decorator squeezes single value arrays to scalars. This behavior can be disabled by setting the <code>squeeze_single_values</code> flag to False.</p> <p>By default, the decorator checks the equality of the values on selected values. This behavior can be disabled by setting the <code>strict</code> flag to False.</p> <p>Wrapper can only be used with models that support batching.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str</code> <p>The input keys selected for conversion.</p> <code>()</code> <code>squeeze_single_values</code> <p>squeeze single value ND array to scalar values. Defaults to True.</p> <code>True</code> <code>strict</code> <code>bool</code> <p>enable checking if all values on single selected input of request are equal. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>PyTritonRuntimeError</code> <p>if not all values on a single selected input of the request are equal</p> <code>PyTritonBadParameterError</code> <p>if any of the keys passed to the decorator are not allowed.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def first_value(*keys: str, squeeze_single_values=True, strict: bool = True):\n\"\"\"This decorator overwrites selected inputs with first element of the given input.\n    It can be used in two ways:\n    1. Wrapping a single request inference callable by chaining with @batch decorator:\n        @batch\n        @first_value(\"temperature\")\n        def infer_fn(**inputs):\n            ...\n            return result\n    2. Wrapping a multiple requests inference callable:\n        @first_value(\"temperature\")\n        def infer_fn(requests):\n            ...\n            return results\n    By default, the decorator squeezes single value arrays to scalars.\n    This behavior can be disabled by setting the `squeeze_single_values` flag to False.\n    By default, the decorator checks the equality of the values on selected values.\n    This behavior can be disabled by setting the `strict` flag to False.\n    Wrapper can only be used with models that support batching.\n    Args:\n        keys: The input keys selected for conversion.\n        squeeze_single_values: squeeze single value ND array to scalar values. Defaults to True.\n        strict: enable checking if all values on single selected input of request are equal. Defaults to True.\n    Raises:\n        PyTritonRuntimeError: if not all values on a single selected input of the request are equal\n        and the strict flag is set to True. Additionally, if the decorator is used with a model that doesn't support batching,\n        PyTritonBadParameterError: if any of the keys passed to the decorator are not allowed.\n    \"\"\"\nif any(k in _SPECIAL_KEYS for k in keys):\nnot_allowed_keys = [key for key in keys if key in _SPECIAL_KEYS]\nraise PyTritonBadParameterError(\nf\"The keys {', '.join(not_allowed_keys)} are not allowed as keys for @first_value wrapper. \"\nf\"The set of not allowed keys are {', '.join(_SPECIAL_KEYS)}\"\n)\n@wrapt.decorator\ndef wrapper(wrapped, instance, args, kwargs):\nmodel_config = get_model_config(wrapped, instance)\nif not model_config.batching:\nraise PyTritonRuntimeError(\"The @first_value decorator can only be used with models that support batching.\")\ndef _replace_inputs_with_first_value(_request):\nfor input_name in keys:\nif input_name not in _request:\ncontinue\nvalues = _request[input_name]\nif strict:\n# do not set axis for arrays with strings (object) or models not supporting batching\naxis_of_uniqueness = None if values.dtype == object else 0\nunique_values = np.unique(values, axis=axis_of_uniqueness)\nif len(unique_values) &gt; 1:\nraise PyTritonRuntimeError(\nf\"The values on the {input_name!r} input are not equal. \"\n\"To proceed, either disable strict mode in @first_value wrapper \"\n\"or ensure that the values always are consistent. \"\nf\"The current values of {input_name!r} are {_request[input_name]!r}.\"\n)\n_first_value = values[0]\nif (\nsqueeze_single_values\nand not np.isscalar(_first_value)\nand all(dim == 1 for dim in _first_value.shape)\n):\n_dim_0_array = np.squeeze(_first_value)\n_first_value = _dim_0_array[()]  # obtain scalar from 0-dim array with numpy type\n_request[input_name] = _first_value\nreturn _request\ninputs_names = set(kwargs) - set(_SPECIAL_KEYS)\nif inputs_names:\nkwargs = _replace_inputs_with_first_value(kwargs)\nreturn wrapped(*args, **kwargs)\nelse:\nrequests, *other_args = args\nrequests = [_replace_inputs_with_first_value(request) for request in requests]\nreturn wrapped(requests, *other_args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.get_inference_request_batch_size","title":"<code>get_inference_request_batch_size(inference_request)</code>","text":"<p>Get batch size from triton request.</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>InferenceRequest</code> <p>Triton request.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Batch size.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_inference_request_batch_size(inference_request: InferenceRequest) -&gt; int:\n\"\"\"Get batch size from triton request.\n    Args:\n        inference_request (InferenceRequest): Triton request.\n    Returns:\n        int: Batch size.\n    \"\"\"\nfirst_input_value = next(iter(inference_request.values()))\nbatch_size, *dims = first_input_value.shape\nreturn batch_size\n</code></pre>"},{"location":"api/#pytriton.decorators.get_model_config","title":"<code>get_model_config(wrapped, instance)</code>","text":"<p>Retrieves instance of TritonModelConfig from callable.</p> <p>It is internally used in convert_output function to get output list from model. You can use this in custom decorators if you need access to model_config information. If you use @triton_context decorator you do not need this function (you can get model_config directly from triton_context passing function/callable to dictionary getter).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_model_config(wrapped, instance) -&gt; TritonModelConfig:\n\"\"\"Retrieves instance of TritonModelConfig from callable.\n    It is internally used in convert_output function to get output list from model.\n    You can use this in custom decorators if you need access to model_config information.\n    If you use @triton_context decorator you do not need this function (you can get model_config directly\n    from triton_context passing function/callable to dictionary getter).\n    \"\"\"\nreturn get_triton_context(wrapped, instance).model_configs[wrapped]\n</code></pre>"},{"location":"api/#pytriton.decorators.get_triton_context","title":"<code>get_triton_context(wrapped, instance)</code>","text":"<p>Retrieves triton context from callable.</p> <p>It is used in @triton_context to get triton context registered by triton binding in inference callable. If you use @triton_context decorator you do not need this function.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_triton_context(wrapped, instance) -&gt; TritonContext:\n\"\"\"Retrieves triton context from callable.\n    It is used in @triton_context to get triton context registered by triton binding in inference callable.\n    If you use @triton_context decorator you do not need this function.\n    \"\"\"\ncaller = instance or wrapped\nif not hasattr(caller, \"__triton_context__\"):\nraise PyTritonValidationError(\"Wrapped function or object must bound with triton to get  __triton_context__\")\nreturn caller.__triton_context__\n</code></pre>"},{"location":"api/#pytriton.decorators.group_by_keys","title":"<code>group_by_keys(wrapped, instance, args, kwargs)</code>","text":"<p>Group by keys.</p> <p>Decorator prepares groups of requests with the same set of keys and calls wrapped function for each group separately (it is convenient to use this decorator before batching, because the batching decorator requires consistent set of inputs as it stacks them into batches).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef group_by_keys(wrapped, instance, args, kwargs):\n\"\"\"Group by keys.\n    Decorator prepares groups of requests with the same set of keys and calls wrapped function\n    for each group separately (it is convenient to use this decorator before batching, because the batching decorator\n    requires consistent set of inputs as it stacks them into batches).\n    \"\"\"\ninputs = args[0]\nidx_inputs = [(idx, tuple(sorted(input.keys())), input) for idx, input in enumerate(inputs)]\nidx_inputs.sort(key=operator.itemgetter(1))\nidx_groups_res = []\nfor _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\nidx, _key, sample_list = zip(*group)\nargs = (list(sample_list),) + args[1:]\nout = wrapped(*args, **kwargs)\nidx_groups_res.extend(zip(idx, out))\nidx_groups_res.sort(key=operator.itemgetter(0))\nres_flat = [r[1] for r in idx_groups_res]\nreturn res_flat\n</code></pre>"},{"location":"api/#pytriton.decorators.group_by_values","title":"<code>group_by_values(*keys)</code>","text":"<p>Decorator for grouping requests by values of selected keys.</p> <p>Splits a batch into multiple sub-batches with the same values of selected keys and calls the decorated function with each of them. This is especially convenient when working with models that require dynamic parameters sent by the user. For example, given an input of the form:</p> <pre><code>{\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n</code></pre> <p>Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:</p> <pre><code>[\n    {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n    {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n]\n</code></pre> <p>This decorator should be used after the @batch decorator.</p> Typical use <p>@batch @group_by_values(\"param1\", \"param2\") def infer_fun(**inputs):     ...     return outputs</p> <p>Parameters:</p> Name Type Description Default <code>*keys</code> <p>List of keys to group by.</p> <code>()</code> <p>Returns:</p> Type Description <p>The decorator function.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def group_by_values(*keys):\n\"\"\"Decorator for grouping requests by values of selected keys.\n    Splits a batch into multiple sub-batches with the same values of selected keys and\n    calls the decorated function with each of them.\n    This is especially convenient when working with models that require dynamic parameters\n    sent by the user. For example, given an input of the form:\n        {\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n    Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:\n        [\n            {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n            {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n        ]\n    This decorator should be used after the @batch decorator.\n    Typical use:\n        @batch\n        @group_by_values(\"param1\", \"param2\")\n        def infer_fun(**inputs):\n            ...\n            return outputs\n    Args:\n        *keys: List of keys to group by.\n    Returns:\n        The decorator function.\n    \"\"\"\ndef _value2key(_v):\nreturn _v.tobytes() if isinstance(_v, np.ndarray) else _v\ndef _get_sort_key_for_sample(_request, _sample_idx: int):\nreturn tuple(_value2key(_request[_key][_sample_idx]) for _key in keys)\ndef _group_request(\n_request: InferenceRequest, _batch_size: int\n) -&gt; typing.Generator[Tuple[Tuple[int, ...], InferenceRequest], None, None]:\nidx_inputs = [(sample_idx, _get_sort_key_for_sample(_request, sample_idx)) for sample_idx in range(_batch_size)]\nidx_inputs.sort(key=operator.itemgetter(1))\nfor _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\n_samples_idxes, _ = zip(*group)\ngrouped_request = {input_name: value[_samples_idxes, ...] for input_name, value in _request.items()}\nyield _samples_idxes, grouped_request\n@wrapt.decorator\ndef _wrapper(wrapped, instance, args, kwargs):\nwrappers_stack = [\ncallable_with_wrapper.wrapper\nfor callable_with_wrapper in _get_wrapt_stack(wrapped)\nif callable_with_wrapper.wrapper is not None\n]\nif batch in wrappers_stack:\nraise PyTritonRuntimeError(\"The @group_by_values decorator must be used after the @batch decorator.\")\nrequest = {k: v for k, v in kwargs.items() if k not in _SPECIAL_KEYS}\nother_kwargs = {k: v for k, v in kwargs.items() if k in _SPECIAL_KEYS}\nbatch_size = get_inference_request_batch_size(request)\nresult = {}\nfor samples_idxes, _grouped_sub_request in _group_request(request, batch_size):\ninterim_result = wrapped(*args, **_grouped_sub_request, **other_kwargs)\nfor key, result_data in interim_result.items():\nresult.setdefault(key, np.empty((batch_size, *result_data.shape[1:]), dtype=result_data.dtype))\nresult[key][samples_idxes, ...] = result_data  # make copy here\nreturn result\nreturn _wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.pad_batch","title":"<code>pad_batch(wrapped, instance, args, kwargs)</code>","text":"<p>Add padding to the inputs batches.</p> <p>Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or max batch size from model config whatever is closer to current input size).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef pad_batch(wrapped, instance, args, kwargs):\n\"\"\"Add padding to the inputs batches.\n    Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or\n    max batch size from model config whatever is closer to current input size).\n    \"\"\"\ninputs = {k: v for k, v in kwargs.items() if k != \"__triton_context__\"}\nfirst_input = next(iter(inputs.values()))\nconfig = get_model_config(wrapped, instance)\nbatch_sizes = (\n[]\nif (config.batcher is None or config.batcher.preferred_batch_size is None)\nelse sorted(config.batcher.preferred_batch_size)\n)\nbatch_sizes.append(config.max_batch_size)\nbatch_size = batch_sizes[bisect_left(batch_sizes, first_input.shape[0])]\nnew_inputs = {\ninput_name: np.repeat(\ninput_array,\nnp.concatenate(\n[np.ones(input_array.shape[0] - 1), np.array([batch_size - input_array.shape[0] + 1])]\n).astype(np.int64),\naxis=0,\n)\nfor input_name, input_array in inputs.items()\n}\nkwargs.update(new_inputs)\nreturn wrapped(*args, **kwargs)\n</code></pre>"},{"location":"api/#pytriton.decorators.sample","title":"<code>sample(wrapped, instance, args, kwargs)</code>","text":"<p>Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.</p> <p>Decorator takes first request and convert it into named inputs. Useful with non-batching models - instead of one element list of request, we will get named inputs - <code>kwargs</code>.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef sample(wrapped, instance, args, kwargs):\n\"\"\"Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.\n    Decorator takes first request and convert it into named inputs.\n    Useful with non-batching models - instead of one element list of request, we will get named inputs - `kwargs`.\n    \"\"\"\nkwargs.update(args[0][0])\noutputs = wrapped(*args[1:], **kwargs)\noutputs = convert_output(outputs, wrapped, instance)\nreturn [outputs]\n</code></pre>"},{"location":"api/#pytriton.decorators.triton_context","title":"<code>triton_context(wrapped, instance, args, kwargs)</code>","text":"<p>Adds triton context.</p> <p>It gives you additional argument passed to the function in **kwargs called 'triton_context'. You can read model config from it and in the future possibly have some interaction with triton.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef triton_context(wrapped, instance, args, kwargs):\n\"\"\"Adds triton context.\n    It gives you additional argument passed to the function in **kwargs called 'triton_context'.\n    You can read model config from it and in the future possibly have some interaction with triton.\n    \"\"\"\nkwargs[TRITON_CONTEXT_FIELD_NAME] = get_triton_context(wrapped, instance)\nreturn wrapped(*args, **kwargs)\n</code></pre>"},{"location":"api/#pytriton.triton.Triton","title":"<code>pytriton.triton.Triton(*, config=None, workspace=None)</code>","text":"<p>Triton Inference Server for Python models.</p> <p>Initialize Triton Inference Server context for starting server and loading models.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[TritonConfig]</code> <p>TritonConfig object with optional customizations for Triton Inference Server. Configuration can be passed also through environment variables. See TritonConfig.from_env() class method for details.</p> <p>Order of precedence:</p> <ul> <li>config defined through <code>config</code> parameter of init method.</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul> <code>None</code> <code>workspace</code> <code>Union[Workspace, str, pathlib.Path, None]</code> <p>workspace or path where the Triton Model Store and files used by pytriton will be created. If workspace is <code>None</code> random workspace will be created. Workspace will be deleted in Triton.stop().</p> <code>None</code> Source code in <code>pytriton/triton.py</code> <pre><code>def __init__(\nself, *, config: Optional[TritonConfig] = None, workspace: Union[Workspace, str, pathlib.Path, None] = None\n):\n\"\"\"Initialize Triton Inference Server context for starting server and loading models.\n    Args:\n        config: TritonConfig object with optional customizations for Triton Inference Server.\n            Configuration can be passed also through environment variables.\n            See [TritonConfig.from_env()][pytriton.triton.TritonConfig.from_env] class method for details.\n            Order of precedence:\n              - config defined through `config` parameter of init method.\n              - config defined in environment variables\n              - default TritonConfig values\n        workspace: workspace or path where the Triton Model Store and files used by pytriton will be created.\n            If workspace is `None` random workspace will be created.\n            Workspace will be deleted in [Triton.stop()][pytriton.triton.Triton.stop].\n    \"\"\"\ndef _without_none_values(_d):\nreturn {name: value for name, value in _d.items() if value is not None}\ndefault_config_dict = _without_none_values(TritonConfig().to_dict())\nenv_config_dict = _without_none_values(TritonConfig.from_env().to_dict())\nexplicit_config_dict = _without_none_values(config.to_dict() if config else {})\nconfig_dict = {**default_config_dict, **env_config_dict, **explicit_config_dict}\nself._config = TritonConfig(**config_dict)\nself._workspace = workspace if isinstance(workspace, Workspace) else Workspace(workspace)\nmodel_repository = TritonModelRepository(path=self._config.model_repository, workspace=self._workspace)\nself._model_manager = ModelManager(model_repository)\nself._triton_server_config = TritonServerConfig()\nconfig_data = self._config.to_dict()\nfor name, value in config_data.items():\nif name not in TritonServerConfig.allowed_keys() or value is None:\ncontinue\nself._triton_server_config[name] = value\nself._triton_server_config[\"model_repository\"] = model_repository.path.as_posix()\nself._triton_server_config[\"backend_directory\"] = (TRITONSERVER_DIST_DIR / \"backends\").as_posix()\nself._triton_server = TritonServer(\npath=(TRITONSERVER_DIST_DIR / \"bin/tritonserver\").as_posix(),\nlibs_path=get_libs_path(),\nconfig=self._triton_server_config,\n)\nself._cv = th.Condition()\nwith self._cv:\nself._stopped = True\nself.triton_context = TritonContext()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context.</p> <p>Returns:</p> Type Description <code>Triton</code> <p>A Triton object</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __enter__(self) -&gt; \"Triton\":\n\"\"\"Enter the context.\n    Returns:\n        A Triton object\n    \"\"\"\nreturn self\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.__exit__","title":"<code>__exit__(*_)</code>","text":"<p>Exit the context stopping the process and cleaning the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>*_</code> <p>unused arguments</p> <code>()</code> Source code in <code>pytriton/triton.py</code> <pre><code>def __exit__(self, *_) -&gt; None:\n\"\"\"Exit the context stopping the process and cleaning the workspace.\n    Args:\n        *_: unused arguments\n    \"\"\"\nself.stop()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.bind","title":"<code>bind(model_name, infer_func, inputs, outputs, model_version=1, config=None)</code>","text":"<p>Create a model with given name and inference callable binding into Triton Inference Server.</p> <p>More information about model configuration: https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md</p> <p>Parameters:</p> Name Type Description Default <code>infer_func</code> <code>Union[Callable, Sequence[Callable]]</code> <p>Inference callable to handle request/response from Triton Inference Server</p> required <code>inputs</code> <code>Sequence[Tensor]</code> <p>Definition of model inputs</p> required <code>outputs</code> <code>Sequence[Tensor]</code> <p>Definition of model outputs</p> required <code>model_name</code> <code>str</code> <p>Name under which model is available in Triton Inference Server. It can only contain</p> required <code>model_version</code> <code>int</code> <p>Version of model</p> <code>1</code> <code>config</code> <code>Optional[ModelConfig]</code> <p>Model configuration for Triton Inference Server deployment</p> <code>None</code> Source code in <code>pytriton/triton.py</code> <pre><code>def bind(\nself,\nmodel_name: str,\ninfer_func: Union[Callable, Sequence[Callable]],\ninputs: Sequence[Tensor],\noutputs: Sequence[Tensor],\nmodel_version: int = 1,\nconfig: Optional[ModelConfig] = None,\n) -&gt; None:\n\"\"\"Create a model with given name and inference callable binding into Triton Inference Server.\n    More information about model configuration:\n    https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md\n    Args:\n        infer_func: Inference callable to handle request/response from Triton Inference Server\n        (or list of inference callable for multi instance model)\n        inputs: Definition of model inputs\n        outputs: Definition of model outputs\n        model_name: Name under which model is available in Triton Inference Server. It can only contain\n        alphanumeric characters, dots, underscores and dashes.\n        model_version: Version of model\n        config: Model configuration for Triton Inference Server deployment\n    \"\"\"\nself._validate_model_name(model_name)\nmodel = Model(\nmodel_name=model_name,\nmodel_version=model_version,\ninference_fn=infer_func,\ninputs=inputs,\noutputs=outputs,\nconfig=config if config else ModelConfig(),\nworkspace=self._workspace,\ntriton_context=self.triton_context,\n)\nmodel.on_model_event(self._on_model_event)\nself._model_manager.add_model(model)\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.is_alive","title":"<code>is_alive()</code>","text":"<p>Verify is deployed models and server are alive.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if server and loaded models are alive, False otherwise.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def is_alive(self) -&gt; bool:\n\"\"\"Verify is deployed models and server are alive.\n    Returns:\n        True if server and loaded models are alive, False otherwise.\n    \"\"\"\nif not self._triton_server.is_alive():\nreturn False\nfor model in self._model_manager.models:\nif not model.is_alive():\nreturn False\nreturn True\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.run","title":"<code>run()</code>","text":"<p>Run Triton Inference Server.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def run(self) -&gt; None:\n\"\"\"Run Triton Inference Server.\"\"\"\nif not self._triton_server.is_alive():\nself._model_manager.create_models()\nself._triton_server.register_on_exit(self._on_tritonserver_exit)\natexit.register(self.stop)\nwith self._cv:\nself._triton_server.start()\nself._stopped = False\nself._wait_for_models()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.serve","title":"<code>serve(monitoring_period_sec=MONITORING_PERIOD_SEC)</code>","text":"<p>Run Triton Inference Server and lock thread for serving requests/response.</p> <p>Parameters:</p> Name Type Description Default <code>monitoring_period_sec</code> <code>int</code> <p>the timeout of monitoring if Triton and models are available. Every monitoring_period_sec seconds main thread wakes up and check if triton server and proxy backend are still alive and sleep again. If triton or proxy is not alive - method returns.</p> <code>MONITORING_PERIOD_SEC</code> Source code in <code>pytriton/triton.py</code> <pre><code>def serve(self, monitoring_period_sec: int = MONITORING_PERIOD_SEC) -&gt; None:\n\"\"\"Run Triton Inference Server and lock thread for serving requests/response.\n    Args:\n        monitoring_period_sec: the timeout of monitoring if Triton and models are available.\n            Every monitoring_period_sec seconds main thread wakes up and check if triton server and proxy backend\n            are still alive and sleep again. If triton or proxy is not alive - method returns.\n    \"\"\"\nself.run()\nwith self._cv:\nwhile self.is_alive():\nself._cv.wait(timeout=monitoring_period_sec)\nself.stop()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.stop","title":"<code>stop()</code>","text":"<p>Stop Triton Inference Server.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def stop(self) -&gt; None:\n\"\"\"Stop Triton Inference Server.\"\"\"\nwith self._cv:\nif self._stopped:\nreturn\nLOGGER.debug(\"Stopping Triton Inference server and proxy backends\")\nself._triton_server.stop()\nself._model_manager.clean()\nself._workspace.clean()\nself._stopped = True\nself._cv.notify_all()\nLOGGER.debug(\"Stopped Triton Inference server and proxy backends\")\natexit.unregister(self.stop)\n</code></pre>"},{"location":"api/#pytriton.model_config.tensor.Tensor","title":"<code>pytriton.model_config.tensor.Tensor</code>  <code>dataclass</code>","text":"<p>Model input and output definition for Triton deployment.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple</code> <p>Shape of the input/output tensor.</p> required <code>dtype</code> <code>Union[np.dtype, Type[np.dtype], Type[object]]</code> <p>Data type of the input/output tensor.</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the input/output of model.</p> <code>None</code> <code>optional</code> <code>Optional[bool]</code> <p>Flag to mark if input is optional.</p> <code>False</code>"},{"location":"api/#pytriton.model_config.tensor.Tensor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Override object values on post init or field override.</p> Source code in <code>pytriton/model_config/tensor.py</code> <pre><code>def __post_init__(self):\n\"\"\"Override object values on post init or field override.\"\"\"\nif isinstance(self.dtype, np.dtype):\nobject.__setattr__(self, \"dtype\", self.dtype.type)  # pytype: disable=attribute-error\n</code></pre>"},{"location":"api/#pytriton.model_config.common","title":"<code>pytriton.model_config.common</code>","text":"<p>Common structures for internal and external ModelConfig.</p>"},{"location":"api/#pytriton.model_config.common.DeviceKind","title":"<code>DeviceKind</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Device kind for model deployment.</p> <p>Parameters:</p> Name Type Description Default <code>KIND_AUTO</code> <p>Automatically select the device for model deployment.</p> required <code>KIND_CPU</code> <p>Model is deployed on CPU.</p> required <code>KIND_GPU</code> <p>Model is deployed on GPU.</p> required"},{"location":"api/#pytriton.model_config.common.DynamicBatcher","title":"<code>DynamicBatcher</code>  <code>dataclass</code>","text":"<p>Dynamic batcher configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> <code>0</code> <code>preferred_batch_size</code> <code>Optional[list]</code> <p>Preferred batch sizes for dynamic batching.</p> <code>None</code> <code>preserve_ordering</code> <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> <code>False</code> <code>priority_levels</code> <code>int</code> <p>The number of priority levels to be enabled for the model.</p> <code>0</code> <code>default_priority_level</code> <code>int</code> <p>The priority level used for requests that don't specify their priority.</p> <code>0</code> <code>default_queue_policy</code> <code>Optional[QueuePolicy]</code> <p>The default queue policy used for requests.</p> <code>None</code> <code>priority_queue_policy</code> <code>Optional[Dict[int, QueuePolicy]]</code> <p>Specify the queue policy for the priority level.</p> <code>None</code>"},{"location":"api/#pytriton.model_config.common.QueuePolicy","title":"<code>QueuePolicy</code>  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>timeout_action</code> <code>TimeoutAction</code> <p>The action applied to timed-out request.</p> <code>TimeoutAction.REJECT</code> <code>default_timeout_microseconds</code> <code>int</code> <p>The default timeout for every request, in microseconds.</p> <code>0</code> <code>allow_timeout_override</code> <code>bool</code> <p>Whether individual request can override the default timeout value.</p> <code>False</code> <code>max_queue_size</code> <code>int</code> <p>The maximum queue size for holding requests.</p> <code>0</code>"},{"location":"api/#pytriton.model_config.common.TimeoutAction","title":"<code>TimeoutAction</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Parameters:</p> Name Type Description Default <code>REJECT</code> <p>Reject the request and return error message accordingly.</p> required <code>DELAY</code> <p>Delay the request until all other requests at the same (or higher) priority levels that have not reached their timeouts are processed.</p> required"},{"location":"api/#pytriton.model_config.model_config.ModelConfig","title":"<code>pytriton.model_config.model_config.ModelConfig</code>  <code>dataclass</code>","text":"<p>Additional model configuration for running model through Triton Inference Server.</p> <p>Parameters:</p> Name Type Description Default <code>batching</code> <code>bool</code> <p>Flag to enable/disable batching for model.</p> <code>True</code> <code>max_batch_size</code> <code>int</code> <p>The maximal batch size that would be handled by model.</p> <code>4</code> <code>batcher</code> <code>DynamicBatcher</code> <p>Configuration of Dynamic Batching for the model.</p> <code>DynamicBatcher()</code> <code>response_cache</code> <code>bool</code> <p>Flag to enable/disable response cache for the model</p> <code>False</code>"},{"location":"api/#pytriton.client.client","title":"<code>pytriton.client.client</code>","text":"<p>Clients for easy interaction with models deployed on the Triton Inference Server.</p> Typical usage example <p>with ModelClient(\"localhost\", \"MyModel\") as client:     result_dict = client.infer_sample(input_a=a, input_b=b)</p> Inference inputs can be provided either as positional or keyword arguments <p>result_dict = client.infer_sample(input1, input2) result_dict = client.infer_sample(a=input1, b=input2)</p> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p>"},{"location":"api/#pytriton.client.client.ModelClient","title":"<code>ModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=_DEFAULT_INIT_TIMEOUT_S)</code>","text":"<p>Synchronous client for model deployed on the Triton Inference Server.</p> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> Common usage <p>with ModelClient(\"localhost\", \"BERT\") as client     result_dict = client.infer_sample(input1_sample, input2_sample)</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> required <code>model_name</code> <code>str</code> <p>name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> <code>None</code> <code>lazy_init</code> <code>bool</code> <p>if initialization should be performed just before sending first request to inference server.</p> <code>True</code> <code>init_timeout_s</code> <code>float</code> <p>timeout for server and model being ready.</p> <code>_DEFAULT_INIT_TIMEOUT_S</code> <p>Raises:</p> Type Description <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientUrlParseError</code> <p>In case of problems with parsing url.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\nself,\nurl: str,\nmodel_name: str,\nmodel_version: Optional[str] = None,\n*,\nlazy_init: bool = True,\ninit_timeout_s: float = _DEFAULT_INIT_TIMEOUT_S,\n):\n\"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n    Common usage:\n      with ModelClient(\"localhost\", \"BERT\") as client\n          result_dict = client.infer_sample(input1_sample, input2_sample)\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for server and model being ready.\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError:\n            if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\nif not isinstance(url, str):\nraise PyTritonClientUrlParseError(f\"Could not parse url {url}\")\nparsed_url = urllib.parse.urlparse(url)\nif not parsed_url.scheme or parsed_url.scheme.lower() not in [\"grpc\", \"http\"]:\n_LOGGER.debug(f\"Adding http scheme to {url}\")\nparsed_url = urllib.parse.urlparse(f\"http://{url}\")\nport = parsed_url.port or {\"grpc\": DEFAULT_GRPC_PORT, \"http\": DEFAULT_HTTP_PORT}[parsed_url.scheme.lower()]\nself._url = f\"{parsed_url.hostname}:{port}\"\nself._model_name = model_name\nself._model_version = model_version\nself._triton_client_lib = {\"grpc\": tritonclient.grpc, \"http\": tritonclient.http}[parsed_url.scheme.lower()]\n_LOGGER.debug(f\"Creating InferenceServerClient for {parsed_url.scheme}://{self._url}\")\n# Monkey patch __del__ method from client to catch error in client when instance is garbage collected.\n# This is needed because we are closing client in __exit__ method or in close method.\n# (InferenceClient uses gevent library which does not support closing twice from different threads)\nself._monkey_patch_client()\nself._client = self._triton_client_lib.InferenceServerClient(self._url)\nself._request_id_generator = itertools.count(0)\nself._init_timeout_s = init_timeout_s\nself._model_config = None\nself._model_ready = None\nself._lazy_init = lazy_init\nif not self._lazy_init:\nself._wait_and_init_model_config(self._init_timeout_s)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.model_config","title":"<code>model_config</code>  <code>property</code>","text":"<p>Obtain configuration of model deployed on the Triton Inference Server.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"api/#pytriton.client.client.ModelClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Create context for use _ModelClientBase as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n\"\"\"Create context for use _ModelClientBase as a context manager.\"\"\"\nreturn self\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.__exit__","title":"<code>__exit__(*_)</code>","text":"<p>Close resources used by _ModelClientBase when exiting from context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, *_):\n\"\"\"Close resources used by _ModelClientBase when exiting from context.\"\"\"\nself.close()\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.close","title":"<code>close()</code>","text":"<p>Close resources used by _ModelClientBase.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self):\n\"\"\"Close resources used by _ModelClientBase.\"\"\"\n_LOGGER.debug(\"Closing InferenceServerClient\")\nself._client.close()\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.infer_batch","title":"<code>infer_batch(*inputs, **named_inputs)</code>","text":"<p>Run synchronous inference on batched data.</p> Typical usage <p>with ModelClient(\"localhost\", \"MyModel\") as client:     result_dict = client.infer_sample(input1, input2)</p> Inference inputs can be provided either as positional or keyword arguments <p>result_dict = client.infer_batch(input1, input2) result_dict = client.infer_batch(a=input1, b=input2)</p> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>inference inputs provided as positional arguments.</p> <code>()</code> <code>**named_inputs</code> <p>inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientModelDoesntSupportBatchingError</code> <p>if model doesn't support batching.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(self, *inputs, **named_inputs) -&gt; Dict[str, np.ndarray]:\n\"\"\"Run synchronous inference on batched data.\n    Typical usage:\n        with ModelClient(\"localhost\", \"MyModel\") as client:\n            result_dict = client.infer_sample(input1, input2)\n    Inference inputs can be provided either as positional or keyword arguments:\n        result_dict = client.infer_batch(input1, input2)\n        result_dict = client.infer_batch(a=input1, b=input2)\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        **named_inputs: inference inputs provided as named arguments.\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelDoesntSupportBatchingError: if model doesn't support batching.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n_verify_inputs_args(inputs, named_inputs)\nmodel_supports_batching = self.model_config.max_batch_size &gt; 0\nif not model_supports_batching:\nraise PyTritonClientModelDoesntSupportBatchingError(\nf\"Model {self.model_config.model_name} doesn't support batching - use infer_sample method instead\"\n)\nreturn self._infer(inputs or named_inputs)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.infer_sample","title":"<code>infer_sample(*inputs, **named_inputs)</code>","text":"<p>Run synchronous inference on single data sample.</p> Typical usage <p>with ModelClient(\"localhost\", \"MyModel\") as client:     result_dict = client.infer_sample(input1, input2)</p> Inference inputs can be provided either as positional or keyword arguments <p>result_dict = client.infer_sample(input1, input2) result_dict = client.infer_sample(a=input1, b=input2)</p> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>inference inputs provided as positional arguments.</p> <code>()</code> <code>**named_inputs</code> <p>inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, np.ndarray]</code> <p>dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(self, *inputs, **named_inputs) -&gt; Dict[str, np.ndarray]:\n\"\"\"Run synchronous inference on single data sample.\n    Typical usage:\n        with ModelClient(\"localhost\", \"MyModel\") as client:\n            result_dict = client.infer_sample(input1, input2)\n    Inference inputs can be provided either as positional or keyword arguments:\n        result_dict = client.infer_sample(input1, input2)\n        result_dict = client.infer_sample(a=input1, b=input2)\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        **named_inputs: inference inputs provided as named arguments.\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n_verify_inputs_args(inputs, named_inputs)\nmodel_supports_batching = self.model_config.max_batch_size &gt; 0\nif model_supports_batching:\nif inputs:\ninputs = tuple(data[np.newaxis, ...] for data in inputs)\nelif named_inputs:\nnamed_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\nresult = self._infer(inputs or named_inputs)\nif model_supports_batching:\nresult = {name: data[0] for name, data in result.items()}\nreturn result\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.wait_for_model","title":"<code>wait_for_model(timeout_s)</code>","text":"<p>Wait for Triton Inference Server and deployed on it model readiness.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>timeout to server and model get into readiness state.</p> required <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If server and model are not in readiness state before given timeout.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>KeyboardInterrupt</code> <p>If hosting process receives SIGINT</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float):\n\"\"\"Wait for Triton Inference Server and deployed on it model readiness.\n    Args:\n        timeout_s: timeout to server and model get into readiness state.\n    Raises:\n        PyTritonClientTimeoutError: If server and model are not in readiness state before given timeout.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\nwait_for_model_ready(self._client, self._model_name, self._model_version, timeout_s=timeout_s)\n</code></pre>"},{"location":"building/","title":"Building binary package","text":""},{"location":"building/#building-binary-package-from-source","title":"Building binary package from source","text":"<p>This guide provides an outline of the process for building the PyTriton binary package from source. It offers the flexibility to modify the PyTriton code and integrate it with various versions of the Triton Inference Server, including custom builds.</p>"},{"location":"building/#prerequisites","title":"Prerequisites","text":"<p>Before building the PyTriton binary package, ensure the following:</p> <ul> <li>Docker is installed on the system. For more information, refer to the Docker documentation.</li> <li>Access to the Docker daemon is available from the system or container.</li> </ul>"},{"location":"building/#building-pytriton-binary-package","title":"Building PyTriton binary package","text":"<p>To build the wheel binary package, follow these steps from the root directory of the project:</p> <pre><code>make install-dev\nmake dist\n</code></pre> <p>The wheel package will be located in the <code>dist</code> directory. To install the library, run the following <code>pip</code> command:</p> <pre><code>pip install dist/nvidia_pytriton-*-py3-none-*_x86_64.whl\n</code></pre>"},{"location":"building/#building-for-a-specific-triton-inference-server-version","title":"Building for a specific Triton Inference Server version","text":"<p>Building for an unsupported OS or hardware platform is possible. PyTriton requires a Python backend and either an HTTP or gRPC endpoint. The build can be CPU-only, as inference is performed on Inference Handlers.</p> <p>For more information on the Triton Inference Server build process, refer to the building section of Triton Inference Server documentation.</p> <p>Untested Build</p> <p>The Triton Inference Server has only been rigorously tested on Ubuntu 20.04. Other OS and hardware platforms are not officially supported. You can test the build by following the steps outlined in the Triton Inference Server testing guide.</p> <p>Using the following docker method steps, you can create a <code>tritonserver:latest</code> Docker image that can be used to build PyTriton with the following command:</p> <p>By the following docker method steps you can create a <code>tritonserver:latest</code> Docker image that can be used to build PyTriton with the following command:</p> <pre><code>make TRITONSERVER_IMAGE_NAME=tritonserver:latest dist\n</code></pre>"},{"location":"deploying_models/","title":"Deploying Models","text":""},{"location":"deploying_models/#deploying-models","title":"Deploying Models","text":"<p>The following page provides more details about possible options for configuring the Triton Inference Server, configuring the model for loading in Triton, and deploying the solution in Docker containers or clusters.</p>"},{"location":"deploying_models/#examples","title":"Examples","text":"<p>Before you move to more advanced topics, you may want to review examples that provide an implementation of various models (in JAX, Python, PyTorch, and TensorFlow) deployed using the library.</p> <p>You can also find the usage of Perf Analyzer for profiling models (throughput, latency) once deployed using the solution.</p> <p>For more information, please review the Examples page.</p>"},{"location":"deploying_models/#configuring-triton","title":"Configuring Triton","text":"<p>The Triton class is the base entry point for working with Triton Inference Server.</p>"},{"location":"deploying_models/#initialization","title":"Initialization","text":"<p>Connecting Python models with Triton Inference Server working in the current environment requires creating a Triton object. This can be done by creating a context:</p> <pre><code>from pytriton.triton import Triton\nwith Triton() as triton:\n...\n</code></pre> <p>or simply creating an object:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n</code></pre> <p>The Triton Inference Server behavior can be configured by passing config parameter:</p> <pre><code>import pathlib\nfrom pytriton.triton import Triton, TritonConfig\ntriton_config = TritonConfig(log_file=pathlib.Path(\"/tmp/triton.log\"))\nwith Triton(config=triton_config) as triton:\n...\n</code></pre> <p>and through environment variables, for example, set as in the command below:</p> <pre><code>PYTRITON_TRITON_CONFIG_LOG_VERBOSITY=4 python my_script.py\n</code></pre> <p>The order of precedence of configuration methods is:</p> <ul> <li>config defined through <code>config</code> parameter of Triton class <code>__init__</code> method</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul>"},{"location":"deploying_models/#blocking-mode","title":"Blocking mode","text":"<p>The blocking mode will stop the execution of the current thread and wait for incoming HTTP/gRPC requests for inference execution. This mode makes your application behave as a pure server. The example of using blocking mode:</p> <pre><code>from pytriton.triton import Triton\nwith Triton() as triton:\n...  # Load models here\ntriton.serve()\n</code></pre>"},{"location":"deploying_models/#background-mode","title":"Background mode","text":"<p>The background mode runs Triton as a subprocess and does not block the execution of the current thread. In this mode, you can run Triton Inference Server and interact with it from the current context. The example of using background mode:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n...  # Load models here\ntriton.run()  # Triton Server started\nprint(\"This print will appear\")\ntriton.stop()  # Triton Server stopped\n</code></pre>"},{"location":"deploying_models/#loading-models","title":"Loading models","text":"<p>The Triton class provides methods to load one or multiple models to the Triton server:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n@batch\ndef infer_fn(**inputs: np.ndarray):\ninput1, input2 = inputs.values()\noutputs = model(input1, input2)\nreturn [outputs]\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"ModelName\",\ninfer_func=infer_fn,\ninputs=[\nTensor(shape=(1,), dtype=np.bytes_),  # sample containing single bytes value\nTensor(shape=(-1,), dtype=np.bytes_)  # sample containing vector of bytes\n],\noutputs=[\nTensor(shape=(-1,), dtype=np.float32),\n],\nconfig=ModelConfig(max_batch_size=8)\n)\n</code></pre> <p>The <code>bind</code> method's mandatory arguments are:</p> <ul> <li><code>model_name</code>: defines under which name the model is available in Triton Inference Server</li> <li><code>infer_func</code>: function or Python <code>Callable</code> object which obtains the data passed in the request and returns the output</li> <li><code>inputs</code>: defines the number, types, and shapes for model inputs</li> <li><code>outputs</code>: defines the number, types, and shapes for model outputs</li> <li><code>config</code>: more customization for model deployment and behavior on the Triton server</li> </ul> <p>Once the <code>bind</code> method is called, the model is created in the Triton Inference Server model store under the provided <code>model_name</code>.</p>"},{"location":"deploying_models/#inference-callable","title":"Inference Callable","text":"<p>The inference callable is an entry point for inference. This can be any callable that receives the data for model inputs in the form of a list of request dictionaries where input names are mapped into ndarrays. Input can be also adapted to different more convenient forms using a set of decorators. More details about designing inference callable and using of decorators can be found in Inference Callable Design page.</p> <p>In the simplest implementation for functionality that passes input data on output, a lambda can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"Identity\",\ninfer_func=lambda requests: requests,\ninputs=[Tensor(dtype=np.float32, shape=(1,))],\noutputs=[Tensor(dtype=np.float32, shape=(1,))],\nconfig=ModelConfig(max_batch_size=8)\n)\n</code></pre>"},{"location":"deploying_models/#multi-instance-model-inference","title":"Multi-instance model inference","text":"<p>Multi-instance model inference is a mechanism for loading multiple instances of the same model and calling them alternately (to hide transfer overhead).</p> <p>With the <code>Triton</code> class, it can be realized by providing the list of multiple inference callables to <code>Triton.bind</code> in the <code>infer_func</code> parameter.</p> <p>The example presents multiple instances of the Linear PyTorch model loaded on separate devices.</p> <p>First, define the wrapper class for the inference handler. The class initialization receives a model and device as arguments. The inference handling is done by method <code>__call__</code> where the <code>model</code> instance is called:</p> <pre><code>import torch\nfrom pytriton.decorators import batch\nclass _InferFuncWrapper:\ndef __init__(self, model: torch.nn.Module, device: str):\nself._model = model\nself._device = device\n@batch\ndef __call__(self, **inputs):\n(input1_batch,) = inputs.values()\ninput1_batch_tensor = torch.from_numpy(input1_batch).to(self._device)\noutput1_batch_tensor = self._model(input1_batch_tensor)\noutput1_batch = output1_batch_tensor.cpu().detach().numpy()\nreturn [output1_batch]\n</code></pre> <p>Next, create a factory function where a model and instances of <code>_InferFuncWrapper</code> are created - one per each device:</p> <pre><code>def _infer_function_factory(devices):\ninfer_fns = []\nfor device in devices:\nmodel = torch.nn.Linear(20, 30).to(device).eval()\ninfer_fns.append(_InferFuncWrapper(model=model, device=device))\nreturn infer_fns\n</code></pre> <p>Finally, the list of callable objects is passed to <code>infer_func</code> parameter of the <code>Triton.bind</code> function:</p> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"Linear\",\ninfer_func=_infer_function_factory(devices=[\"cuda\", \"cpu\"]),\ninputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\noutputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\nconfig=ModelConfig(max_batch_size=16),\n)\n...\n</code></pre> <p>Once the multiple callable objects are passed to <code>infer_func</code>, the Triton server gets information that multiple instances of the same model have been created. The incoming requests are distributed among created instances. In our case executing two instances of a <code>Linear</code> model loaded on CPU and GPU devices.</p>"},{"location":"deploying_models/#inputs-and-outputs","title":"Inputs and Outputs","text":"<p>The integration of the Python model requires the inputs and outputs types of the model. This is required to correctly map the input and output data passed through the Triton Inference Server.</p> <p>The simplest definition of model inputs and outputs expects providing the type of data and the shape per input:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\ninputs = [\nTensor(dtype=np.float32, shape=(-1,)),\n]\noutput = [\nTensor(dtype=np.float32, shape=(-1,)),\nTensor(dtype=np.int32, shape=(-1,)),\n]\n</code></pre> <p>The provided configuration creates the following tensors:</p> <ul> <li>Single input:</li> <li>name: INPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>Two outputs:</li> <li>name: OUTPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>name: OUTPUT_2, data type: INT32, shape=(-1,)</li> </ul> <p>The <code>-1</code> means a dynamic shape of the input or output.</p> <p>To define the name of the input and its exact shape, the following definition can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\ninputs = [\nTensor(name=\"image\", dtype=np.float32, shape=(224, 224, 3)),\n]\noutputs = [\nTensor(name=\"class\", dtype=np.int32, shape=(1000,)),\n]\n</code></pre> <p>This definition describes that the model has:</p> <ul> <li>a single input named <code>image</code> of size 224x224x3 and 32-bit floating-point data type</li> <li>a single output named <code>class</code> of size 1000 and 32-bit integer data type.</li> </ul> <p>The <code>dtype</code> parameter can be either <code>numpy.dtype</code>, <code>numpy.dtype.type</code>, or <code>str</code>. For example:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\ntensor1 = Tensor(name=\"tensor1\", shape=(-1,), dtype=np.float32),\ntensor2 = Tensor(name=\"tensor2\", shape=(-1,), dtype=np.float32().dtype),\ntensor3 = Tensor(name=\"tensor3\", shape=(-1,), dtype=\"float32\"),\n</code></pre> <p>dtype for bytes and string inputs/outputs</p> <p>When using the <code>bytes</code> dtype, NumPy removes trailing <code>\\x00</code> bytes. Therefore, for arbitrary bytes, it is required to use <code>object</code> dtype.</p> <pre><code>&gt; np.array([b\"\\xff\\x00\"])\narray([b'\\xff'], dtype='|S2')\n\n&gt; np.array([b\"\\xff\\x00\"], dtype=object)\narray([b'\\xff\\x00'], dtype=object)\n</code></pre> <p>For ease of use, for encoded string values, users might use <code>bytes</code> dtype.</p>"},{"location":"deploying_models/#unrecoverable-errors","title":"Unrecoverable errors","text":"<p>When the model gets into a state where further inference is impossible, you can throw PyTritonUnrecoverableError from the inference callable. This will cause NVIDIA Triton Inference Server to shut down. This might be useful when the model is deployed on a cluster in a multi-node setup. In that case to recover the model you need to restart all \"workers\" on the cluster.</p> <p>When the model gets into a state where further inference is impossible, you can throw the PyTritonUnrecoverableError from the inference callable. This will cause the NVIDIA Triton Inference Server to shut down. This might be useful when the model is deployed on a cluster in a multi-node setup. In that case, to recover the model, you need to restart all \"workers\" on the cluster.</p> <pre><code>from typing import Dict\nimport numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.exceptions import PyTritonUnrecoverableError\n@batch\ndef infer_fn(**inputs: np.ndarray) -&gt; Dict[str, np.ndarray]:\n...\ntry:\noutputs = model(**inputs)\nexcept Exception as e:\nraise PyTritonUnrecoverableError(\n\"Some unrecoverable error occurred, \"\n\"thus no further inferences are possible.\"\n) from e\n...\nreturn outputs\n</code></pre>"},{"location":"deploying_models/#model-configuration","title":"Model Configuration","text":"<p>The additional model configuration for running a model through the Triton Inference Server can be provided in the <code>config</code> argument in the <code>bind</code> method. This section describes the possible configuration enhancements. The configuration of the model can be adjusted by overriding the defaults for the <code>ModelConfig</code> object.</p> <pre><code>from pytriton.model_config.common import DynamicBatcher\nclass ModelConfig:\nbatching: bool = True\nmax_batch_size: int = 4\nbatcher: DynamicBatcher = DynamicBatcher()\nresponse_cache: bool = False\n</code></pre>"},{"location":"deploying_models/#batching","title":"Batching","text":"<p>The batching feature collects one or more samples and passes them to the model together. The model processes multiple samples at the same time and returns the output for all the samples processed together.</p> <p>Batching can significantly improve throughput. Processing multiple samples at the same time leverages the benefits of utilizing GPU performance for inference.</p> <p>The Triton Inference Server is responsible for collecting multiple incoming requests into a single batch. The batch is passed to the model, which improves the inference performance (throughput and latency). This feature is called <code>dynamic batching</code>, which collects samples from multiple clients into a single batch processed by the model.</p> <p>On the PyTriton side, the <code>infer_fn</code> obtain the fully created batch by Triton Inference Server so the only responsibility is to perform computation and return the output.</p> <p>By default, batching is enabled for the model. The default behavior for Triton is to have dynamic batching enabled. If your model does not support batching, use <code>batching=False</code> to disable it in Triton.</p>"},{"location":"deploying_models/#maximal-batch-size","title":"Maximal batch size","text":"<p>The maximal batch size defines the number of samples that can be processed at the same time by the model. This configuration has an impact not only on throughput but also on memory usage, as a bigger batch means more data loaded to the memory at the same time.</p> <p>The <code>max_batch_size</code> has to be a value greater than or equal to 1.</p>"},{"location":"deploying_models/#dynamic-batching","title":"Dynamic batching","text":"<p>The dynamic batching is a Triton Inference Server feature and can be configured by defining the <code>DynamicBatcher</code> object:</p> <pre><code>from typing import Dict, Optional\nfrom pytriton.model_config.common import QueuePolicy\nclass DynamicBatcher:\nmax_queue_delay_microseconds: int = 0\npreferred_batch_size: Optional[list] = None\npreserve_ordering: bool = False\npriority_levels: int = 0\ndefault_priority_level: int = 0\ndefault_queue_policy: Optional[QueuePolicy] = None\npriority_queue_policy: Optional[Dict[int, QueuePolicy]] = None\n</code></pre> <p>More about dynamic batching can be found in the Triton Inference Server documentation and API spec</p>"},{"location":"deploying_models/#response-cache","title":"Response cache","text":"<p>The Triton Inference Server provides functionality to use a cached response for the model. To use the response cache:</p> <ul> <li>provide the <code>response_cache_byte_size</code> in <code>TritonConfig</code></li> <li>set <code>response_cache=True</code> in <code>ModelConfig</code></li> </ul> <p>Example:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\ntriton_config = TritonConfig(\nresponse_cache_byte_size=1024 * 1024,  # 1 MB\n)\n@batch\ndef _add_sub(**inputs):\na_batch, b_batch = inputs.values()\nadd_batch = a_batch + b_batch\nsub_batch = a_batch - b_batch\nreturn {\"add\": add_batch, \"sub\": sub_batch}\nwith Triton(config=triton_config) as triton:\ntriton.bind(\nmodel_name=\"AddSub\",\ninfer_func=_add_sub,\ninputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\noutputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=8, response_cache=True)\n)\n...\n</code></pre>"},{"location":"deploying_models/#deploying-in-cluster","title":"Deploying in Cluster","text":"<p>The library can be used inside containers and deployed on Kubernetes clusters. There are certain prerequisites and information that would help deploy the library in your cluster.</p>"},{"location":"deploying_models/#health-checks","title":"Health checks","text":"<p>The library uses the Triton Inference Server to handle HTTP/gRPC requests. Triton Server provides endpoints to validate if the server is ready and in a healthy state. The following API endpoints can be used in your orchestrator to control the application ready and live states:</p> <ul> <li>Ready: <code>/v2/health/ready</code></li> <li>Live: <code>/v2/health/live</code></li> </ul>"},{"location":"deploying_models/#exposing-ports","title":"Exposing ports","text":"<p>The library uses the Triton Inference Server, which exposes the HTTP, gRPC, and metrics ports for communication. In the default configuration, the following ports have to be exposed:</p> <ul> <li>8000 for HTTP</li> <li>8001 for gRPC</li> <li>8002 for metrics</li> </ul> <p>If the library is inside a Docker container, the ports can be exposed by passing an extra argument to the <code>docker run</code> command. An example of passing ports configuration:</p> <pre><code>docker run -p 8000:8000 -p 8001:8001 -p 8002:8002 {image}\n</code></pre> <p>To deploy a container in Kubernetes, add a ports definition for the container in YAML deployment configuration:</p> <pre><code>containers:\n- name: pytriton\n...\nports:\n- containerPort: 8000\nname: http\n- containerPort: 8001\nname: grpc\n- containerPort: 8002\nname: metrics\n</code></pre>"},{"location":"deploying_models/#configuring-shared-memory","title":"Configuring shared memory","text":"<p>The connection between Python callbacks and the Triton Inference Server uses shared memory to pass data between the processes. In the Docker container, the default amount of shared memory is 64MB, which may not be enough to pass input and output data of the model. To increase the available shared memory size, pass an additional flag to the <code>docker run</code> command. An example of increasing the shared memory size to 8GB:</p> <p><pre><code>docker run --shm-size 8GB {image}\n</code></pre> To increase the shared memory size for Kubernetes, the following configuration can be used:</p> <pre><code>spec:\nvolumes:\n- name: shared-memory\nemptyDir:\nmedium: Memory\ncontainers:\n- name: pytriton\n...\nvolumeMounts:\n- mountPath: /dev/shm\nname: shared-memory\n</code></pre>"},{"location":"deploying_models/#specify-container-init-process","title":"Specify container init process","text":"<p>You can use the <code>--init</code> flag of the <code>docker run</code> command to indicate that an init process should be used as the PID 1 in the container. Specifying an init process ensures that reaping zombie processes are performed inside the container. The reaping zombie processes functionality is important in case of an unexpected error occurrence in scripts hosting PyTriton.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide simple examples on how to integrate PyTorch, TensorFlow2, JAX, and simple Python models with the Triton Inference Server using PyTriton. The examples are available in the GitHub repository.</p>"},{"location":"examples/#samples-models-deployment","title":"Samples Models Deployment","text":"<p>The list of example models deployments:</p> <ul> <li>Add-Sub Python model</li> <li>Add-Sub Python model Jupyter Notebook</li> <li>BART PyTorch from HuggingFace</li> <li>BERT JAX from HuggingFace</li> <li>Identity Python model</li> <li>Linear RAPIDS/CuPy model</li> <li>Linear RAPIDS/CuPy model Jupyter Notebook</li> <li>Linear PyTorch model</li> <li>Multi-Layer TensorFlow2</li> <li>Multi Instance deployment for Linear PyTorch model</li> <li>Multi Model deployment for Python models</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> </ul>"},{"location":"examples/#profiling-models","title":"Profiling models","text":"<p>The Perf Analyzer can be used to profile the models served through PyTriton. We have prepared an example of using Perf Analyzer to profile BART PyTorch. See the example code in the GitHub repository.</p>"},{"location":"examples/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>The following examples contain a guide on how to deploy them on a Kubernetes cluster:</p> <ul> <li>BART PyTorch from HuggingFace</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> </ul>"},{"location":"inference_callable/","title":"Inference Callable Design","text":""},{"location":"inference_callable/#inference-callable-design","title":"Inference Callable Design","text":"<p>This document provides guidelines for creating an inference callable for PyTriton, which serves as the entry point for handling inference requests.</p>"},{"location":"inference_callable/#inference-callable","title":"Inference Callable","text":"<p>The inference callable is an entry point for handling inference requests. The interface of the inference callable assumes it receives a list of requests as dictionaries, where each dictionary represents one request mapping model input names to NumPy ndarrays.</p> <p>There are two common implementations for inference callables:</p> <ol> <li> <p>Functions:</p> <pre><code>import numpy as np\nfrom typing import Dict, List\ndef infer_fn(requests: List[Dict[str, np.ndarray]]) -&gt; List[Dict[str, np.ndarray]]:\n...\n</code></pre> </li> <li> <p>Class:</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nclass InferCallable:\ndef __call__(self, requests: List[Dict[str, np.ndarray]]) -&gt; List[Dict[str, np.ndarray]]:\n...\n</code></pre> </li> </ol> <p>To use the inference callable with PyTriton, it must be bound to a Triton server instance using the <code>bind</code> method:</p> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"MyInferenceFn\",\ninfer_func=infer_fn,\ninputs=[Tensor(shape=(1, ), dtype=np.float32)],\noutputs=[Tensor(shape=(1, ), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=8)\n)\ninfer_callable = InferCallable()\ntriton.bind(\nmodel_name=\"MyInferenceCallable\",\ninfer_func=infer_callable,\ninputs=[Tensor(shape=(1, ), dtype=np.float32)],\noutputs=[Tensor(shape=(1, ), dtype=np.float32)],\nconfig=ModelConfig(max_batch_size=8)\n)\n</code></pre> <p>For more information on serving the inference callable, refer to the Loading models section on Deploying Models page.</p>"},{"location":"inference_callable/#batching-decorator","title":"Batching decorator","text":"<p>In many cases, it is more convenient to receive input already batched in the form of a NumPy array instead of a list of separate requests. For such cases, we have prepared the <code>@batch</code> decorator that adapts generic input into a batched form. It passes kwargs to the inference function where each named input contains a NumPy array with a batch of requests received by the Triton server.</p> <p>Below, we show the difference between decorated and undecorated functions bound with Triton:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n# Sample input data with 2 requests - each with 2 inputs\ninput_data = [\n{'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])},\n{'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])}\n]\ndef undecorated_identity_fn(requests):\nprint(requests)\n# As expected, requests = [\n#     {'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])},\n#     {'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])},\n# ]\nresults = requests\nreturn results\n@batch\ndef decorated_identity_fn(in1, in2):\nprint(in1, in2)\n# in1 = np.array([[1, 1], [1, 2]])\n# in2 = np.array([[2, 2], [2, 3]])\n# Inputs are batched by `@batch` decorator and passed to the function as kwargs, so they can be automatically mapped\n# with in1, in2 function parameters\n# Of course, we could get these params explicitly with **kwargs like this:\n# def decorated_infer_fn(**kwargs):\nreturn {\"out1\": in1, \"out2\": in2}\nundecorated_identity_fn(input_data)\ndecorated_identity_fn(input_data)\n</code></pre> <p>More examples using the <code>@batch</code> decorator with different frameworks are shown below.</p> <p>Example implementation for TensorFlow model:</p> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom pytriton.decorators import batch\n@batch\ndef infer_tf_fn(**inputs: np.ndarray):\n(images_batch,) = inputs.values()\nimages_batch_tensor = tf.convert_to_tensor(images_batch)\noutput1_batch = model.predict(images_batch_tensor)\nreturn [output1_batch]\n</code></pre> <p>Example implementation for PyTorch model:</p> <pre><code>import numpy as np\nimport torch\nfrom pytriton.decorators import batch\n@batch\ndef infer_pt_fn(**inputs: np.ndarray):\n(input1_batch,) = inputs.values()\ninput1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\noutput1_batch_tensor = model(input1_batch_tensor)\noutput1_batch = output1_batch_tensor.cpu().detach().numpy()\nreturn [output1_batch]\n</code></pre> <p>Example implementation with named inputs and outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n@batch\ndef add_subtract_fn(a: np.ndarray, b: np.ndarray):\nreturn {\"add\": a + b, \"sub\": a - b}\n@batch\ndef multiply_fn(**inputs: np.ndarray):\na = inputs[\"a\"]\nb = inputs[\"b\"]\nreturn [a * b]\n</code></pre> <p>Example implementation with strings:</p> <pre><code>import numpy as np\nfrom transformers import pipeline\nfrom pytriton.decorators import batch\nCLASSIFIER = pipeline(\"zero-shot-classification\", model=\"facebook/bart-base\", device=0)\n@batch\ndef classify_text_fn(text_array: np.ndarray):\ntext = text_array[0]  # text_array contains one string at index 0\ntext = text.decode(\"utf-8\")  # string is stored in byte array encoded in utf-8\nresult = CLASSIFIER(text)\nreturn [np.array(result)]  # return statistics generated by classifier\n</code></pre>"},{"location":"inference_callable/#additional-decorators-for-adapting-input-to-user-needs","title":"Additional decorators for adapting input to user needs","text":"<p>We have prepared several useful decorators for converting generic request input into common user needs. You can create custom decorators tailored to your requirements and chain them with other decorators.</p> <p>Our standard decorators implement three types of interfaces:</p> <ol> <li> <p>Receive a list of request dictionaries and return a list of response dictionaries (dictionaries map input/output names to NumPy arrays)</p> <ul> <li><code>@group_by_keys</code> - groups requests with the same set of keys and calls the wrapped function for each group separately. This decorator is convenient to use before batching because the batching decorator requires a consistent set of inputs as it stacks them into batches.</li> </ul> <pre><code>from pytriton.decorators import batch, group_by_keys\n@group_by_keys\n@batch\ndef infer_fn(mandatory_input, optional_input=None):\n# perform inference\npass\n</code></pre> <ul> <li><code>@group_by_values(*keys)</code> - groups requests with the same input value (for selected keys) and calls the wrapped function for each group separately. This decorator is particularly useful with models requiring dynamic parameters sent by users, such as temperature. In this case, we want to run the model only for requests with the same temperature value.</li> </ul> <pre><code>from pytriton.decorators import batch, group_by_values\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n# perform inference\npass\n</code></pre> <ul> <li><code>@fill_optionals(**defaults)</code> - fills missing inputs in requests with default values provided by the user. If model owners have default values for some optional parameters, it's a good idea to provide them at the beginning, so other decorators can create larger consistent groups and send them to the inference callable.</li> </ul> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, group_by_values\n@fill_optionals(temperature=np.array([10.0]))\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n# perform inference\npass\n</code></pre> </li> <li> <p>Receive a list of request dictionaries and return a dictionary that maps input names to arrays (passes the dictionary to the wrapped infer function as named arguments - <code>kwargs</code>):</p> <ul> <li><code>@batch</code> - generates a batch from input requests.</li> <li><code>@sample</code> - takes the first request and converts it into named inputs. This decorator is useful with non-batching models. Instead of a one-element list of requests, we get named inputs - <code>kwargs</code> (usage is shown in previous examples).</li> </ul> </li> <li> <p>Receive a batch (a dictionary that maps input names to arrays) and return a batch after some processing:</p> <ul> <li><code>@pad_batch</code> - appends the last row to the input multiple times to achieve the desired batch size (preferred batch size or max batch size from the model config, whichever is closer to the current input size).</li> </ul> <pre><code>from pytriton.decorators import batch, pad_batch\n@batch\n@pad_batch\ndef infer_fn(mandatory_input):\n# this model requires mandatory_input batch to be the size provided in the model config\npass\n</code></pre> <ul> <li><code>@first_value</code> - this decorator takes the first elements from batches for selected inputs specified by the <code>keys</code> parameter.   If the value is a one-element array, it is converted to a scalar value.   This decorator is convenient to use with dynamic model parameters that users send in requests.   You can use <code>@group_by_values</code> before to have batches with the same values in each batch.</li> </ul> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_values\n@fill_optionals(temperature=np.array([10.0]))\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\ndef infer_fn(mandatory_input, temperature):\n# perform inference with scalar temperature=10\npass\n</code></pre> </li> <li> <p>The <code>@triton_context</code> decorator provides an additional argument called <code>triton_context</code>,    from which you can read the model config.</p> <pre><code>from pytriton.decorators import triton_context\n@triton_context\ndef infer_fn(input_list, **kwargs):\nmodel_config = kwargs['triton_context'].model_config\n# perform inference using some information from model_config\npass\n</code></pre> </li> </ol> <p>Here is an example of stacking multiple decorators together. We recommend starting with type 1 decorators, followed by types 2 and 3. Place the <code>@triton_context</code> decorator last in the chain.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_keys, group_by_values, triton_context\n@fill_optionals(temperature=np.array([10.0]))\n@group_by_keys\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\n@triton_context\ndef infer(triton_context, mandatory_input, temperature, opt1=None, opt2=None):\nmodel_config = triton_context.model_config\n# perform inference using:\n#   - some information from model_config\n#   - scalar temperature value\n#   - optional parameters opt1 and/or opt2\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This page explains how to install the library. We assume that you have a basic understanding of the Python programming language and are familiar with machine learning models. Using Docker is optional but not required.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the library, ensure that you meet the following requirements:</p> <ul> <li>An operating system with glibc &gt;= 2.31. Triton Inference Server and PyTriton have only been rigorously tested on Ubuntu 20.04. Other supported operating systems include Ubuntu 20.04+, Debian 11+, Rocky Linux 9+, and Red Hat Universal Base Image 9+.</li> <li>Python version &gt;= 3.8. If you are using Python 3.9+, see the section \"Installation on Python 3.9+\" for additional steps.</li> <li>pip &gt;= 20.3</li> </ul> <p>The library can be installed in the system environment, a virtual environment, or a Docker image. NVIDIA optimized Docker images for Python frameworks can be obtained from the NVIDIA NGC Catalog. If you want to use the Docker runtime, we recommend that you install NVIDIA Container Toolkit to enable running model inference on NVIDIA GPU.</p>"},{"location":"installation/#installing-using-pip","title":"Installing using pip","text":"<p>You can install the package from pypi.org by running the following command:</p> <pre><code>pip install -U nvidia-pytriton\n</code></pre> <p>Important: The Triton Inference Server binary is installed as part of the PyTriton package.</p>"},{"location":"installation/#installation-on-python-39","title":"Installation on Python 3.9+","text":"<p>The Triton Inference Server Python backend is linked to a fixed Python 3.8. Therefore, if you want to install PyTriton on a different version of Python, you need to prepare the environment for the Triton Inference Server Python backend. The environment should be located in the <code>~/.cache/pytriton/python_backend_interpreter</code> directory and should contain the packages <code>numpy~=1.21</code> and <code>pyzmq~=23.0</code>.</p>"},{"location":"installation/#using-pyenv","title":"Using pyenv","text":"<pre><code>apt update\n# need git and build dependencies https://github.com/pyenv/pyenv/wiki\\#suggested-build-environment\nDEBIAN_FRONTEND=noninteractive apt install -y python3 python3-distutils python-is-python3 git \\\nbuild-essential libssl-dev zlib1g-dev \\\nlibbz2-dev libreadline-dev libsqlite3-dev curl \\\nlibncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n\n# install pyenv\ncurl https://pyenv.run | bash\nexport PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n# compile python 3.8\npyenv install 3.8\n\n# prepare venv\npyenv global 3.8\npip3 install virtualenv\nmkdir -p ~/.cache/pytriton/\npython -mvenv ~/.cache/pytriton/python_backend_interpreter --copies --clear\nsource ~/.cache/pytriton/python_backend_interpreter/bin/activate\npip3 install numpy~=1.21 pyzmq~=23.0\n\n# recover system python\ndeactivate\npyenv global system\n</code></pre>"},{"location":"installation/#using-miniconda","title":"Using miniconda","text":"<pre><code>apt update\napt install -y python3 python3-distutils python-is-python3 curl\n\nCONDA_VERSION=latest\nTARGET_MACHINE=x86_64\ncurl \"https://repo.anaconda.com/miniconda/Miniconda3-${CONDA_VERSION}-Linux-${TARGET_MACHINE}.sh\" --output miniconda.sh\n\nsh miniconda.sh -b -p ~/.cache/conda\nrm miniconda.sh\n~/.cache/conda/bin/conda create -y -p ~/.cache/pytriton/python_backend_interpreter python=3.8 numpy~=1.21 pyzmq~=23.0\n</code></pre>"},{"location":"installation/#building-binaries-from-source","title":"Building binaries from source","text":"<p>The binary package can be built from the source, which enables modifications to the PyTriton code, as well as the integration of other versions of the Triton Inference Server, including custom builds. For further information on building the PyTriton binary, refer to the Building page.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>There is no one-to-one match between our solution and Triton Inference Server features, especially in terms of supporting a user model store.</li> <li>Currently, only the x86-64 instruction set architecture is supported.</li> </ul>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>The prerequisite for this page is to install PyTriton, which can be found in the installation page.</p> <p>The Quick Start presents how to run a Python model in the Triton Inference Server without needing to change the current working environment. In this example, we are using a simple <code>Linear</code> PyTorch model.</p> <p>The integration of the model requires providing the following elements:</p> <ul> <li>The model - a framework or Python model or function that handles inference requests</li> <li>Inference Callable - function or class with <code>__call__</code> method, that handles the input data coming from Triton and returns the result</li> <li>Python function connection with Triton Inference Server - a binding for communication between Triton and the Inference Callable</li> </ul> <p>The requirement for the example is to have PyTorch installed in your environment. You can do this by running:</p> <pre><code>pip install torch\n</code></pre> <p>In the next step, define the <code>Linear</code> model:</p> <pre><code>import torch\nmodel = torch.nn.Linear(2, 3).to(\"cuda\").eval()\n</code></pre> <p>In the second step, create an inference callable as a function. The function obtains the HTTP/gRPC request data in the form of a NumPy array as an argument. The expected return object is also a NumPy array.</p> <p>Example implementation:</p> <pre><code>import numpy as np\nimport torch\nfrom pytriton.decorators import batch\n@batch\ndef infer_fn(**inputs: np.ndarray):\n(input1_batch,) = inputs.values()\ninput1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\noutput1_batch_tensor = model(input1_batch_tensor) # Calling the Python model inference\noutput1_batch = output1_batch_tensor.cpu().detach().numpy()\nreturn [output1_batch]\n</code></pre> <p>In the next step, create the connection between the model and Triton Inference Server using the bind method:</p> <pre><code>from pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n# Connecting inference callback with Triton Inference Server\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"Linear\",\ninfer_func=infer_fn,\ninputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\noutputs=[\nTensor(dtype=np.float32, shape=(-1,)),\n],\nconfig=ModelConfig(max_batch_size=128)\n)\n...\n</code></pre> <p>Finally, serve the model with the Triton Inference Server:</p> <pre><code>from pytriton.triton import Triton\nwith Triton() as triton:\n...  # Load models here\ntriton.serve()\n</code></pre> <p>The <code>bind</code> method creates a connection between the Triton Inference Server and the <code>infer_fn</code>, which handles the inference queries. The <code>inputs</code> and <code>outputs</code> describe the model inputs and outputs that are exposed in Triton. The config field allows more parameters for model deployment.</p> <p>The <code>serve</code> method is blocking, and at this point, the application waits for incoming HTTP/gRPC requests. From that moment, the model is available under the name <code>Linear</code> in the Triton server. The inference queries can be sent to <code>localhost:8000/v2/models/Linear/infer</code>, which are passed to the <code>infer_fn</code> function.</p> <p>If you would like to use Triton in the background mode, use <code>run</code>. More about that can be found in the Deploying Models page.</p> <p>Once the <code>serve</code> or <code>run</code> method is called on the <code>Triton</code> object, the server status can be obtained using:</p> <pre><code>curl -v localhost:8000/v2/health/live\n</code></pre> <p>The model is loaded right after the server starts, and its status can be queried using:</p> <pre><code>curl -v localhost:8000/v2/models/Linear/ready\n</code></pre> <p>Finally, you can send an inference query to the model:</p> <pre><code>curl -X POST \\\n-H \"Content-Type: application/json\"  \\\n-d @input.json \\\nlocalhost:8000/v2/models/Linear/infer\n</code></pre> <p>The <code>input.json</code> with sample query:</p> <pre><code>{\n\"id\": \"0\",\n\"inputs\": [\n{\n\"name\": \"INPUT_1\",\n\"shape\": [1, 2],\n\"datatype\": \"FP32\",\n\"parameters\": {},\n\"data\": [[-0.04281254857778549, 0.6738349795341492]]\n}\n]\n}\n</code></pre> <p>Read more about the HTTP/gRPC interface in the Triton Inference Server documentation.</p> <p>You can also validate the deployed model using a simple client that can perform inference requests:</p> <pre><code>import torch\nfrom pytriton.client import ModelClient\ninput1_data = torch.randn(128, 2).cpu().detach().numpy()\nwith ModelClient(\"localhost:8000\", \"Linear\") as client:\nresult_dict = client.infer_batch(input1_data)\nprint(result_dict)\n</code></pre> <p>The full example code can be found in examples/linear_random_pytorch.</p> <p>More information about running the server and models can be found in Deploying Models page.</p>"}]}