{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pytriton","title":"PyTriton","text":"<p>PyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. The library allows serving Machine Learning models directly from Python through NVIDIA's Triton Inference Server.</p>"},{"location":"#how-it-works","title":"How it works?","text":"<p>In PyTriton, as in Flask or FastAPI, you can define any Python function that executes a machine learning model prediction and exposes it through an HTTP/gRPC API. PyTriton installs Triton Inference Server in your environment and uses it for handling HTTP/gRPC requests and responses. Our library provides a Python API that allows attaching a Python function to Triton and a communication layer to send/receive data between Triton and the function. This solution helps utilize the performance features of Triton Inference Server, such as dynamic batching or response cache, without changing your model environment. Thus, it improves the performance of running inference on GPU for models implemented in Python. The solution is framework-agnostic and can be used along with frameworks like PyTorch, TensorFlow, or JAX.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The diagram below presents the schema of how the Python models are served through Triton Inference Server using PyTriton. The solution consists of two main components:</p> <ul> <li>Triton Inference Server: for exposing the HTTP/gRPC API and benefiting from performance features like dynamic batching or response cache.</li> <li>Python Model Environment: your environment where the Python model is executed.</li> </ul> <p>The Triton Inference Server binaries are provided as part of the PyTriton installation. The Triton Server is installed in your current environment (system or container). The PyTriton controls the Triton Server process through the <code>Triton Controller</code>.</p> <p>Exposing the model through PyTriton requires the definition of an <code>Inference Callable</code> - a Python function that is connected to Triton Inference Server and executes the model or ensemble for predictions. The integration layer binds the <code>Inference Callable</code> to Triton Server and exposes it through the Triton HTTP/gRPC API under a provided <code>&lt;model name&gt;</code>. Once the integration is done, the defined <code>Inference Callable</code> receives data sent to the HTTP/gRPC API endpoint <code>v2/models/&lt;model name&gt;/infer</code>. Read more about HTTP/gRPC interface in Triton Inference Server documentation.</p> <p>The HTTP/gRPC requests sent to <code>v2/models/&lt;model name&gt;/infer</code> are handled by Triton Inference Server. The server batches requests and passes them to the <code>Proxy Backend</code>, which sends the batched requests to the appropriate <code>Inference Callable</code>. The data is sent as a <code>numpy</code> array. Once the <code>Inference Callable</code> finishes execution of the model prediction, the result is returned to the <code>Proxy Backend</code>, and a response is created by Triton Server.</p> <p></p>"},{"location":"#serving-the-models","title":"Serving the models","text":"<p>PyTriton provides an option to serve your Python model using Triton Inference Server to handle HTTP/gRPC requests and pass the input/output tensors to and from the model. We use a blocking mode where the application is a long-lived process deployed in your cluster to serve the requests from clients.</p> <p>Before you run the model for serving the inference callback function, it has to be defined. The inference callback receives the inputs and should return the model outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    input1, input2 = inputs.values()\n    outputs = model(input1, input2)\n    return [outputs]\n</code></pre> <p>The <code>infer_fn</code> receives the batched input data for the model and should return the batched outputs.</p> <p>In the next step, you need to create a connection between Triton and the model. For that purpose, the <code>Triton</code> class has to be used, and the <code>bind</code> method is required to be called to create a dedicated connection between Triton Inference Server and the defined <code>infer_fn</code>.</p> <p>In the blocking mode, we suggest using the <code>Triton</code> object as a context manager where multiple models can be loaded in the way presented below:</p> <pre><code>from pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\n\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"MyModel\",\n        infer_func=infer_fn,\n        inputs=[\n            Tensor(dtype=bytes, shape=(1,)),  # sample containing single bytes value\n            Tensor(dtype=bytes, shape=(-1,)),  # sample containing vector of bytes\n        ],\n        outputs=[\n            Tensor(dtype=np.float32, shape=(-1,)),\n        ],\n        config=ModelConfig(max_batch_size=16),\n    )\n</code></pre> <p>At this point, you have defined how the model has to be handled by Triton and where the HTTP/gRPC requests for the model have to be directed. The last part for serving the model is to call the <code>serve</code> method on the Triton object:</p> <pre><code>with Triton() as triton:\n    # ...\n    triton.serve()\n</code></pre> <p>When the <code>.serve()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p>"},{"location":"#working-in-the-jupyter-notebook","title":"Working in the Jupyter Notebook","text":"<p>The package provides an option to work with your model inside the Jupyter Notebook. We call it a background mode where the model is deployed on Triton Inference Server for handling HTTP/gRPC requests, but there are other actions that you want to perform after loading and starting serving the model.</p> <p>Having the <code>infer_fn</code> defined in the same way as described in the serving the models section, you can use the <code>Triton</code> object without a context:</p> <pre><code>from pytriton.triton import Triton\ntriton = Triton()\n</code></pre> <p>In the next step, the model has to be loaded for serving in Triton Inference Server (which is also the same as in the serving example):</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    input1, input2 = inputs.values()\n    outputs = input1 + input2\n    return [outputs]\n\ntriton.bind(\n    model_name=\"MyModel\",\n    infer_func=infer_fn,\n    inputs=[\n        Tensor(shape=(1,), dtype=np.float32),\n        Tensor(shape=(-1,), dtype=np.float32),\n    ],\n    outputs=[Tensor(shape=(-1,), dtype=np.float32)],\n    config=ModelConfig(max_batch_size=16),\n)\n</code></pre> <p>Finally, to run the model in background mode, use the <code>run</code> method:</p> <pre><code>triton.run()\n</code></pre> <p>When the <code>.run()</code> method is called on the <code>Triton</code> object, the inference queries can be sent to <code>localhost:8000/v2/models/MyModel</code>, and the <code>infer_fn</code> is called to handle the inference query.</p> <p>The Triton server can be stopped at any time using the <code>stop</code> method:</p> <pre><code>triton.stop()\n</code></pre>"},{"location":"#what-next","title":"What next?","text":"<p>Read more about using PyTriton in the Quick Start, Examples and find more options on how to configure Triton, models, and deployment on a cluster in the Deploying Models section.</p> <p>The details about classes and methods can be found in the API Reference page.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#041-2023-11-09","title":"0.4.1 (2023-11-09)","text":"<ul> <li>New: Place where workspaces with temporary Triton model repositories and communication file sockets can be configured by <code>$PYTRITON_HOME</code> environment variable</li> <li>Fix: Recover handling <code>KeyboardInterrupt</code> in <code>triton.serve()</code></li> <li>Fix: Remove limit for handling bytes dtype tensors</li> <li>Build scripts update</li> <li> <p>Added support for arm64 platform builds</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.39.0</p> </li> </ul>"},{"location":"CHANGELOG/#040-2023-10-20","title":"0.4.0 (2023-10-20)","text":"<ul> <li>New: Remote Mode - PyTriton can be used to connect to a remote Triton Inference Server</li> <li>Introduced RemoteTriton class which can be used to connect to a remote Triton Inference Server     running on the same machine, by passing triton url.</li> <li>Changed Triton lifecycle - now the Triton Inference Server is started while entering the context.     This allows to load models dynamically to the running server while calling the bind method.     It is still allowed to create Triton instance without entering the context and bind models before starting     the server (in this case the models are lazy loaded when calling run or serve method like it worked before).</li> <li>In RemoteTriton class, calling enter or connect method connects to triton server, so we can safely load models     while binding inference functions (if RemoteTriton is used without context manager, models are lazy loaded     when calling connect or serve method).</li> <li>Change: <code>@batch</code> decorator raises a <code>ValueError</code> if any of the outputs have a different batch size than expected.</li> <li> <p>fix: gevent resources leak in <code>FuturesModelClient</code></p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.36.0</p> </li> </ul>"},{"location":"CHANGELOG/#031-2023-09-26","title":"0.3.1 (2023-09-26)","text":"<ul> <li>Change: <code>KeyboardInterrupt</code> is now handled in <code>triton.serve()</code>. PyTriton hosting scripts return an exit code of 0 instead of 130 when they receive a SIGINT signal.</li> <li> <p>Fix: Addressed potential instability in shared memory management.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.36.0</p> </li> </ul>"},{"location":"CHANGELOG/#030-2023-09-05","title":"0.3.0 (2023-09-05)","text":"<ul> <li>new: Support for multiple Python versions starting from 3.8+</li> <li>new: Added support for decoupled models enabling to support streaming models (alpha state)</li> <li> <p>change: Upgraded Triton Inference Server binaries to version 2.36.0. Note that this Triton Inference Server requires glibc 2.35+ or a more recent version.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.36.0</p> </li> </ul>"},{"location":"CHANGELOG/#025-2023-08-24","title":"0.2.5 (2023-08-24)","text":"<ul> <li>new: Allow to execute multiple PyTriton instances in the same process and/or host</li> <li> <p>fix: Invalid flags for Proxy Backend configuration passed to Triton</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#024-2023-08-10","title":"0.2.4 (2023-08-10)","text":"<ul> <li>new: Introduced <code>strict</code> flag in <code>Triton.bind</code> which enables data types and shapes validation of inference callable outputs   against model config</li> <li>new: <code>AsyncioModelClient</code> which works in FastAPI and other async frameworks</li> <li>fix: <code>FuturesModelClient</code> do not raise <code>gevent.exceptions.InvalidThreadUseError</code></li> <li> <p>fix: Do not throw TimeoutError if could not connect to server during model verification</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#023-2023-07-21","title":"0.2.3 (2023-07-21)","text":"<ul> <li>Improved verification of Proxy Backend environment when running under same Python interpreter</li> <li> <p>Fixed pytriton.version to represent currently installed version</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#022-2023-07-19","title":"0.2.2 (2023-07-19)","text":"<ul> <li>Added <code>inference_timeout_s</code> parameters to client classes</li> <li>Renamed <code>PyTritonClientUrlParseError</code> to <code>PyTritonClientInvalidUrlError</code></li> <li><code>ModelClient</code> and <code>FuturesModelClient</code> methods raise <code>PyTritonClientClosedError</code> when used after client is closed</li> <li>Pinned tritonclient dependency due to issues with tritonclient &gt;= 2.34 on systems with glibc version lower than 2.34</li> <li> <p>Added warning after Triton Server setup and teardown while using too verbose logging level as it may cause a significant performance drop in model inference</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#021-2023-06-28","title":"0.2.1 (2023-06-28)","text":"<ul> <li>Fixed handling <code>TritonConfig.cache_directory</code> option - the directory was always overwritten with the default value.</li> <li>Fixed tritonclient dependency - PyTriton need tritonclient supporting http headers and parameters</li> <li> <p>Improved shared memory usage to match 64MB limit (default value for Docker, Kubernetes) reducing the initial size for PyTriton Proxy Backend.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#020-2023-05-30","title":"0.2.0 (2023-05-30)","text":"<ul> <li>Added support for using custom HTTP/gRPC request headers and parameters.</li> </ul> <p>This change breaks backward compatibility of the inference function signature.   The undecorated inference function now accepts a list of <code>Request</code> instances instead   of a list of dictionaries. The <code>Request</code> class contains data for inputs and parameters   for combined parameters and headers.</p> <p>See docs/custom_params.md for further information</p> <ul> <li>Added <code>FuturesModelClient</code> which enables sending inference requests in a parallel manner.</li> <li> <p>Added displaying documentation link after models are loaded.</p> </li> <li> <p>Version of Triton Inference Server embedded in wheel: 2.33.0</p> </li> </ul>"},{"location":"CHANGELOG/#015-2023-05-12","title":"0.1.5 (2023-05-12)","text":"<ul> <li>Improved <code>pytriton.decorators.group_by_values</code> function</li> <li>Modified the function to avoid calling the inference callable on each individual sample when grouping by string/bytes input</li> <li>Added <code>pad_fn</code> argument for easy padding and combining of the inference results</li> <li>Fixed Triton binaries search</li> <li> <p>Improved Workspace management (remove workspace on shutdown)</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#014-2023-03-16","title":"0.1.4 (2023-03-16)","text":"<ul> <li>Add validation of the model name passed to Triton bind method.</li> <li> <p>Add monkey patching of <code>InferenceServerClient.__del__</code> method to prevent unhandled exceptions.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#013-2023-02-20","title":"0.1.3 (2023-02-20)","text":"<ul> <li> <p>Fixed getting model config in <code>fill_optionals</code> decorator.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#012-2023-02-14","title":"0.1.2 (2023-02-14)","text":"<ul> <li>Fixed wheel build to support installations on operating systems with glibc version 2.31 or higher.</li> <li>Updated the documentation on custom builds of the package.</li> <li>Change: TritonContext instance is shared across bound models and contains model_configs dictionary.</li> <li> <p>Fixed support of binding multiple models that uses methods of the same class.</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#011-2023-01-31","title":"0.1.1 (2023-01-31)","text":"<ul> <li>Change: The <code>@first_value</code> decorator has been updated with new features:</li> <li>Renamed from <code>@first_values</code> to <code>@first_value</code></li> <li>Added a <code>strict</code> flag to toggle the checking of equality of values on a single selected input of the request. Default is True</li> <li>Added a <code>squeeze_single_values</code> flag to toggle the squeezing of single value ND arrays to scalars. Default is True</li> <li>Fix: <code>@fill_optionals</code> now supports non-batching models</li> <li>Fix: <code>@first_value</code> fixed to work with optional inputs</li> <li>Fix: <code>@group_by_values</code> fixed to work with string inputs</li> <li> <p>Fix: <code>@group_by_values</code> fixed to work per sample-wise</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#010-2023-01-12","title":"0.1.0 (2023-01-12)","text":"<ul> <li> <p>Initial release of PyTriton</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>Triton Inference Server: 2.29.0</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.     Refer to its support matrix     for a detailed summary.</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/pytriton/issues.</p> <p>When reporting a bug, please include the following information:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Browse through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The PyTriton could always use more documentation, whether as part of the official PyTriton docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/pytriton/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes:</p> <pre><code>$ git commit -s -m \"Add a cool feature.\"\n</code></pre> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the following:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":""},{"location":"CONTRIBUTING/#local-development","title":"Local Development","text":"<p>Ready to contribute? Here's how to set up the <code>PyTriton</code> for local development.</p> <ol> <li>Fork the <code>PyTriton</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/pytriton.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, here's how you set up your fork for local development:</li> </ol> <pre><code>$ mkvirtualenv pytriton\n$ cd pytriton/\n</code></pre> <p>If you do not use the virtualenvwrapper package, you can initialize a virtual environment using the pure Python command:</p> <pre><code>$ python -m venv pytriton\n$ cd pytriton/\n$ source bin/activate\n</code></pre> <p>Once the virtualenv is activated, install the development dependencies:</p> <pre><code>$ make install-dev\n</code></pre> <ol> <li>Extract Triton Server to your environment so you can debug PyTriton while serving some models on Triton:</li> </ol> <pre><code>$ make extract-triton\n</code></pre> <ol> <li>Install pre-commit hooks:</li> </ol> <pre><code>$ pre-commit install\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ make lint  # will run, among others, flake8 and pytype linters\n$ make test  # will run a test on your current virtualenv\n</code></pre> <p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_subset\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put your new functionality into a function with a docstring and add the feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in Google Style Guide.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>PyTriton requires that all contributors (or their corporate entity) send a signed copy of the Contributor License Agreement to triton-cla@nvidia.com.</p> <p>NOTE: Contributors with no company affiliation can fill <code>N/A</code> in the <code>Corporation Name</code> and <code>Corporation Address</code> fields.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#api-reference","title":"API Reference","text":""},{"location":"api/#pytriton.triton.TritonConfig","title":"<code>pytriton.triton.TritonConfig</code>  <code>dataclass</code>","text":"<p>Triton Inference Server configuration class for customization of server execution.</p> <p>The arguments are optional. If value is not provided the defaults for Triton Inference Server are used. Please, refer to https://github.com/triton-inference-server/server/ for more details.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>Optional[str]</code> <p>Identifier for this server.</p> <code>None</code> <code>log_verbose</code> <code>Optional[int]</code> <p>Set verbose logging level. Zero (0) disables verbose logging and values &gt;= 1 enable verbose logging.</p> <code>None</code> <code>log_file</code> <code>Optional[Path]</code> <p>Set the name of the log output file.</p> <code>None</code> <code>exit_timeout_secs</code> <code>Optional[int]</code> <p>Timeout (in seconds) when exiting to wait for in-flight inferences to finish.</p> <code>None</code> <code>exit_on_error</code> <code>Optional[bool]</code> <p>Exit the inference server if an error occurs during initialization.</p> <code>None</code> <code>strict_readiness</code> <code>Optional[bool]</code> <p>If true /v2/health/ready endpoint indicates ready if the server is responsive and all models are available.</p> <code>None</code> <code>allow_http</code> <code>Optional[bool]</code> <p>Allow the server to listen for HTTP requests.</p> <code>None</code> <code>http_address</code> <code>Optional[str]</code> <p>The address for the http server to bind to. Default is 0.0.0.0.</p> <code>None</code> <code>http_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for HTTP requests. Default is 8000.</p> <code>None</code> <code>http_header_forward_pattern</code> <code>Optional[str]</code> <p>The regular expression pattern that will be used for forwarding HTTP headers as inference request parameters.</p> <code>None</code> <code>http_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling HTTP requests.</p> <code>None</code> <code>allow_grpc</code> <code>Optional[bool]</code> <p>Allow the server to listen for GRPC requests.</p> <code>None</code> <code>grpc_address</code> <code>Optional[str]</code> <p>The address for the grpc server to binds to. Default is 0.0.0.0.</p> <code>None</code> <code>grpc_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for GRPC requests. Default is 8001.</p> <code>None</code> <code>grpc_header_forward_pattern</code> <code>Optional[str]</code> <p>The regular expression pattern that will be used for forwarding GRPC headers as inference request parameters.</p> <code>None</code> <code>grpc_infer_allocation_pool_size</code> <code>Optional[int]</code> <p>The maximum number of inference request/response objects that remain allocated for reuse. As long as the number of in-flight requests doesn't exceed this value there will be no allocation/deallocation of request/response objects.</p> <code>None</code> <code>grpc_use_ssl</code> <code>Optional[bool]</code> <p>Use SSL authentication for GRPC requests. Default is false.</p> <code>None</code> <code>grpc_use_ssl_mutual</code> <code>Optional[bool]</code> <p>Use mututal SSL authentication for GRPC requests. This option will preempt grpc_use_ssl if it is also specified. Default is false.</p> <code>None</code> <code>grpc_server_cert</code> <code>Optional[Path]</code> <p>File holding PEM-encoded server certificate. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_server_key</code> <code>Optional[Path]</code> <p>Path to file holding PEM-encoded server key. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_root_cert</code> <code>Optional[Path]</code> <p>Path to file holding PEM-encoded root certificate. Ignored unless grpc_use_ssl is true.</p> <code>None</code> <code>grpc_infer_response_compression_level</code> <code>Optional[str]</code> <p>The compression level to be used while returning the inference response to the peer. Allowed values are none, low, medium and high. Default is none.</p> <code>None</code> <code>grpc_keepalive_time</code> <code>Optional[int]</code> <p>The period (in milliseconds) after which a keepalive ping is sent on the transport.</p> <code>None</code> <code>grpc_keepalive_timeout</code> <code>Optional[int]</code> <p>The period (in milliseconds) the sender of the keepalive ping waits for an acknowledgement.</p> <code>None</code> <code>grpc_keepalive_permit_without_calls</code> <code>Optional[bool]</code> <p>Allows keepalive pings to be sent even if there are no calls in flight</p> <code>None</code> <code>grpc_http2_max_pings_without_data</code> <code>Optional[int]</code> <p>The maximum number of pings that can be sent when there is no data/header frame to be sent.</p> <code>None</code> <code>grpc_http2_min_recv_ping_interval_without_data</code> <code>Optional[int]</code> <p>If there are no data/header frames being sent on the transport, this channel argument on the server side controls the minimum time (in milliseconds) that gRPC Core would expect between receiving successive pings.</p> <code>None</code> <code>grpc_http2_max_ping_strikes</code> <code>Optional[int]</code> <p>Maximum number of bad pings that the server will tolerate before sending an HTTP2 GOAWAY frame and closing the transport.</p> <code>None</code> <code>grpc_restricted_protocol</code> <p>Specify restricted GRPC protocol setting. The format of this flag is ,=. Where  is a comma-separated list of protocols to be restricted.  will be additional header key to be checked when a GRPC request is received, and  is the value expected to be matched. required <code>allow_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide prometheus metrics.</p> <code>None</code> <code>allow_gpu_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide GPU metrics.</p> <code>None</code> <code>allow_cpu_metrics</code> <code>Optional[bool]</code> <p>Allow the server to provide CPU metrics.</p> <code>None</code> <code>metrics_interval_ms</code> <code>Optional[int]</code> <p>Metrics will be collected once every  milliseconds. <code>None</code> <code>metrics_port</code> <code>Optional[int]</code> <p>The port reporting prometheus metrics.</p> <code>None</code> <code>metrics_address</code> <code>Optional[str]</code> <p>The address for the metrics server to bind to. Default is the same as http_address.</p> <code>None</code> <code>allow_sagemaker</code> <code>Optional[bool]</code> <p>Allow the server to listen for Sagemaker requests.</p> <code>None</code> <code>sagemaker_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for Sagemaker requests.</p> <code>None</code> <code>sagemaker_safe_port_range</code> <code>Optional[str]</code> <p>Set the allowed port range for endpoints other than the SageMaker endpoints.</p> <code>None</code> <code>sagemaker_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling Sagemaker requests.</p> <code>None</code> <code>allow_vertex_ai</code> <code>Optional[bool]</code> <p>Allow the server to listen for Vertex AI requests.</p> <code>None</code> <code>vertex_ai_port</code> <code>Optional[int]</code> <p>The port for the server to listen on for Vertex AI requests.</p> <code>None</code> <code>vertex_ai_thread_count</code> <code>Optional[int]</code> <p>Number of threads handling Vertex AI requests.</p> <code>None</code> <code>vertex_ai_default_model</code> <code>Optional[str]</code> <p>The name of the model to use for single-model inference requests.</p> <code>None</code> <code>metrics_config</code> <code>Optional[List[str]]</code> <p>Specify a metrics-specific configuration setting. The format of this flag is =. It can be specified multiple times <code>None</code> <code>trace_config</code> <code>Optional[List[str]]</code> <p>Specify global or trace mode specific configuration setting. The format of this flag is ,=. Where  is either 'triton' or 'opentelemetry'. The default is 'triton'. To specify global trace settings (level, rate, count, or mode), the format would be =. For 'triton' mode, the server will use Triton's Trace APIs. For 'opentelemetry' mode, the server will use OpenTelemetry's APIs to generate, collect and export traces for individual inference requests. <code>None</code> <code>cache_config</code> <code>Optional[List[str]]</code> <p>Specify a cache-specific configuration setting. The format of this flag is ,=. Where  is the name of the cache, such as 'local' or 'redis'. Example: local,size=1048576 will configure a 'local' cache implementation with a fixed buffer pool of size 1048576 bytes. <code>None</code> <code>cache_directory</code> <code>Optional[str]</code> <p>The global directory searched for cache shared libraries. Default is '/opt/tritonserver/caches'. This directory is expected to contain a cache implementation as a shared library with the name 'libtritoncache.so'.</p> <code>None</code> <code>buffer_manager_thread_count</code> <code>Optional[int]</code> <p>The number of threads used to accelerate copies and other operations required to manage input and output tensor contents.</p> <code>None</code>"},{"location":"api/#pytriton.triton.TritonConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration for early error handling.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate configuration for early error handling.\"\"\"\n    if self.allow_http not in [True, None] and self.allow_grpc not in [True, None]:\n        raise PyTritonValidationError(\"The `http` or `grpc` endpoint has to be allowed.\")\n</code></pre>"},{"location":"api/#pytriton.triton.TritonConfig.from_dict","title":"<code>from_dict(config)</code>  <code>classmethod</code>","text":"<p>Creates a <code>TritonConfig</code> instance from an input dictionary. Values are converted into correct types.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>a dictionary with all required fields</p> required <p>Returns:</p> Type Description <code>TritonConfig</code> <p>a <code>TritonConfig</code> instance</p> Source code in <code>pytriton/triton.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: Dict[str, Any]) -&gt; \"TritonConfig\":\n    \"\"\"Creates a ``TritonConfig`` instance from an input dictionary. Values are converted into correct types.\n\n    Args:\n        config: a dictionary with all required fields\n\n    Returns:\n        a ``TritonConfig`` instance\n    \"\"\"\n    fields: Dict[str, dataclasses.Field] = {field.name: field for field in dataclasses.fields(cls)}\n    unknown_config_parameters = {name: value for name, value in config.items() if name not in fields}\n    for name, value in unknown_config_parameters.items():\n        LOGGER.warning(\n            f\"Ignoring {name}={value} as could not find matching config field. \"\n            f\"Available fields: {', '.join(map(str, fields))}\"\n        )\n\n    def _cast_value(_field, _value):\n        field_type = _field.type\n        is_optional = typing_inspect.is_optional_type(field_type)\n        if is_optional:\n            field_type = field_type.__args__[0]\n        return field_type(_value)\n\n    config_with_casted_values = {\n        name: _cast_value(fields[name], value) for name, value in config.items() if name in fields\n    }\n    return cls(**config_with_casted_values)\n</code></pre>"},{"location":"api/#pytriton.triton.TritonConfig.from_env","title":"<code>from_env()</code>  <code>classmethod</code>","text":"<p>Creates TritonConfig from environment variables.</p> <p>Environment variables should start with <code>PYTRITON_TRITON_CONFIG_</code> prefix. For example:</p> <pre><code>PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\nPYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n</code></pre> <p>Typical use:</p> <pre><code>triton_config = TritonConfig.from_env()\n</code></pre> <p>Returns:</p> Type Description <code>TritonConfig</code> <p>TritonConfig class instantiated from environment variables.</p> Source code in <code>pytriton/triton.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; \"TritonConfig\":\n    \"\"\"Creates TritonConfig from environment variables.\n\n    Environment variables should start with `PYTRITON_TRITON_CONFIG_` prefix. For example:\n\n        PYTRITON_TRITON_CONFIG_GRPC_PORT=45436\n        PYTRITON_TRITON_CONFIG_LOG_VERBOSE=4\n\n    Typical use:\n\n        triton_config = TritonConfig.from_env()\n\n    Returns:\n        TritonConfig class instantiated from environment variables.\n    \"\"\"\n    prefix = \"PYTRITON_TRITON_CONFIG_\"\n    config = {name[len(prefix) :].lower(): value for name, value in os.environ.items() if name.startswith(prefix)}\n    return cls.from_dict(config)\n</code></pre>"},{"location":"api/#pytriton.triton.TritonConfig.to_dict","title":"<code>to_dict()</code>","text":"<p>Map config object to dictionary.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def to_dict(self):\n    \"\"\"Map config object to dictionary.\"\"\"\n    return dataclasses.asdict(self)\n</code></pre>"},{"location":"api/#pytriton.decorators","title":"<code>pytriton.decorators</code>","text":"<p>Inference callable decorators.</p>"},{"location":"api/#pytriton.decorators.ConstantPadder","title":"<code>ConstantPadder(pad_value=0)</code>","text":"<p>Padder that pads the given batches with a constant value.</p> <p>Initialize the padder.</p> <p>Parameters:</p> Name Type Description Default <code>pad_value</code> <code>int</code> <p>Padding value. Defaults to 0.</p> <code>0</code> Source code in <code>pytriton/decorators.py</code> <pre><code>def __init__(self, pad_value=0):\n    \"\"\"Initialize the padder.\n\n    Args:\n        pad_value (int, optional): Padding value. Defaults to 0.\n    \"\"\"\n    self.pad_value = pad_value\n</code></pre>"},{"location":"api/#pytriton.decorators.ConstantPadder.__call__","title":"<code>__call__(batches_list)</code>","text":"<p>Pad the given batches with the specified value to pad size enabling further batching to single arrays.</p> <p>Parameters:</p> Name Type Description Default <code>batches_list</code> <code>List[Dict[str, ndarray]]</code> <p>List of batches to pad.</p> required <p>Returns:</p> Type Description <code>InferenceResults</code> <p>List[Dict[str, np.ndarray]]: List of padded batches.</p> <p>Raises:</p> Type Description <code>PyTritonRuntimeError</code> <p>If the input arrays for a given input name have different dtypes.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __call__(self, batches_list: InferenceResults) -&gt; InferenceResults:\n    \"\"\"Pad the given batches with the specified value to pad size enabling further batching to single arrays.\n\n    Args:\n        batches_list (List[Dict[str, np.ndarray]]): List of batches to pad.\n\n    Returns:\n        List[Dict[str, np.ndarray]]: List of padded batches.\n\n    Raises:\n        PyTritonRuntimeError: If the input arrays for a given input name have different dtypes.\n    \"\"\"\n\n    def _get_padded_shape(_batches: List[np.ndarray]) -&gt; Tuple[int, ...]:\n        \"\"\"Get the shape of the padded array without batch axis.\"\"\"\n        return tuple(np.max([batch.shape[1:] for batch in _batches if batch is not None], axis=0))\n\n    def _get_padded_dtype(_batches: List[np.ndarray]) -&gt; np.dtype:\n        dtypes = [batch.dtype for batch in _batches if batch is not None]\n        result_dtype = dtypes[0]\n\n        if not all(dtype.kind == result_dtype.kind for dtype in dtypes):\n            raise PyTritonRuntimeError(\"All input arrays for given input name must have the same dtype.\")\n\n        # for bytes (encoded string) or unicode string need to obtain the max length\n        if result_dtype.kind in \"SU\":\n            order_and_kind = result_dtype.str[:2]\n            max_len = max([int(dtype.str[2:]) for dtype in dtypes])\n            result_dtype = f\"{order_and_kind}{max_len}\"\n        else:\n            if not all(dtype == result_dtype for dtype in dtypes):\n                raise PyTritonRuntimeError(\"All input arrays for given input name must have the same dtype.\")\n\n        return np.dtype(result_dtype)\n\n    input_names = list(\n        collections.OrderedDict.fromkeys(input_name for batch in batches_list for input_name in batch.keys())\n    )\n    batches_by_name = {input_name: [batch.get(input_name) for batch in batches_list] for input_name in input_names}\n    for input_batches in batches_by_name.values():\n        result_shape, result_dtype = _get_padded_shape(input_batches), _get_padded_dtype(input_batches)\n        for batch_idx, batch in enumerate(input_batches):\n            if batch is not None:\n                input_batches[batch_idx] = np.pad(\n                    batch,\n                    [(0, 0)] + [(0, b - a) for a, b in zip(batch.shape[1:], result_shape)],\n                    mode=\"constant\",\n                    constant_values=self.pad_value if result_dtype.kind not in [\"S\", \"U\", \"O\"] else b\"\",\n                ).astype(result_dtype)\n\n    return [\n        {name: batches[batch_idx] for name, batches in batches_by_name.items() if batches[batch_idx] is not None}\n        for batch_idx in range(len(batches_list))\n    ]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict","title":"<code>ModelConfigDict()</code>","text":"<p>             Bases: <code>MutableMapping</code></p> <p>Dictionary for storing model configs for inference callable.</p> <p>Create ModelConfigDict object.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __init__(self):\n    \"\"\"Create ModelConfigDict object.\"\"\"\n    self._data: Dict[str, TritonModelConfig] = {}\n    self._keys: List[Callable] = []\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__delitem__","title":"<code>__delitem__(infer_callable)</code>","text":"<p>Delete model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __delitem__(self, infer_callable: Callable):\n    \"\"\"Delete model config for inference callable.\"\"\"\n    key = self._get_model_config_key(infer_callable)\n    del self._data[key]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__getitem__","title":"<code>__getitem__(infer_callable)</code>","text":"<p>Get model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __getitem__(self, infer_callable: Callable) -&gt; TritonModelConfig:\n    \"\"\"Get model config for inference callable.\"\"\"\n    key = self._get_model_config_key(infer_callable)\n    return self._data[key]\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over inference callable keys.\"\"\"\n    return iter(self._keys)\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__len__","title":"<code>__len__()</code>","text":"<p>Get number of inference callable keys.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __len__(self):\n    \"\"\"Get number of inference callable keys.\"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"api/#pytriton.decorators.ModelConfigDict.__setitem__","title":"<code>__setitem__(infer_callable, item)</code>","text":"<p>Set model config for inference callable.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def __setitem__(self, infer_callable: Callable, item: TritonModelConfig):\n    \"\"\"Set model config for inference callable.\"\"\"\n    self._keys.append(infer_callable)\n    key = self._get_model_config_key(infer_callable)\n    self._data[key] = item\n</code></pre>"},{"location":"api/#pytriton.decorators.TritonContext","title":"<code>TritonContext</code>  <code>dataclass</code>","text":"<p>Triton context definition class.</p>"},{"location":"api/#pytriton.decorators.batch","title":"<code>batch(wrapped, instance, args, kwargs)</code>","text":"<p>Decorator for converting list of request dicts to dict of input batches.</p> <p>Converts list of request dicts to dict of input batches. It passes **kwargs to inference callable where each named input contains numpy array with batch of requests received by Triton server. We assume that each request has the same set of keys (you can use group_by_keys decorator before using @batch decorator if your requests may have different set of keys).</p> <p>Raises:</p> Type Description <code>PyTritonValidationError</code> <p>If the requests have different set of keys.</p> <code>ValueError</code> <p>If the output tensors have different than expected batch sizes. Expected batch size is calculated as a sum of batch sizes of all requests.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef batch(wrapped, instance, args, kwargs):\n    \"\"\"Decorator for converting list of request dicts to dict of input batches.\n\n    Converts list of request dicts to dict of input batches.\n    It passes **kwargs to inference callable where each named input contains numpy array with batch of requests\n    received by Triton server.\n    We assume that each request has the same set of keys (you can use group_by_keys decorator before\n    using @batch decorator if your requests may have different set of keys).\n\n    Raises:\n        PyTritonValidationError: If the requests have different set of keys.\n        ValueError: If the output tensors have different than expected batch sizes. Expected batch size is\n            calculated as a sum of batch sizes of all requests.\n    \"\"\"\n    req_list = args[0]\n    input_names = req_list[0].keys()\n\n    for req_dict2 in req_list[1:]:\n        if input_names != req_dict2.keys():\n            raise PyTritonValidationError(\"Cannot batch requests with different set of inputs keys\")\n\n    inputs = {}\n    for model_input in input_names:\n        concatenated_input_data = np.concatenate([req[model_input] for req in req_list])\n        inputs[model_input] = concatenated_input_data\n\n    args = args[1:]\n    new_kwargs = dict(kwargs)\n    new_kwargs.update(inputs)\n    outputs = wrapped(*args, **new_kwargs)\n\n    def _split_result(_result):\n        outputs = convert_output(_result, wrapped, instance)\n        output_names = outputs.keys()\n\n        requests_total_batch_size = sum(get_inference_request_batch_size(req) for req in req_list)\n        not_matching_tensors_shapes = {\n            output_name: output_tensor.shape\n            for output_name, output_tensor in outputs.items()\n            if output_tensor.shape[0] != requests_total_batch_size\n        }\n        if not_matching_tensors_shapes:\n            raise ValueError(\n                f\"Received output tensors with different batch sizes: {', '.join(': '.join(map(str, item)) for item in not_matching_tensors_shapes.items())}. \"\n                f\"Expected batch size: {requests_total_batch_size}. \"\n            )\n\n        out_list = []\n        start_idx = 0\n        for request in req_list:\n            # get batch_size of first input for each request - assume that all inputs have same batch_size\n            request_batch_size = get_inference_request_batch_size(request)\n            req_output_dict = {}\n            for _output_ind, output_name in enumerate(output_names):\n                req_output = outputs[output_name][start_idx : start_idx + request_batch_size, ...]\n                req_output_dict[output_name] = req_output\n            out_list.append(req_output_dict)\n            start_idx += request_batch_size\n        return out_list\n\n    if inspect.isgenerator(outputs):\n        return (_split_result(_result) for _result in outputs)\n    else:\n        return _split_result(outputs)\n</code></pre>"},{"location":"api/#pytriton.decorators.convert_output","title":"<code>convert_output(outputs, wrapped=None, instance=None, model_config=None)</code>","text":"<p>Converts output from tuple ot list to dictionary.</p> <p>It is utility function useful for mapping output list into dictionary of outputs. Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs instead of dictionary if this list matches output list in model config (size and order).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def convert_output(\n    outputs: Union[Dict, List, Tuple], wrapped=None, instance=None, model_config: Optional[TritonModelConfig] = None\n):\n    \"\"\"Converts output from tuple ot list to dictionary.\n\n    It is utility function useful for mapping output list into dictionary of outputs.\n    Currently, it is used in @sample and @batch decorators (we assume that user can return list or tuple of outputs\n    instead of dictionary if this list matches output list in model config (size and order).\n    \"\"\"\n    if isinstance(outputs, dict):\n        return outputs\n    elif isinstance(outputs, (list, tuple)):\n        if model_config is None:\n            model_config = get_model_config(wrapped, instance)\n        if len(outputs) != len(model_config.outputs):\n            raise PyTritonValidationError(\"Outputs length different than config outputs length\")\n        outputs = {config_output.name: output for config_output, output in zip(model_config.outputs, outputs)}\n        return outputs\n    else:\n        raise PyTritonValidationError(f\"Unsupported output type {type(outputs)}.\")\n</code></pre>"},{"location":"api/#pytriton.decorators.fill_optionals","title":"<code>fill_optionals(**defaults)</code>","text":"<p>This decorator ensures that any missing inputs in requests are filled with default values specified by the user.</p> <p>Default values should be NumPy arrays without batch axis.</p> <p>If you plan to group requests ex. with @group_by_keys or @group_by_vales decorators provide default values for optional parameters at the beginning of decorators stack. The other decorators can then group requests into bigger batches resulting in a better model performance.</p> <p>Typical use:</p> <pre><code>@fill_optionals()\n@group_by_keys()\n@batch\ndef infer_fun(**inputs):\n    ...\n    return outputs\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>defaults</code> <p>keyword arguments containing default values for missing inputs</p> <code>{}</code> <p>If you have default values for some optional parameter it is good idea to provide them at the very beginning, so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def fill_optionals(**defaults):\n    \"\"\"This decorator ensures that any missing inputs in requests are filled with default values specified by the user.\n\n    Default values should be NumPy arrays without batch axis.\n\n    If you plan to group requests ex. with\n    [@group_by_keys][pytriton.decorators.group_by_keys] or\n    [@group_by_vales][pytriton.decorators.group_by_values] decorators\n    provide default values for optional parameters at the beginning of decorators stack.\n    The other decorators can then group requests into bigger batches resulting in a better model performance.\n\n    Typical use:\n\n        @fill_optionals()\n        @group_by_keys()\n        @batch\n        def infer_fun(**inputs):\n            ...\n            return outputs\n\n    Args:\n        defaults: keyword arguments containing default values for missing inputs\n\n\n    If you have default values for some optional parameter it is good idea to provide them at the very beginning,\n    so the other decorators (e.g. @group_by_keys) can make bigger consistent groups.\n    \"\"\"\n\n    def _verify_defaults(model_config: TritonModelConfig):\n        inputs = {spec.name: spec for spec in model_config.inputs}\n        not_matching_default_names = sorted(set(defaults) - set(inputs))\n        if not_matching_default_names:\n            raise PyTritonBadParameterError(f\"Could not found {', '.join(not_matching_default_names)} inputs\")\n\n        non_numpy_items = {k: v for k, v in defaults.items() if not isinstance(v, np.ndarray)}\n        if non_numpy_items:\n            raise PyTritonBadParameterError(\n                f\"Could not use {', '.join([f'{k}={v}' for k, v in non_numpy_items.items()])} defaults \"\n                \"as they are not NumPy arrays\"\n            )\n\n        not_matching_dtypes = {k: (v.dtype, inputs[k].dtype) for k, v in defaults.items() if v.dtype != inputs[k].dtype}\n        if not_matching_dtypes:\n            non_matching_dtypes_str_list = [\n                f\"{name}: dtype={have_dtype} expected_dtype={expected_dtype}\"\n                for name, (have_dtype, expected_dtype) in not_matching_dtypes.items()\n            ]\n            raise PyTritonBadParameterError(\n                f\"Could not use {', '.join(non_matching_dtypes_str_list)} \"\n                f\"defaults as they have different than input signature dtypes\"\n            )\n\n        def _shape_match(_have_shape, _expected_shape):\n            return len(_have_shape) == len(_expected_shape) and all(\n                e == -1 or h == e for h, e in zip(_have_shape, _expected_shape)\n            )\n\n        not_matching_shapes = {\n            k: (v.shape, inputs[k].shape) for k, v in defaults.items() if not _shape_match(v.shape, inputs[k].shape)\n        }\n        if not_matching_shapes:\n            non_matching_shapes_str_list = [\n                f\"{name}: shape={have_shape} expected_shape={expected_shape}\"\n                for name, (have_shape, expected_shape) in not_matching_shapes.items()\n            ]\n            raise PyTritonBadParameterError(\n                f\"Could not use {', '.join(non_matching_shapes_str_list)} \"\n                f\"defaults as they have different than input signature shapes\"\n            )\n\n    @wrapt.decorator\n    def _wrapper(wrapped, instance, args, kwargs):\n        model_config = get_model_config(wrapped, instance)\n        _verify_defaults(model_config)\n        # verification if not after group wrappers is in group wrappers\n\n        (requests,) = args\n\n        model_supports_batching = model_config.batching\n        for request in requests:\n            batch_size = get_inference_request_batch_size(request) if model_supports_batching else None\n            for default_key, default_value in defaults.items():\n                if default_key in request:\n                    continue\n\n                if model_supports_batching:\n                    ones_reps = (1,) * default_value.ndim  # repeat once default_value on each axis\n                    axis_reps = (batch_size,) + ones_reps  # ... except on batch axis. we repeat it batch_size times\n                    default_value = np.tile(default_value, axis_reps)\n\n                request[default_key] = default_value\n        return wrapped(*args, **kwargs)\n\n    return _wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.first_value","title":"<code>first_value(*keys, squeeze_single_values=True, strict=True)</code>","text":"<p>This decorator overwrites selected inputs with first element of the given input.</p> <p>It can be used in two ways:</p> <ol> <li> <p>Wrapping a single request inference callable by chaining with @batch decorator:     @batch     @first_value(\"temperature\")     def infer_fn(**inputs):         ...         return result</p> </li> <li> <p>Wrapping a multiple requests inference callable:     @first_value(\"temperature\")     def infer_fn(requests):         ...         return results</p> </li> </ol> <p>By default, the decorator squeezes single value arrays to scalars. This behavior can be disabled by setting the <code>squeeze_single_values</code> flag to False.</p> <p>By default, the decorator checks the equality of the values on selected values. This behavior can be disabled by setting the <code>strict</code> flag to False.</p> <p>Wrapper can only be used with models that support batching.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str</code> <p>The input keys selected for conversion.</p> <code>()</code> <code>squeeze_single_values</code> <p>squeeze single value ND array to scalar values. Defaults to True.</p> <code>True</code> <code>strict</code> <code>bool</code> <p>enable checking if all values on single selected input of request are equal. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>PyTritonRuntimeError</code> <p>if not all values on a single selected input of the request are equal</p> <code>PyTritonBadParameterError</code> <p>if any of the keys passed to the decorator are not allowed.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def first_value(*keys: str, squeeze_single_values=True, strict: bool = True):\n    \"\"\"This decorator overwrites selected inputs with first element of the given input.\n\n    It can be used in two ways:\n\n    1. Wrapping a single request inference callable by chaining with @batch decorator:\n        @batch\n        @first_value(\"temperature\")\n        def infer_fn(**inputs):\n            ...\n            return result\n\n    2. Wrapping a multiple requests inference callable:\n        @first_value(\"temperature\")\n        def infer_fn(requests):\n            ...\n            return results\n\n    By default, the decorator squeezes single value arrays to scalars.\n    This behavior can be disabled by setting the `squeeze_single_values` flag to False.\n\n    By default, the decorator checks the equality of the values on selected values.\n    This behavior can be disabled by setting the `strict` flag to False.\n\n    Wrapper can only be used with models that support batching.\n\n    Args:\n        keys: The input keys selected for conversion.\n        squeeze_single_values: squeeze single value ND array to scalar values. Defaults to True.\n        strict: enable checking if all values on single selected input of request are equal. Defaults to True.\n\n    Raises:\n        PyTritonRuntimeError: if not all values on a single selected input of the request are equal\n        and the strict flag is set to True. Additionally, if the decorator is used with a model that doesn't support batching,\n        PyTritonBadParameterError: if any of the keys passed to the decorator are not allowed.\n    \"\"\"\n    if any(k in _SPECIAL_KEYS for k in keys):\n        not_allowed_keys = [key for key in keys if key in _SPECIAL_KEYS]\n        raise PyTritonBadParameterError(\n            f\"The keys {', '.join(not_allowed_keys)} are not allowed as keys for @first_value wrapper. \"\n            f\"The set of not allowed keys are {', '.join(_SPECIAL_KEYS)}\"\n        )\n\n    @wrapt.decorator\n    def wrapper(wrapped, instance, args, kwargs):\n\n        model_config = get_model_config(wrapped, instance)\n        if not model_config.batching:\n            raise PyTritonRuntimeError(\"The @first_value decorator can only be used with models that support batching.\")\n\n        def _replace_inputs_with_first_value(_request):\n            for input_name in keys:\n                if input_name not in _request:\n                    continue\n\n                values = _request[input_name]\n                if strict:\n                    # do not set axis for arrays with strings (object) or models not supporting batching\n                    axis_of_uniqueness = None if values.dtype == object else 0\n                    unique_values = np.unique(values, axis=axis_of_uniqueness)\n                    if len(unique_values) &gt; 1:\n                        raise PyTritonRuntimeError(\n                            f\"The values on the {input_name!r} input are not equal. \"\n                            \"To proceed, either disable strict mode in @first_value wrapper \"\n                            \"or ensure that the values always are consistent. \"\n                            f\"The current values of {input_name!r} are {_request[input_name]!r}.\"\n                        )\n\n                _first_value = values[0]\n                if (\n                    squeeze_single_values\n                    and not np.isscalar(_first_value)\n                    and all(dim == 1 for dim in _first_value.shape)\n                ):\n                    _dim_0_array = np.squeeze(_first_value)\n                    _first_value = _dim_0_array[()]  # obtain scalar from 0-dim array with numpy type\n\n                _request[input_name] = _first_value\n            return _request\n\n        inputs_names = set(kwargs) - set(_SPECIAL_KEYS)\n        if inputs_names:\n            kwargs = _replace_inputs_with_first_value(kwargs)\n            return wrapped(*args, **kwargs)\n        else:\n            requests, *other_args = args\n            requests = [_replace_inputs_with_first_value(request) for request in requests]\n            return wrapped(requests, *other_args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.get_inference_request_batch_size","title":"<code>get_inference_request_batch_size(inference_request)</code>","text":"<p>Get batch size from triton request.</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>InferenceRequest</code> <p>Triton request.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Batch size.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_inference_request_batch_size(inference_request: InferenceRequest) -&gt; int:\n    \"\"\"Get batch size from triton request.\n\n    Args:\n        inference_request (InferenceRequest): Triton request.\n\n    Returns:\n        int: Batch size.\n    \"\"\"\n    first_input_value = next(iter(inference_request.values()))\n    batch_size, *dims = first_input_value.shape\n    return batch_size\n</code></pre>"},{"location":"api/#pytriton.decorators.get_model_config","title":"<code>get_model_config(wrapped, instance)</code>","text":"<p>Retrieves instance of TritonModelConfig from callable.</p> <p>It is internally used in convert_output function to get output list from model. You can use this in custom decorators if you need access to model_config information. If you use @triton_context decorator you do not need this function (you can get model_config directly from triton_context passing function/callable to dictionary getter).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_model_config(wrapped, instance) -&gt; TritonModelConfig:\n    \"\"\"Retrieves instance of TritonModelConfig from callable.\n\n    It is internally used in convert_output function to get output list from model.\n    You can use this in custom decorators if you need access to model_config information.\n    If you use @triton_context decorator you do not need this function (you can get model_config directly\n    from triton_context passing function/callable to dictionary getter).\n    \"\"\"\n    return get_triton_context(wrapped, instance).model_configs[wrapped]\n</code></pre>"},{"location":"api/#pytriton.decorators.get_triton_context","title":"<code>get_triton_context(wrapped, instance)</code>","text":"<p>Retrieves triton context from callable.</p> <p>It is used in @triton_context to get triton context registered by triton binding in inference callable. If you use @triton_context decorator you do not need this function.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def get_triton_context(wrapped, instance) -&gt; TritonContext:\n    \"\"\"Retrieves triton context from callable.\n\n    It is used in @triton_context to get triton context registered by triton binding in inference callable.\n    If you use @triton_context decorator you do not need this function.\n    \"\"\"\n    caller = instance or wrapped\n    if not hasattr(caller, \"__triton_context__\"):\n        raise PyTritonValidationError(\"Wrapped function or object must bound with triton to get  __triton_context__\")\n    return caller.__triton_context__\n</code></pre>"},{"location":"api/#pytriton.decorators.group_by_keys","title":"<code>group_by_keys(wrapped, instance, args, kwargs)</code>","text":"<p>Group by keys.</p> <p>Decorator prepares groups of requests with the same set of keys and calls wrapped function for each group separately (it is convenient to use this decorator before batching, because the batching decorator requires consistent set of inputs as it stacks them into batches).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef group_by_keys(wrapped, instance, args, kwargs):\n    \"\"\"Group by keys.\n\n    Decorator prepares groups of requests with the same set of keys and calls wrapped function\n    for each group separately (it is convenient to use this decorator before batching, because the batching decorator\n    requires consistent set of inputs as it stacks them into batches).\n    \"\"\"\n    inputs = args[0]\n    idx_inputs = [(idx, tuple(sorted(input.keys())), input) for idx, input in enumerate(inputs)]\n    idx_inputs.sort(key=operator.itemgetter(1))\n    idx_groups_res = []\n    for _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\n        idx, _key, sample_list = zip(*group)\n        args = (list(sample_list),) + args[1:]\n        out = wrapped(*args, **kwargs)\n        idx_groups_res.extend(zip(idx, out))\n\n    idx_groups_res.sort(key=operator.itemgetter(0))\n    res_flat = [r[1] for r in idx_groups_res]\n    return res_flat\n</code></pre>"},{"location":"api/#pytriton.decorators.group_by_values","title":"<code>group_by_values(*keys, pad_fn=None)</code>","text":"<p>Decorator for grouping requests by values of selected keys.</p> <p>This function splits a batch into multiple sub-batches based on the specified keys values and calls the decorated function with each sub-batch. This is particularly useful when working with models that require dynamic parameters sent by the user.</p> <p>For example, given an input of the form:</p> <pre><code>{\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n</code></pre> <p>Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:</p> <pre><code>[\n    {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n    {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n]\n</code></pre> <p>This decorator should be used after the @batch decorator.</p> <p>Example usage:</p> <pre><code>@batch\n@group_by_values(\"param1\", \"param2\")\ndef infer_fun(**inputs):\n    ...\n    return outputs\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*keys</code> <p>List of keys to group by.</p> <code>()</code> <code>pad_fn</code> <code>Optional[Callable[[InferenceRequests], InferenceRequests]]</code> <p>Optional function to pad the batch to the same size before merging again to a single batch.</p> <code>None</code> <p>Returns:</p> Type Description <p>The decorator function.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>def group_by_values(*keys, pad_fn: typing.Optional[typing.Callable[[InferenceRequests], InferenceRequests]] = None):\n    \"\"\"Decorator for grouping requests by values of selected keys.\n\n    This function splits a batch into multiple sub-batches based on the specified keys values and\n    calls the decorated function with each sub-batch. This is particularly useful when working with models\n    that require dynamic parameters sent by the user.\n\n    For example, given an input of the form:\n\n        {\"sentences\": [b\"Sentence1\", b\"Sentence2\", b\"Sentence3\"], \"param1\": [1, 1, 2], \"param2\": [1, 1, 1]}\n\n    Using @group_by_values(\"param1\", \"param2\") will split the batch into two sub-batches:\n\n        [\n            {\"sentences\": [b\"Sentence1\", b\"Sentence2\"], \"param1\": [1, 1], \"param2\": [1, 1]},\n            {\"sentences\": [b\"Sentence3\"], \"param1\": [2], \"param2\": [1]}\n        ]\n\n    This decorator should be used after the @batch decorator.\n\n    Example usage:\n\n        @batch\n        @group_by_values(\"param1\", \"param2\")\n        def infer_fun(**inputs):\n            ...\n            return outputs\n\n    Args:\n        *keys: List of keys to group by.\n        pad_fn: Optional function to pad the batch to the same size before merging again to a single batch.\n\n    Returns:\n        The decorator function.\n    \"\"\"\n\n    def value_to_key(value):\n        if isinstance(value, np.ndarray):\n            if value.dtype == np.object_ or value.dtype.type == np.bytes_:\n                return _serialize_byte_tensor(value)\n            else:\n                return value.tobytes()\n        return value\n\n    def _get_sort_key_for_sample(_request, _sample_idx: int):\n        return tuple(value_to_key(_request[_key][_sample_idx]) for _key in keys)\n\n    def _group_request(_request: InferenceRequest, _batch_size: int):\n        idx_inputs = [(sample_idx, _get_sort_key_for_sample(_request, sample_idx)) for sample_idx in range(_batch_size)]\n        idx_inputs.sort(key=operator.itemgetter(1))\n        for _, group in itertools.groupby(idx_inputs, key=operator.itemgetter(1)):\n            _samples_idxes, _ = zip(*group)\n            grouped_request = {input_name: value[_samples_idxes, ...] for input_name, value in _request.items()}\n            yield _samples_idxes, grouped_request\n\n    @wrapt.decorator\n    def _wrapper(wrapped, instance, args, kwargs):\n\n        wrappers_stack = [\n            callable_with_wrapper.wrapper\n            for callable_with_wrapper in _get_wrapt_stack(wrapped)\n            if callable_with_wrapper.wrapper is not None\n        ]\n        if batch in wrappers_stack:\n            raise PyTritonRuntimeError(\"The @group_by_values decorator must be used after the @batch decorator.\")\n\n        request = {k: v for k, v in kwargs.items() if k not in _SPECIAL_KEYS}\n        other_kwargs = {k: v for k, v in kwargs.items() if k in _SPECIAL_KEYS}\n\n        batch_size = get_inference_request_batch_size(request)\n        sample_indices_with_interim_result = []\n        for sample_indices, _grouped_sub_request in _group_request(request, batch_size):\n            interim_result = wrapped(*args, **_grouped_sub_request, **other_kwargs)\n            sample_indices_with_interim_result.append((sample_indices, interim_result))\n\n        if pad_fn is not None:\n            indices, results = tuple(map(tuple, zip(*sample_indices_with_interim_result)))\n            results = pad_fn(results)\n            sample_indices_with_interim_result = tuple(zip(indices, results))\n\n        _, first_result_data = sample_indices_with_interim_result[0]\n        result = {\n            output_name: np.zeros((batch_size,) + data.shape[1:], dtype=data.dtype)\n            for output_name, data in first_result_data.items()\n        }\n        for indices, results in sample_indices_with_interim_result:\n            for output_name, data in results.items():\n                result[output_name][indices, ...] = data\n\n        return result\n\n    return _wrapper\n</code></pre>"},{"location":"api/#pytriton.decorators.pad_batch","title":"<code>pad_batch(wrapped, instance, args, kwargs)</code>","text":"<p>Add padding to the inputs batches.</p> <p>Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or max batch size from model config whatever is closer to current input size).</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef pad_batch(wrapped, instance, args, kwargs):\n    \"\"\"Add padding to the inputs batches.\n\n    Decorator appends last rows to the inputs multiple times to get desired batch size (preferred batch size or\n    max batch size from model config whatever is closer to current input size).\n    \"\"\"\n    inputs = {k: v for k, v in kwargs.items() if k != \"__triton_context__\"}\n    first_input = next(iter(inputs.values()))\n    config = get_model_config(wrapped, instance)\n    batch_sizes = (\n        []\n        if (config.batcher is None or config.batcher.preferred_batch_size is None)\n        else sorted(config.batcher.preferred_batch_size)\n    )\n    batch_sizes.append(config.max_batch_size)\n    batch_size = batch_sizes[bisect_left(batch_sizes, first_input.shape[0])]\n\n    new_inputs = {\n        input_name: np.repeat(\n            input_array,\n            np.concatenate(\n                [np.ones(input_array.shape[0] - 1), np.array([batch_size - input_array.shape[0] + 1])]\n            ).astype(np.int64),\n            axis=0,\n        )\n        for input_name, input_array in inputs.items()\n    }\n\n    kwargs.update(new_inputs)\n    return wrapped(*args, **kwargs)\n</code></pre>"},{"location":"api/#pytriton.decorators.sample","title":"<code>sample(wrapped, instance, args, kwargs)</code>","text":"<p>Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.</p> <p>Decorator takes first request and convert it into named inputs. Useful with non-batching models - instead of one element list of request, we will get named inputs - <code>kwargs</code>.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef sample(wrapped, instance, args, kwargs):\n    \"\"\"Decorator is used for non-batched inputs to convert from one element list of requests to request kwargs.\n\n    Decorator takes first request and convert it into named inputs.\n    Useful with non-batching models - instead of one element list of request, we will get named inputs - `kwargs`.\n    \"\"\"\n    kwargs.update(args[0][0])\n    outputs = wrapped(*args[1:], **kwargs)\n    outputs = convert_output(outputs, wrapped, instance)\n    return [outputs]\n</code></pre>"},{"location":"api/#pytriton.decorators.triton_context","title":"<code>triton_context(wrapped, instance, args, kwargs)</code>","text":"<p>Adds triton context.</p> <p>It gives you additional argument passed to the function in **kwargs called 'triton_context'. You can read model config from it and in the future possibly have some interaction with triton.</p> Source code in <code>pytriton/decorators.py</code> <pre><code>@wrapt.decorator\ndef triton_context(wrapped, instance, args, kwargs):\n    \"\"\"Adds triton context.\n\n    It gives you additional argument passed to the function in **kwargs called 'triton_context'.\n    You can read model config from it and in the future possibly have some interaction with triton.\n    \"\"\"\n    kwargs[TRITON_CONTEXT_FIELD_NAME] = get_triton_context(wrapped, instance)\n    return wrapped(*args, **kwargs)\n</code></pre>"},{"location":"api/#pytriton.triton.Triton","title":"<code>pytriton.triton.Triton(*, config=None, workspace=None)</code>","text":"<p>             Bases: <code>TritonBase</code></p> <p>Triton Inference Server for Python models.</p> <p>Initialize Triton Inference Server context for starting server and loading models.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[TritonConfig]</code> <p>TritonConfig object with optional customizations for Triton Inference Server. Configuration can be passed also through environment variables. See TritonConfig.from_env() class method for details.</p> <p>Order of precedence:</p> <ul> <li>config defined through <code>config</code> parameter of init method.</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul> <code>None</code> <code>workspace</code> <code>Union[Workspace, str, Path, None]</code> <p>workspace or path where the Triton Model Store and files used by pytriton will be created. If workspace is <code>None</code> random workspace will be created. Workspace will be deleted in Triton.stop().</p> <code>None</code> Source code in <code>pytriton/triton.py</code> <pre><code>def __init__(\n    self, *, config: Optional[TritonConfig] = None, workspace: Union[Workspace, str, pathlib.Path, None] = None\n):\n    \"\"\"Initialize Triton Inference Server context for starting server and loading models.\n\n    Args:\n        config: TritonConfig object with optional customizations for Triton Inference Server.\n            Configuration can be passed also through environment variables.\n            See [TritonConfig.from_env()][pytriton.triton.TritonConfig.from_env] class method for details.\n\n            Order of precedence:\n\n              - config defined through `config` parameter of init method.\n              - config defined in environment variables\n              - default TritonConfig values\n        workspace: workspace or path where the Triton Model Store and files used by pytriton will be created.\n            If workspace is `None` random workspace will be created.\n            Workspace will be deleted in [Triton.stop()][pytriton.triton.Triton.stop].\n    \"\"\"\n\n    def _without_none_values(_d):\n        return {name: value for name, value in _d.items() if value is not None}\n\n    default_config_dict = _without_none_values(TritonConfig().to_dict())\n    env_config_dict = _without_none_values(TritonConfig.from_env().to_dict())\n    explicit_config_dict = _without_none_values(config.to_dict() if config else {})\n    config_dict = {**default_config_dict, **env_config_dict, **explicit_config_dict}\n    self._config = TritonConfig(**config_dict)\n    workspace_instance = workspace if isinstance(workspace, Workspace) else Workspace(workspace)\n    self._prepare_triton_config(workspace_instance)\n    endpoint_protocol = \"http\" if self._config.allow_http in [True, None] else \"grpc\"\n    super().__init__(\n        url=endpoint_utils.get_endpoint(self._triton_server_config, endpoint_protocol),\n        workspace=workspace_instance,\n    )\n    self._triton_server = None\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.__enter__","title":"<code>__enter__()</code>","text":"<p>Entering the context launches the triton server.</p> <p>Returns:</p> Type Description <code>Triton</code> <p>A Triton object</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __enter__(self) -&gt; \"Triton\":\n    \"\"\"Entering the context launches the triton server.\n\n    Returns:\n        A Triton object\n    \"\"\"\n    self._run_server()\n    super().__enter__()\n    return self\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.run","title":"<code>run()</code>","text":"<p>Run Triton Inference Server.</p> Source code in <code>pytriton/triton.py</code> <pre><code>def run(self) -&gt; None:\n    \"\"\"Run Triton Inference Server.\"\"\"\n    self._run_server()\n    self.connect()\n</code></pre>"},{"location":"api/#pytriton.triton.Triton.serve","title":"<code>serve(monitoring_period_s=MONITORING_PERIOD_S)</code>","text":"<p>Run Triton Inference Server and lock thread for serving requests/response.</p> <p>Parameters:</p> Name Type Description Default <code>monitoring_period_s</code> <code>float</code> <p>the timeout of monitoring if Triton and models are available. Every monitoring_period_s seconds main thread wakes up and check if triton server and proxy backend are still alive and sleep again. If triton or proxy is not alive - method returns.</p> <code>MONITORING_PERIOD_S</code> Source code in <code>pytriton/triton.py</code> <pre><code>def serve(self, monitoring_period_s: float = MONITORING_PERIOD_S) -&gt; None:\n    \"\"\"Run Triton Inference Server and lock thread for serving requests/response.\n\n    Args:\n        monitoring_period_s: the timeout of monitoring if Triton and models are available.\n            Every monitoring_period_s seconds main thread wakes up and check if triton server and proxy backend\n            are still alive and sleep again. If triton or proxy is not alive - method returns.\n    \"\"\"\n    self._run_server()\n    super().serve(monitoring_period_s=monitoring_period_s)\n</code></pre>"},{"location":"api/#pytriton.triton.RemoteTriton","title":"<code>pytriton.triton.RemoteTriton(url, workspace=None)</code>","text":"<p>             Bases: <code>TritonBase</code></p> <p>RemoteTriton connects to Triton Inference Server running on remote host.</p> <p>Initialize RemoteTriton.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Triton Inference Server URL in form of ://: If scheme is not provided, http is used as default. If port is not provided, 8000 is used as default for http and 8001 for grpc. required <code>workspace</code> <code>Union[Workspace, str, Path, None]</code> <p>path to be created where the files used by pytriton will be stored (e.g. socket files for communication). If workspace is <code>None</code> temporary workspace will be created. Workspace should be created in shared filesystem space between RemoteTriton and Triton Inference Server to allow access to socket files (if you use containers, folder must be shared between containers).</p> <code>None</code> Source code in <code>pytriton/triton.py</code> <pre><code>def __init__(self, url: str, workspace: Union[Workspace, str, pathlib.Path, None] = None):\n    \"\"\"Initialize RemoteTriton.\n\n    Args:\n        url: Triton Inference Server URL in form of &lt;scheme&gt;://&lt;host&gt;:&lt;port&gt;\n            If scheme is not provided, http is used as default.\n            If port is not provided, 8000 is used as default for http and 8001 for grpc.\n        workspace: path to be created where the files used by pytriton will be stored\n            (e.g. socket files for communication).\n            If workspace is `None` temporary workspace will be created.\n            Workspace should be created in shared filesystem space between RemoteTriton\n            and Triton Inference Server to allow access to socket files\n            (if you use containers, folder must be shared between containers).\n\n    \"\"\"\n    super().__init__(url=TritonUrl.from_url(url).with_scheme, workspace=workspace)\n\n    with self._cv:\n        self._stopped = False\n</code></pre>"},{"location":"api/#pytriton.triton.RemoteTriton.__enter__","title":"<code>__enter__()</code>","text":"<p>Entering the context connects to remote Triton server.</p> <p>Returns:</p> Type Description <code>RemoteTriton</code> <p>A RemoteTriton object</p> Source code in <code>pytriton/triton.py</code> <pre><code>def __enter__(self) -&gt; \"RemoteTriton\":\n    \"\"\"Entering the context connects to remote Triton server.\n\n    Returns:\n        A RemoteTriton object\n    \"\"\"\n    super().__enter__()\n    return self\n</code></pre>"},{"location":"api/#pytriton.model_config.tensor.Tensor","title":"<code>pytriton.model_config.tensor.Tensor</code>  <code>dataclass</code>","text":"<p>Model input and output definition for Triton deployment.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple</code> <p>Shape of the input/output tensor.</p> required <code>dtype</code> <code>Union[dtype, Type[dtype], Type[object]]</code> <p>Data type of the input/output tensor.</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the input/output of model.</p> <code>None</code> <code>optional</code> <code>Optional[bool]</code> <p>Flag to mark if input is optional.</p> <code>False</code>"},{"location":"api/#pytriton.model_config.tensor.Tensor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Override object values on post init or field override.</p> Source code in <code>pytriton/model_config/tensor.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Override object values on post init or field override.\"\"\"\n    if isinstance(self.dtype, np.dtype):\n        object.__setattr__(self, \"dtype\", self.dtype.type)  # pytype: disable=attribute-error\n</code></pre>"},{"location":"api/#pytriton.model_config.common","title":"<code>pytriton.model_config.common</code>","text":"<p>Common structures for internal and external ModelConfig.</p>"},{"location":"api/#pytriton.model_config.common.DeviceKind","title":"<code>DeviceKind</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Device kind for model deployment.</p> <p>Parameters:</p> Name Type Description Default <code>KIND_AUTO</code> <p>Automatically select the device for model deployment.</p> required <code>KIND_CPU</code> <p>Model is deployed on CPU.</p> required <code>KIND_GPU</code> <p>Model is deployed on GPU.</p> required"},{"location":"api/#pytriton.model_config.common.DynamicBatcher","title":"<code>DynamicBatcher</code>  <code>dataclass</code>","text":"<p>Dynamic batcher configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> <code>0</code> <code>preferred_batch_size</code> <code>Optional[list]</code> <p>Preferred batch sizes for dynamic batching.</p> <code>None</code> <code>preserve_ordering</code> <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> <code>False</code> <code>priority_levels</code> <code>int</code> <p>The number of priority levels to be enabled for the model.</p> <code>0</code> <code>default_priority_level</code> <code>int</code> <p>The priority level used for requests that don't specify their priority.</p> <code>0</code> <code>default_queue_policy</code> <code>Optional[QueuePolicy]</code> <p>The default queue policy used for requests.</p> <code>None</code> <code>priority_queue_policy</code> <code>Optional[Dict[int, QueuePolicy]]</code> <p>Specify the queue policy for the priority level.</p> <code>None</code>"},{"location":"api/#pytriton.model_config.common.QueuePolicy","title":"<code>QueuePolicy</code>  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>timeout_action</code> <code>TimeoutAction</code> <p>The action applied to timed-out request.</p> <code>REJECT</code> <code>default_timeout_microseconds</code> <code>int</code> <p>The default timeout for every request, in microseconds.</p> <code>0</code> <code>allow_timeout_override</code> <code>bool</code> <p>Whether individual request can override the default timeout value.</p> <code>False</code> <code>max_queue_size</code> <code>int</code> <p>The maximum queue size for holding requests.</p> <code>0</code>"},{"location":"api/#pytriton.model_config.common.TimeoutAction","title":"<code>TimeoutAction</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Parameters:</p> Name Type Description Default <code>REJECT</code> <p>Reject the request and return error message accordingly.</p> required <code>DELAY</code> <p>Delay the request until all other requests at the same (or higher) priority levels that have not reached their timeouts are processed.</p> required"},{"location":"api/#pytriton.model_config.model_config.ModelConfig","title":"<code>pytriton.model_config.model_config.ModelConfig</code>  <code>dataclass</code>","text":"<p>Additional model configuration for running model through Triton Inference Server.</p> <p>Parameters:</p> Name Type Description Default <code>batching</code> <code>bool</code> <p>Flag to enable/disable batching for model.</p> <code>True</code> <code>max_batch_size</code> <code>int</code> <p>The maximal batch size that would be handled by model.</p> <code>4</code> <code>batcher</code> <code>DynamicBatcher</code> <p>Configuration of Dynamic Batching for the model.</p> <code>field(default_factory=DynamicBatcher)</code> <code>response_cache</code> <code>bool</code> <p>Flag to enable/disable response cache for the model</p> <code>False</code> <code>decoupled</code> <code>bool</code> <p>Flag to enable/disable decoupled from requests execution</p> <code>False</code>"},{"location":"api/#pytriton.client.client","title":"<code>pytriton.client.client</code>","text":"<p>Clients for easy interaction with models deployed on the Triton Inference Server.</p> <p>Typical usage example:</p> <pre><code>with ModelClient(\"localhost\", \"MyModel\") as client:\n    result_dict = client.infer_sample(input_a=a, input_b=b)\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = client.infer_sample(input1, input2)\nresult_dict = client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p>"},{"location":"api/#pytriton.client.client.AsyncioModelClient","title":"<code>AsyncioModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>             Bases: <code>BaseModelClient</code></p> <p>Asyncio client for model deployed on the Triton Inference Server.</p> <p>This client is based on Triton Inference Server Python clients and GRPC library: * <code>tritonclient.http.aio.InferenceServerClient</code> * <code>tritonclient.grpc.aio.InferenceServerClient</code></p> <p>It can wait for server to be ready with model loaded and then perform inference on it. <code>AsyncioModelClient</code> supports asyncio context manager protocol.</p> <p>Typical usage: <pre><code>from pytriton.client import AsyncioModelClient\nimport numpy as np\n\ninput1_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\ninput2_sample = np.random.rand(1, 3, 224, 224).astype(np.float32)\n\nasync with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\n    result_dict = await client.infer_sample(input1_sample, input2_sample)\n    print(result_dict[\"output_name\"])\n</code></pre></p> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> required <code>model_name</code> <code>str</code> <p>name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> <code>None</code> <code>lazy_init</code> <code>bool</code> <p>if initialization should be performed just before sending first request to inference server.</p> <code>True</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>timeout for server and model being ready.</p> <code>None</code> <p>Raises:</p> Type Description <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientTimeoutError</code> <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> <code>PyTritonClientUrlParseError</code> <p>In case of problems with parsing url.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n):\n    \"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for server and model being ready.\n\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError: if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\n    super().__init__(\n        url=url,\n        model_name=model_name,\n        model_version=model_version,\n        lazy_init=lazy_init,\n        init_timeout_s=init_timeout_s,\n        inference_timeout_s=inference_timeout_s,\n    )\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.model_config","title":"<code>model_config</code>  <code>async</code> <code>property</code>","text":"<p>Obtain configuration of model deployed on the Triton Inference Server.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Create context for use AsyncioModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Create context for use AsyncioModelClient as a context manager.\"\"\"\n    _LOGGER.debug(\"Entering AsyncioModelClient context\")\n    try:\n        if not self._lazy_init:\n            _LOGGER.debug(\"Waiting in AsyncioModelClient context for model to be ready\")\n            await self._wait_and_init_model_config(self._init_timeout_s)\n            _LOGGER.debug(\"Model is ready in AsyncioModelClient context\")\n        return self\n    except Exception as e:\n        _LOGGER.error(\"Error occurred during AsyncioModelClient context initialization\")\n        await self.close()\n        raise e\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.__aexit__","title":"<code>__aexit__(*_)</code>  <code>async</code>","text":"<p>Close resources used by AsyncioModelClient when exiting from context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def __aexit__(self, *_):\n    \"\"\"Close resources used by AsyncioModelClient when exiting from context.\"\"\"\n    await self.close()\n    _LOGGER.debug(\"Exiting AsyncioModelClient context\")\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close resources used by _ModelClientBase.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def close(self):\n    \"\"\"Close resources used by _ModelClientBase.\"\"\"\n    _LOGGER.debug(\"Closing InferenceServerClient\")\n    await self._general_client.close()\n    await self._infer_client.close()\n    _LOGGER.debug(\"InferenceServerClient closed\")\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.get_lib","title":"<code>get_lib()</code>","text":"<p>Get Triton Inference Server Python client library.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n    \"\"\"Get Triton Inference Server Python client library.\"\"\"\n    return {\"grpc\": tritonclient.grpc.aio, \"http\": tritonclient.http.aio}[self._triton_url.scheme.lower()]\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.infer_batch","title":"<code>infer_batch(*inputs, parameters=None, headers=None, **named_inputs)</code>  <code>async</code>","text":"<p>Run asynchronous inference on batched data.</p> <p>Typical usage:</p> <pre><code>async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\n    result_dict = await client.infer_batch(input1, input2)\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = await client.infer_batch(input1, input2)\nresult_dict = await client.infer_batch(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> <code>PyTritonClientModelDoesntSupportBatchingError</code> <p>if model doesn't support batching.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run asynchronous inference on batched data.\n\n    Typical usage:\n\n    ```python\n    async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\n        result_dict = await client.infer_batch(input1, input2)\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = await client.infer_batch(input1, input2)\n    result_dict = await client.infer_batch(a=input1, b=input2)\n    ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.\n\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelDoesntSupportBatchingError: if model doesn't support batching.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    _LOGGER.debug(f\"Running inference for {self._model_name}\")\n    model_config = await self.model_config\n    _LOGGER.debug(f\"Model config for {self._model_name} obtained\")\n\n    model_supports_batching = model_config.max_batch_size &gt; 0\n    if not model_supports_batching:\n        _LOGGER.error(f\"Model {model_config.model_name} doesn't support batching\")\n        raise PyTritonClientModelDoesntSupportBatchingError(\n            f\"Model {model_config.model_name} doesn't support batching - use infer_sample method instead\"\n        )\n\n    _LOGGER.debug(f\"Running _infer for {self._model_name}\")\n    result = await self._infer(inputs or named_inputs, parameters, headers)\n    _LOGGER.debug(f\"_infer for {self._model_name} finished\")\n    return result\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.infer_sample","title":"<code>infer_sample(*inputs, parameters=None, headers=None, **named_inputs)</code>  <code>async</code>","text":"<p>Run asynchronous inference on single data sample.</p> <p>Typical usage:</p> <pre><code>async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\n    result_dict = await client.infer_sample(input1, input2)\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>result_dict = await client.infer_sample(input1, input2)\nresult_dict = await client.infer_sample(a=input1, b=input2)\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>timeout_s</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run asynchronous inference on single data sample.\n\n    Typical usage:\n\n    ```python\n    async with AsyncioModelClient(\"localhost\", \"MyModel\") as client:\n        result_dict = await client.infer_sample(input1, input2)\n    ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n    ```python\n    result_dict = await client.infer_sample(input1, input2)\n    result_dict = await client.infer_sample(a=input1, b=input2)\n    ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientRuntimeError.\n\n    Args:\n        *inputs: inference inputs provided as positional arguments.\n        parameters: custom inference parameters.\n        headers: custom inference headers.\n        **named_inputs: inference inputs provided as named arguments.\n\n    Returns:\n        dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s`\n            or inference time exceeds `timeout_s`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    _LOGGER.debug(f\"Running inference for {self._model_name}\")\n    model_config = await self.model_config\n    _LOGGER.debug(f\"Model config for {self._model_name} obtained\")\n\n    model_supports_batching = model_config.max_batch_size &gt; 0\n    if model_supports_batching:\n        if inputs:\n            inputs = tuple(data[np.newaxis, ...] for data in inputs)\n        elif named_inputs:\n            named_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\n\n    _LOGGER.debug(f\"Running _infer for {self._model_name}\")\n    result = await self._infer(inputs or named_inputs, parameters, headers)\n    _LOGGER.debug(f\"_infer for {self._model_name} finished\")\n    if model_supports_batching:\n        result = {name: data[0] for name, data in result.items()}\n\n    return result\n</code></pre>"},{"location":"api/#pytriton.client.client.AsyncioModelClient.wait_for_model","title":"<code>wait_for_model(timeout_s)</code>  <code>async</code>","text":"<p>Asynchronous wait for Triton Inference Server and deployed on it model readiness.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>timeout to server and model get into readiness state.</p> required <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If server and model are not in readiness state before given timeout.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>KeyboardInterrupt</code> <p>If hosting process receives SIGINT</p> Source code in <code>pytriton/client/client.py</code> <pre><code>async def wait_for_model(self, timeout_s: float):\n    \"\"\"Asynchronous wait for Triton Inference Server and deployed on it model readiness.\n\n    Args:\n        timeout_s: timeout to server and model get into readiness state.\n\n    Raises:\n        PyTritonClientTimeoutError: If server and model are not in readiness state before given timeout.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\n    _LOGGER.debug(f\"Waiting for model {self._model_name} to be ready\")\n    try:\n        async with async_timeout.timeout(self._init_timeout_s):\n            await asyncio_wait_for_model_ready(\n                self._general_client, self._model_name, self._model_version, timeout_s=timeout_s\n            )\n    except asyncio.TimeoutError as e:\n        message = f\"Timeout while waiting for model {self._model_name} to be ready for {self._init_timeout_s}s\"\n        _LOGGER.error(message)\n        raise PyTritonClientTimeoutError(message) from e\n</code></pre>"},{"location":"api/#pytriton.client.client.BaseModelClient","title":"<code>BaseModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>Base client for model deployed on the Triton Inference Server.</p> <p>Inits BaseModelClient for given model deployed on the Triton Inference Server.</p> <p>Common usage:</p> <pre><code>```\nwith ModelClient(\"localhost\", \"BERT\") as client\n    result_dict = client.infer_sample(input1_sample, input2_sample)\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. <code>grpc://localhost:8001</code>. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> required <code>model_name</code> <code>str</code> <p>name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> <code>None</code> <code>lazy_init</code> <code>bool</code> <p>if initialization should be performed just before sending first request to inference server.</p> <code>True</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>timeout in seconds for the server and model to be ready. If not passed, the default timeout of 300 seconds will be used.</p> <code>None</code> <code>inference_timeout_s</code> <code>Optional[float]</code> <p>timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientTimeoutError</code> <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> <code>PyTritonClientInvalidUrlError</code> <p>If provided Triton Inference Server url is invalid.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n):\n    \"\"\"Inits BaseModelClient for given model deployed on the Triton Inference Server.\n\n    Common usage:\n\n        ```\n        with ModelClient(\"localhost\", \"BERT\") as client\n            result_dict = client.infer_sample(input1_sample, input2_sample)\n        ```\n\n    Args:\n        url: The Triton Inference Server url, e.g. `grpc://localhost:8001`.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout in seconds for the server and model to be ready. If not passed, the default timeout of 300 seconds will be used.\n        inference_timeout_s: timeout in seconds for a single model inference request. If not passed, the default timeout of 60 seconds will be used.\n\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError:\n            if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\n    self._init_timeout_s = _DEFAULT_SYNC_INIT_TIMEOUT_S if init_timeout_s is None else init_timeout_s\n    self._inference_timeout_s = DEFAULT_INFERENCE_TIMEOUT_S if inference_timeout_s is None else inference_timeout_s\n    self._network_timeout_s = min(_DEFAULT_NETWORK_TIMEOUT_S, self._init_timeout_s)\n\n    self._general_client = self.create_client_from_url(url, network_timeout_s=self._network_timeout_s)\n    self._infer_client = self.create_client_from_url(url, network_timeout_s=self._inference_timeout_s)\n\n    self._model_name = model_name\n    self._model_version = model_version\n\n    self._request_id_generator = itertools.count(0)\n\n    # Monkey patch __del__ method from client to catch error in client when instance is garbage collected.\n    # This is needed because we are closing client in __exit__ method or in close method.\n    # (InferenceClient uses gevent library which does not support closing twice from different threads)\n    self._monkey_patch_client()\n\n    self._model_config = None\n    self._model_ready = None\n    self._lazy_init: bool = lazy_init\n\n    self._handle_lazy_init()\n</code></pre>"},{"location":"api/#pytriton.client.client.BaseModelClient.create_client_from_url","title":"<code>create_client_from_url(url, network_timeout_s=None)</code>","text":"<p>Create Triton Inference Server client.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>url of the server to connect to. If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added. If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.</p> required <code>network_timeout_s</code> <code>Optional[float]</code> <p>timeout for client commands. Default value is 60.0 s.</p> <code>None</code> <p>Returns:</p> Type Description <p>Triton Inference Server client.</p> <p>Raises:</p> Type Description <code>PyTritonClientInvalidUrlError</code> <p>If provided Triton Inference Server url is invalid.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def create_client_from_url(self, url: str, network_timeout_s: Optional[float] = None):\n    \"\"\"Create Triton Inference Server client.\n\n    Args:\n        url: url of the server to connect to.\n            If url doesn't contain scheme (e.g. \"localhost:8001\") http scheme is added.\n            If url doesn't contain port (e.g. \"localhost\") default port for given scheme is added.\n        network_timeout_s: timeout for client commands. Default value is 60.0 s.\n\n    Returns:\n        Triton Inference Server client.\n\n    Raises:\n        PyTritonClientInvalidUrlError: If provided Triton Inference Server url is invalid.\n    \"\"\"\n    self._triton_url = TritonUrl.from_url(url)\n    self._url = self._triton_url.without_scheme\n    self._triton_client_lib = self.get_lib()\n    self._monkey_patch_client()\n\n    if self._triton_url.scheme == \"grpc\":\n        # by default grpc client has very large number of timeout, thus we want to make it equal to http client timeout\n        network_timeout_s = _DEFAULT_NETWORK_TIMEOUT_S if network_timeout_s is None else network_timeout_s\n        _LOGGER.warning(\n            f\"tritonclient.grpc doesn't support timeout for other commands than infer. Ignoring network_timeout: {network_timeout_s}.\"\n        )\n\n    triton_client_init_kwargs = self._get_init_extra_args()\n\n    _LOGGER.debug(\n        f\"Creating InferenceServerClient for {self._triton_url.with_scheme} with {triton_client_init_kwargs}\"\n    )\n    return self._triton_client_lib.InferenceServerClient(self._url, **triton_client_init_kwargs)\n</code></pre>"},{"location":"api/#pytriton.client.client.BaseModelClient.get_lib","title":"<code>get_lib()</code>","text":"<p>Returns tritonclient library for given scheme.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n    \"\"\"Returns tritonclient library for given scheme.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#pytriton.client.client.DecoupledModelClient","title":"<code>DecoupledModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>             Bases: <code>ModelClient</code></p> <p>Synchronous client for decoupled model deployed on the Triton Inference Server.</p> <p>Inits DecoupledModelClient for given model deployed on the Triton Inference Server.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n):\n    \"\"\"Inits DecoupledModelClient for given model deployed on the Triton Inference Server.\"\"\"\n    super().__init__(\n        url,\n        model_name,\n        model_version,\n        lazy_init=lazy_init,\n        init_timeout_s=init_timeout_s,\n        inference_timeout_s=inference_timeout_s,\n    )\n    if self._triton_url.scheme == \"http\":\n        raise PyTritonClientValueError(\"DecoupledModelClient is only supported for grpc protocol\")\n    self.queue = Queue()\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient","title":"<code>FuturesModelClient(url, model_name, model_version=None, *, max_workers=None, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>A client for interacting with a model deployed on the Triton Inference Server using concurrent.futures.</p> <p>This client allows asynchronous inference requests using a thread pool executor. It can be used to perform inference on a model by providing input data and receiving the corresponding output data. The client can be used in a <code>with</code> statement to ensure proper resource management.</p> <p>Example usage:</p> <pre><code>```python\nwith FuturesModelClient(\"localhost\", \"MyModel\") as client:\n    result_future = client.infer_sample(input1=input1_data, input2=input2_data)\n    # do something else\n    print(result_future.result())\n```\n</code></pre> <p>Initializes the FuturesModelClient for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. <code>grpc://localhost:8001</code>.</p> required <code>model_name</code> <code>str</code> <p>The name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>The version of the model to interact with. If None, the latest version will be used.</p> <code>None</code> <code>max_workers</code> <code>Optional[int]</code> <p>The maximum number of threads that can be used to execute the given calls. If None, there is not limit on the number of threads.</p> <code>None</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>Timeout in seconds for server and model being ready. If non passed default 60 seconds timeout will be used.</p> <code>None</code> <code>inference_timeout_s</code> <code>Optional[float]</code> <p>Timeout in seconds for the single model inference request. If non passed default 60 seconds timeout will be used.</p> <code>None</code> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    max_workers: Optional[int] = None,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n):\n    \"\"\"Initializes the FuturesModelClient for a given model.\n\n    Args:\n        url: The Triton Inference Server url, e.g. `grpc://localhost:8001`.\n        model_name: The name of the model to interact with.\n        model_version: The version of the model to interact with. If None, the latest version will be used.\n        max_workers: The maximum number of threads that can be used to execute the given calls. If None, there is not limit on the number of threads.\n        init_timeout_s: Timeout in seconds for server and model being ready. If non passed default 60 seconds timeout will be used.\n        inference_timeout_s: Timeout in seconds for the single model inference request. If non passed default 60 seconds timeout will be used.\n    \"\"\"\n    self._url = url\n    self._model_name = model_name\n    self._model_version = model_version\n    self._threads = []\n    self._max_workers = max_workers\n    if self._max_workers is not None and self._max_workers &lt;= 0:\n        raise ValueError(\"max_workers must be greater than 0\")\n    kwargs = {}\n    if self._max_workers is not None:\n        kwargs[\"maxsize\"] = self._max_workers\n    self._queue = Queue(**kwargs)\n    self._queue.put((_INIT, None, None))\n    self._init_timeout_s = _DEFAULT_FUTURES_INIT_TIMEOUT_S if init_timeout_s is None else init_timeout_s\n    self._inference_timeout_s = inference_timeout_s\n    self._closed = False\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Create context for using FuturesModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n    \"\"\"Create context for using FuturesModelClient as a context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Close resources used by FuturesModelClient instance when exiting from the context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, exc_type, exc_value, traceback):\n    \"\"\"Close resources used by FuturesModelClient instance when exiting from the context.\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.close","title":"<code>close(wait=True)</code>","text":"<p>Close resources used by FuturesModelClient.</p> <p>This method closes the resources used by the FuturesModelClient instance, including the Triton Inference Server connections. Once this method is called, the FuturesModelClient instance should not be used again.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <p>If True, then shutdown will not return until all running futures have finished executing.</p> <code>True</code> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self, wait=True):\n    \"\"\"Close resources used by FuturesModelClient.\n\n    This method closes the resources used by the FuturesModelClient instance, including the Triton Inference Server connections.\n    Once this method is called, the FuturesModelClient instance should not be used again.\n\n    Args:\n        wait: If True, then shutdown will not return until all running futures have finished executing.\n    \"\"\"\n    if self._closed:\n        _LOGGER.warning(\"FuturesModelClient is already closed\")\n        return\n    _LOGGER.debug(\"Closing FuturesModelClient.\")\n\n    self._closed = True\n    for _ in range(len(self._threads)):\n        self._queue.put((_CLOSE, None, None))\n\n    if wait:\n        _LOGGER.debug(\"Waiting for futures to finish.\")\n        for thread in self._threads:\n            thread.join()\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.infer_batch","title":"<code>infer_batch(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run asynchronous inference on batched data and return a Future object.</p> <p>This method allows the user to perform inference on batched data by providing input data and receiving the corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.</p> <p>Example usage:</p> <pre><code>```python\nwith FuturesModelClient(\"localhost\", \"BERT\") as client:\n    future = client.infer_batch(input1_sample, input2_sample)\n    # do something else\n    print(future.result())\n```\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>```python\nfuture = client.infer_batch(input1, input2)\nfuture = client.infer_batch(a=input1, b=input2)\n```\n</code></pre> <p>Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.</p> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of HTTP headers for the inference request.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Future</code> <p>A Future object wrapping a dictionary of inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientClosedError</code> <p>If the FuturesModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Future:\n    \"\"\"Run asynchronous inference on batched data and return a Future object.\n\n    This method allows the user to perform inference on batched data by providing input data and receiving the corresponding output data.\n    The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.\n\n    Example usage:\n\n        ```python\n        with FuturesModelClient(\"localhost\", \"BERT\") as client:\n            future = client.infer_batch(input1_sample, input2_sample)\n            # do something else\n            print(future.result())\n        ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n        ```python\n        future = client.infer_batch(input1, input2)\n        future = client.infer_batch(a=input1, b=input2)\n        ```\n\n    Mixing of argument passing conventions is not supported and will raise PyTritonClientValueError.\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Optional dictionary of inference parameters.\n        headers: Optional dictionary of HTTP headers for the inference request.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        A Future object wrapping a dictionary of inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\n    return self._execute(name=_INFER_BATCH, request=(inputs, parameters, headers, named_inputs))\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.infer_sample","title":"<code>infer_sample(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run asynchronous inference on a single data sample and return a Future object.</p> <p>This method allows the user to perform inference on a single data sample by providing input data and receiving the corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.</p> <p>Example usage:</p> <pre><code>```python\nwith FuturesModelClient(\"localhost\", \"BERT\") as client:\n    result_future = client.infer_sample(input1=input1_data, input2=input2_data)\n    # do something else\n    print(result_future.result())\n```\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>```python\nfuture = client.infer_sample(input1, input2)\nfuture = client.infer_sample(a=input1, b=input2)\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Optional dictionary of HTTP headers for the inference request.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Future</code> <p>A Future object wrapping a dictionary of inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientClosedError</code> <p>If the FuturesModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Future:\n    \"\"\"Run asynchronous inference on a single data sample and return a Future object.\n\n    This method allows the user to perform inference on a single data sample by providing input data and receiving the\n    corresponding output data. The method returns a Future object that wraps a dictionary of inference results, where dictionary keys are output names.\n\n    Example usage:\n\n        ```python\n        with FuturesModelClient(\"localhost\", \"BERT\") as client:\n            result_future = client.infer_sample(input1=input1_data, input2=input2_data)\n            # do something else\n            print(result_future.result())\n        ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n        ```python\n        future = client.infer_sample(input1, input2)\n        future = client.infer_sample(a=input1, b=input2)\n        ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Optional dictionary of inference parameters.\n        headers: Optional dictionary of HTTP headers for the inference request.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        A Future object wrapping a dictionary of inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\n    return self._execute(\n        name=_INFER_SAMPLE,\n        request=(inputs, parameters, headers, named_inputs),\n    )\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.model_config","title":"<code>model_config()</code>","text":"<p>Obtain the configuration of the model deployed on the Triton Inference Server.</p> <p>This method returns a Future object that will contain the TritonModelConfig object when it is ready. Client will wait init_timeout_s for the server to get into readiness state before obtaining the model configuration.</p> <p>Returns:</p> Type Description <code>Future</code> <p>A Future object that will contain the TritonModelConfig object when it is ready.</p> <p>Raises:</p> Type Description <code>PyTritonClientClosedError</code> <p>If the FuturesModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def model_config(self) -&gt; Future:\n    \"\"\"Obtain the configuration of the model deployed on the Triton Inference Server.\n\n    This method returns a Future object that will contain the TritonModelConfig object when it is ready.\n    Client will wait init_timeout_s for the server to get into readiness state before obtaining the model configuration.\n\n    Returns:\n        A Future object that will contain the TritonModelConfig object when it is ready.\n\n    Raises:\n        PyTritonClientClosedError: If the FuturesModelClient is closed.\n    \"\"\"\n    return self._execute(name=_MODEL_CONFIG)\n</code></pre>"},{"location":"api/#pytriton.client.client.FuturesModelClient.wait_for_model","title":"<code>wait_for_model(timeout_s)</code>","text":"<p>Returns a Future object which result will be None when the model is ready.</p> <p>Typical usage:</p> <pre><code>```python\nwith FuturesModelClient(\"localhost\", \"BERT\") as client\n    future = client.wait_for_model(300.)\n    # do something else\n    future.result()   # wait rest of timeout_s time\n                      # till return None if model is ready\n                      # or raise PyTritonClientTimeutError\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>The maximum amount of time to wait for the model to be ready, in seconds.</p> required <p>Returns:</p> Type Description <code>Future</code> <p>A Future object which result is None when the model is ready.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float) -&gt; Future:\n    \"\"\"Returns a Future object which result will be None when the model is ready.\n\n    Typical usage:\n\n        ```python\n        with FuturesModelClient(\"localhost\", \"BERT\") as client\n            future = client.wait_for_model(300.)\n            # do something else\n            future.result()   # wait rest of timeout_s time\n                              # till return None if model is ready\n                              # or raise PyTritonClientTimeutError\n        ```\n\n    Args:\n        timeout_s: The maximum amount of time to wait for the model to be ready, in seconds.\n\n    Returns:\n        A Future object which result is None when the model is ready.\n    \"\"\"\n    return self._execute(\n        name=_WAIT_FOR_MODEL,\n        request=timeout_s,\n    )\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient","title":"<code>ModelClient(url, model_name, model_version=None, *, lazy_init=True, init_timeout_s=None, inference_timeout_s=None)</code>","text":"<p>             Bases: <code>BaseModelClient</code></p> <p>Synchronous client for model deployed on the Triton Inference Server.</p> <p>Inits ModelClient for given model deployed on the Triton Inference Server.</p> <p>If <code>lazy_init</code> argument is False, model configuration will be read from inference server during initialization.</p> <p>Common usage: <pre><code>with ModelClient(\"localhost\", \"BERT\") as client\n    result_dict = client.infer_sample(input1_sample, input2_sample)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The Triton Inference Server url, e.g. 'grpc://localhost:8001'. In case no scheme is provided http scheme will be used as default. In case no port is provided default port for given scheme will be used - 8001 for grpc scheme, 8000 for http scheme.</p> required <code>model_name</code> <code>str</code> <p>name of the model to interact with.</p> required <code>model_version</code> <code>Optional[str]</code> <p>version of the model to interact with. If model_version is None inference on latest model will be performed. The latest versions of the model are numerically the greatest version numbers.</p> <code>None</code> <code>lazy_init</code> <code>bool</code> <p>if initialization should be performed just before sending first request to inference server.</p> <code>True</code> <code>init_timeout_s</code> <code>Optional[float]</code> <p>timeout for maximum waiting time in loop, which sends retry requests ask if model is ready. It is applied at initialization time only when <code>lazy_init</code> argument is False. Default is to do retry loop at first inference.</p> <code>None</code> <code>inference_timeout_s</code> <code>Optional[float]</code> <p>timeout in seconds for the model inference process. If non passed default 60 seconds timeout will be used. For HTTP client it is not only inference timeout but any client request timeout - get model config, is model loaded. For GRPC client it is only inference timeout.</p> <code>None</code> <p>Raises:</p> Type Description <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientTimeoutError</code> <p>if <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code>.</p> <code>PyTritonClientUrlParseError</code> <p>In case of problems with parsing url.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    model_name: str,\n    model_version: Optional[str] = None,\n    *,\n    lazy_init: bool = True,\n    init_timeout_s: Optional[float] = None,\n    inference_timeout_s: Optional[float] = None,\n):\n    \"\"\"Inits ModelClient for given model deployed on the Triton Inference Server.\n\n    If `lazy_init` argument is False, model configuration will be read\n    from inference server during initialization.\n\n    Common usage:\n    ```\n    with ModelClient(\"localhost\", \"BERT\") as client\n        result_dict = client.infer_sample(input1_sample, input2_sample)\n    ```\n\n    Args:\n        url: The Triton Inference Server url, e.g. 'grpc://localhost:8001'.\n            In case no scheme is provided http scheme will be used as default.\n            In case no port is provided default port for given scheme will be used -\n            8001 for grpc scheme, 8000 for http scheme.\n        model_name: name of the model to interact with.\n        model_version: version of the model to interact with.\n            If model_version is None inference on latest model will be performed.\n            The latest versions of the model are numerically the greatest version numbers.\n        lazy_init: if initialization should be performed just before sending first request to inference server.\n        init_timeout_s: timeout for maximum waiting time in loop, which sends retry requests ask if model is ready. It is applied at initialization time only when `lazy_init` argument is False. Default is to do retry loop at first inference.\n        inference_timeout_s: timeout in seconds for the model inference process.\n            If non passed default 60 seconds timeout will be used.\n            For HTTP client it is not only inference timeout but any client request timeout\n            - get model config, is model loaded. For GRPC client it is only inference timeout.\n\n    Raises:\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientTimeoutError:\n            if `lazy_init` argument is False and wait time for server and model being ready exceeds `init_timeout_s`.\n        PyTritonClientUrlParseError: In case of problems with parsing url.\n    \"\"\"\n    super().__init__(\n        url=url,\n        model_name=model_name,\n        model_version=model_version,\n        lazy_init=lazy_init,\n        init_timeout_s=init_timeout_s,\n        inference_timeout_s=inference_timeout_s,\n    )\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.is_batching_supported","title":"<code>is_batching_supported</code>  <code>property</code>","text":"<p>Checks if model supports batching.</p> <p>Also waits for server to get into readiness state.</p>"},{"location":"api/#pytriton.client.client.ModelClient.model_config","title":"<code>model_config: TritonModelConfig</code>  <code>property</code>","text":"<p>Obtain the configuration of the model deployed on the Triton Inference Server.</p> <p>This method waits for the server to get into readiness state before obtaining the model configuration.</p> <p>Returns:</p> Name Type Description <code>TritonModelConfig</code> <code>TritonModelConfig</code> <p>configuration of the model deployed on the Triton Inference Server.</p> <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If the server and model are not in readiness state before the given timeout.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>KeyboardInterrupt</code> <p>If the hosting process receives SIGINT.</p> <code>PyTritonClientClosedError</code> <p>If the ModelClient is closed.</p>"},{"location":"api/#pytriton.client.client.ModelClient.__enter__","title":"<code>__enter__()</code>","text":"<p>Create context for using ModelClient as a context manager.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __enter__(self):\n    \"\"\"Create context for using ModelClient as a context manager.\"\"\"\n    return self\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.__exit__","title":"<code>__exit__(*_)</code>","text":"<p>Close resources used by ModelClient instance when exiting from the context.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def __exit__(self, *_):\n    \"\"\"Close resources used by ModelClient instance when exiting from the context.\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.close","title":"<code>close()</code>","text":"<p>Close resources used by ModelClient.</p> <p>This method closes the resources used by the ModelClient instance, including the Triton Inference Server connections. Once this method is called, the ModelClient instance should not be used again.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def close(self):\n    \"\"\"Close resources used by ModelClient.\n\n    This method closes the resources used by the ModelClient instance,\n    including the Triton Inference Server connections.\n    Once this method is called, the ModelClient instance should not be used again.\n    \"\"\"\n    _LOGGER.debug(\"Closing ModelClient\")\n    try:\n        if self._general_client is not None:\n            self._general_client.close()\n        if self._infer_client is not None:\n            self._infer_client.close()\n        self._general_client = None\n        self._infer_client = None\n    except Exception as e:\n        _LOGGER.warning(\"Error while closing ModelClient resources: %s\", e)\n        raise e\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.get_lib","title":"<code>get_lib()</code>","text":"<p>Returns tritonclient library for given scheme.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def get_lib(self):\n    \"\"\"Returns tritonclient library for given scheme.\"\"\"\n    return {\"grpc\": tritonclient.grpc, \"http\": tritonclient.http}[self._triton_url.scheme.lower()]\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.infer_batch","title":"<code>infer_batch(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run synchronous inference on batched data.</p> <p>Typical usage:</p> <pre><code>```python\nwith ModelClient(\"localhost\", \"MyModel\") as client:\n    result_dict = client.infer_batch(input1, input2)\n```\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>```python\nresult_dict = client.infer_batch(input1, input2)\nresult_dict = client.infer_batch(a=input1, b=input2)\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>If mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If an error occurred on the inference callable or Triton Inference Server side.</p> <code>PyTritonClientModelDoesntSupportBatchingError</code> <p>If the model doesn't support batching.</p> <code>PyTritonClientValueError</code> <p>if mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>in case of first method call, <code>lazy_init</code> argument is False and wait time for server and model being ready exceeds <code>init_timeout_s</code> or inference time exceeds <code>inference_timeout_s</code> passed to <code>__init__</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If model with given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If error occurred on inference callable or Triton Inference Server side,</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_batch(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run synchronous inference on batched data.\n\n    Typical usage:\n\n        ```python\n        with ModelClient(\"localhost\", \"MyModel\") as client:\n            result_dict = client.infer_batch(input1, input2)\n        ```\n\n    Inference inputs can be provided either as positional or keyword arguments:\n\n        ```python\n        result_dict = client.infer_batch(input1, input2)\n        result_dict = client.infer_batch(a=input1, b=input2)\n        ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n        PyTritonClientModelDoesntSupportBatchingError: If the model doesn't support batching.\n        PyTritonClientValueError: if mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError:\n            in case of first method call, `lazy_init` argument is False\n            and wait time for server and model being ready exceeds `init_timeout_s` or\n            inference time exceeds `inference_timeout_s` passed to `__init__`.\n        PyTritonClientModelUnavailableError: If model with given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If error occurred on inference callable or Triton Inference Server side,\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    if not self.is_batching_supported:\n        raise PyTritonClientModelDoesntSupportBatchingError(\n            f\"Model {self.model_config.model_name} doesn't support batching - use infer_sample method instead\"\n        )\n\n    return self._infer(inputs or named_inputs, parameters, headers)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.infer_sample","title":"<code>infer_sample(*inputs, parameters=None, headers=None, **named_inputs)</code>","text":"<p>Run synchronous inference on a single data sample.</p> <p>Typical usage:</p> <pre><code>```python\nwith ModelClient(\"localhost\", \"MyModel\") as client:\n    result_dict = client.infer_sample(input1, input2)\n```\n</code></pre> <p>Inference inputs can be provided either as positional or keyword arguments:</p> <pre><code>```python\nresult_dict = client.infer_sample(input1, input2)\nresult_dict = client.infer_sample(a=input1, b=input2)\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <p>Inference inputs provided as positional arguments.</p> <code>()</code> <code>parameters</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference parameters.</p> <code>None</code> <code>headers</code> <code>Optional[Dict[str, Union[str, int, bool]]]</code> <p>Custom inference headers.</p> <code>None</code> <code>**named_inputs</code> <p>Inference inputs provided as named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary with inference results, where dictionary keys are output names.</p> <p>Raises:</p> Type Description <code>PyTritonClientValueError</code> <p>If mixing of positional and named arguments passing detected.</p> <code>PyTritonClientTimeoutError</code> <p>If the wait time for the server and model being ready exceeds <code>init_timeout_s</code> or inference request time exceeds <code>inference_timeout_s</code>.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>PyTritonClientInferenceServerError</code> <p>If an error occurred on the inference callable or Triton Inference Server side.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def infer_sample(\n    self,\n    *inputs,\n    parameters: Optional[Dict[str, Union[str, int, bool]]] = None,\n    headers: Optional[Dict[str, Union[str, int, bool]]] = None,\n    **named_inputs,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Run synchronous inference on a single data sample.\n\n    Typical usage:\n\n        ```python\n        with ModelClient(\"localhost\", \"MyModel\") as client:\n            result_dict = client.infer_sample(input1, input2)\n        ```\n    Inference inputs can be provided either as positional or keyword arguments:\n\n        ```python\n        result_dict = client.infer_sample(input1, input2)\n        result_dict = client.infer_sample(a=input1, b=input2)\n        ```\n\n    Args:\n        *inputs: Inference inputs provided as positional arguments.\n        parameters: Custom inference parameters.\n        headers: Custom inference headers.\n        **named_inputs: Inference inputs provided as named arguments.\n\n    Returns:\n        Dictionary with inference results, where dictionary keys are output names.\n\n    Raises:\n        PyTritonClientValueError: If mixing of positional and named arguments passing detected.\n        PyTritonClientTimeoutError: If the wait time for the server and model being ready exceeds `init_timeout_s` or\n            inference request time exceeds `inference_timeout_s`.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        PyTritonClientInferenceServerError: If an error occurred on the inference callable or Triton Inference Server side.\n    \"\"\"\n    _verify_inputs_args(inputs, named_inputs)\n    _verify_parameters(parameters)\n    _verify_parameters(headers)\n\n    if self.is_batching_supported:\n        if inputs:\n            inputs = tuple(data[np.newaxis, ...] for data in inputs)\n        elif named_inputs:\n            named_inputs = {name: data[np.newaxis, ...] for name, data in named_inputs.items()}\n\n    result = self._infer(inputs or named_inputs, parameters, headers)\n\n    return self._debatch_result(result)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.load_model","title":"<code>load_model(config=None, files=None)</code>","text":"<p>Load model on the Triton Inference Server.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[str]</code> <p>str - Optional JSON representation of a model config provided for the load request, if provided, this config will be used for loading the model.</p> <code>None</code> <code>files</code> <code>Optional[dict]</code> <p>dict - Optional dictionary specifying file path (with \"file:\" prefix) in the override model directory to the file content as bytes. The files will form the model directory that the model will be loaded from. If specified, 'config' must be provided to be the model configuration of the override model directory.</p> <code>None</code> Source code in <code>pytriton/client/client.py</code> <pre><code>def load_model(self, config: Optional[str] = None, files: Optional[dict] = None):\n    \"\"\"Load model on the Triton Inference Server.\n\n    Args:\n        config: str - Optional JSON representation of a model config provided for\n            the load request, if provided, this config will be used for\n            loading the model.\n        files: dict - Optional dictionary specifying file path (with \"file:\" prefix) in\n            the override model directory to the file content as bytes.\n            The files will form the model directory that the model will be\n            loaded from. If specified, 'config' must be provided to be\n            the model configuration of the override model directory.\n    \"\"\"\n    self._general_client.load_model(self._model_name, config=config, files=files)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.unload_model","title":"<code>unload_model()</code>","text":"<p>Unload model from the Triton Inference Server.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def unload_model(self):\n    \"\"\"Unload model from the Triton Inference Server.\"\"\"\n    self._general_client.unload_model(self._model_name)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.wait_for_model","title":"<code>wait_for_model(timeout_s)</code>","text":"<p>Wait for the Triton Inference Server and the deployed model to be ready.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>timeout in seconds to wait for the server and model to be ready.</p> required <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If the server and model are not ready before the given timeout.</p> <code>PyTritonClientModelUnavailableError</code> <p>If the model with the given name (and version) is unavailable.</p> <code>KeyboardInterrupt</code> <p>If the hosting process receives SIGINT.</p> <code>PyTritonClientClosedError</code> <p>If the ModelClient is closed.</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_model(self, timeout_s: float):\n    \"\"\"Wait for the Triton Inference Server and the deployed model to be ready.\n\n    Args:\n        timeout_s: timeout in seconds to wait for the server and model to be ready.\n\n    Raises:\n        PyTritonClientTimeoutError: If the server and model are not ready before the given timeout.\n        PyTritonClientModelUnavailableError: If the model with the given name (and version) is unavailable.\n        KeyboardInterrupt: If the hosting process receives SIGINT.\n        PyTritonClientClosedError: If the ModelClient is closed.\n    \"\"\"\n    if self._general_client is None:\n        raise PyTritonClientClosedError(\"ModelClient is closed\")\n    wait_for_model_ready(self._general_client, self._model_name, self._model_version, timeout_s=timeout_s)\n</code></pre>"},{"location":"api/#pytriton.client.client.ModelClient.wait_for_server","title":"<code>wait_for_server(timeout_s)</code>","text":"<p>Wait for Triton Inference Server readiness.</p> <p>Parameters:</p> Name Type Description Default <code>timeout_s</code> <code>float</code> <p>timeout to server get into readiness state.</p> required <p>Raises:</p> Type Description <code>PyTritonClientTimeoutError</code> <p>If server is not in readiness state before given timeout.</p> <code>KeyboardInterrupt</code> <p>If hosting process receives SIGINT</p> Source code in <code>pytriton/client/client.py</code> <pre><code>def wait_for_server(self, timeout_s: float):\n    \"\"\"Wait for Triton Inference Server readiness.\n\n    Args:\n        timeout_s: timeout to server get into readiness state.\n\n    Raises:\n        PyTritonClientTimeoutError: If server is not in readiness state before given timeout.\n        KeyboardInterrupt: If hosting process receives SIGINT\n    \"\"\"\n    wait_for_server_ready(self._general_client, timeout_s=timeout_s)\n</code></pre>"},{"location":"binding_configuration/","title":"Binding Configuration","text":""},{"location":"binding_configuration/#binding-configuration","title":"Binding Configuration","text":"<p>The additional configuration of binding the model for running a model through the Triton Inference Server can be provided in the <code>config</code> argument in the <code>bind</code> method. This section describes the possible configuration enhancements. The configuration of the model can be adjusted by overriding the defaults for the <code>ModelConfig</code> object.</p> <pre><code>from pytriton.model_config.common import DynamicBatcher\n\nclass ModelConfig:\n    batching: bool = True\n    max_batch_size: int = 4\n    batcher: DynamicBatcher = DynamicBatcher()\n    response_cache: bool = False\n</code></pre>"},{"location":"binding_configuration/#batching","title":"Batching","text":"<p>The batching feature collects one or more samples and passes them to the model together. The model processes multiple samples at the same time and returns the output for all the samples processed together.</p> <p>Batching can significantly improve throughput. Processing multiple samples at the same time leverages the benefits of utilizing GPU performance for inference.</p> <p>The Triton Inference Server is responsible for collecting multiple incoming requests into a single batch. The batch is passed to the model, which improves the inference performance (throughput and latency). This feature is called <code>dynamic batching</code>, which collects samples from multiple clients into a single batch processed by the model.</p> <p>On the PyTriton side, the <code>infer_fn</code> obtain the fully created batch by Triton Inference Server so the only responsibility is to perform computation and return the output.</p> <p>By default, batching is enabled for the model. The default behavior for Triton is to have dynamic batching enabled. If your model does not support batching, use <code>batching=False</code> to disable it in Triton.</p>"},{"location":"binding_configuration/#maximal-batch-size","title":"Maximal batch size","text":"<p>The maximal batch size defines the number of samples that can be processed at the same time by the model. This configuration has an impact not only on throughput but also on memory usage, as a bigger batch means more data loaded to the memory at the same time.</p> <p>The <code>max_batch_size</code> has to be a value greater than or equal to 1.</p>"},{"location":"binding_configuration/#dynamic-batching","title":"Dynamic batching","text":"<p>The dynamic batching is a Triton Inference Server feature and can be configured by defining the <code>DynamicBatcher</code> object:</p> <pre><code>from typing import Dict, Optional\nfrom pytriton.model_config.common import QueuePolicy\n\nclass DynamicBatcher:\n    max_queue_delay_microseconds: int = 0\n    preferred_batch_size: Optional[list] = None\n    preserve_ordering: bool = False\n    priority_levels: int = 0\n    default_priority_level: int = 0\n    default_queue_policy: Optional[QueuePolicy] = None\n    priority_queue_policy: Optional[Dict[int, QueuePolicy]] = None\n</code></pre> <p>More about dynamic batching can be found in the Triton Inference Server documentation and API spec</p>"},{"location":"binding_configuration/#response-cache","title":"Response cache","text":"<p>The Triton Inference Server provides functionality to use a cached response for the model. To use the response cache:</p> <ul> <li>provide the <code>cache_config</code> in <code>TritonConfig</code></li> <li>set <code>response_cache=True</code> in <code>ModelConfig</code></li> </ul> <p>More about response cache can be found in the Triton Response Cache page.</p> <p>Example:</p> <pre><code>import numpy as np\n\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\n\ntriton_config = TritonConfig(\n    cache_config=[f\"local,size={1024 * 1024}\"],  # 1MB\n)\n\n@batch\ndef _add_sub(**inputs):\n    a_batch, b_batch = inputs.values()\n    add_batch = a_batch + b_batch\n    sub_batch = a_batch - b_batch\n    return {\"add\": add_batch, \"sub\": sub_batch}\n\nwith Triton(config=triton_config) as triton:\n    triton.bind(\n        model_name=\"AddSub\",\n        infer_func=_add_sub,\n        inputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        outputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        config=ModelConfig(max_batch_size=8, response_cache=True)\n    )\n    ...\n</code></pre>"},{"location":"binding_models/","title":"Binding Model to Triton","text":""},{"location":"binding_models/#binding-models-to-triton","title":"Binding Models to Triton","text":"<p>The Triton class provides methods to bind one or multiple models to the Triton server in order to expose HTTP/gRPC endpoints for inference serving:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    input1, input2 = inputs.values()\n    outputs = model(input1, input2)\n    return [outputs]\n\nwith Triton() as triton:\n  triton.bind(\n      model_name=\"ModelName\",\n      infer_func=infer_fn,\n      inputs=[\n          Tensor(shape=(1,), dtype=np.bytes_),  # sample containing single bytes value\n          Tensor(shape=(-1,), dtype=np.bytes_)  # sample containing vector of bytes\n      ],\n      outputs=[\n          Tensor(shape=(-1,), dtype=np.float32),\n      ],\n      config=ModelConfig(max_batch_size=8),\n      strict=True,\n  )\n</code></pre> <p>The <code>bind</code> method's mandatory arguments are:</p> <ul> <li><code>model_name</code>: defines under which name the model is available in Triton Inference Server</li> <li><code>infer_func</code>: function or Python <code>Callable</code> object which obtains the data passed in the request and returns the output</li> <li><code>inputs</code>: defines the number, types, and shapes for model inputs</li> <li><code>outputs</code>: defines the number, types, and shapes for model outputs</li> <li><code>config</code>: more customization for model deployment and behavior on the Triton server</li> <li><code>strict</code>: enable inference callable output validation of data types and shapes against provided model config (default: False)</li> </ul> <p>Once the <code>bind</code> method is called, the model is created in the Triton Inference Server model store under the provided <code>model_name</code>.</p>"},{"location":"binding_models/#inference-callable","title":"Inference Callable","text":"<p>The inference callable is an entry point for inference. This can be any callable that receives the data for model inputs in the form of a list of request dictionaries where input names are mapped into ndarrays. Input can be also adapted to different more convenient forms using a set of decorators. More details about designing inference callable and using of decorators can be found in Inference Callable page.</p> <p>In the simplest implementation for functionality that passes input data on output, a lambda can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n\nwith Triton() as triton:\n  triton.bind(\n      model_name=\"Identity\",\n      infer_func=lambda requests: requests,\n      inputs=[Tensor(dtype=np.float32, shape=(1,))],\n      outputs=[Tensor(dtype=np.float32, shape=(1,))],\n      config=ModelConfig(max_batch_size=8)\n  )\n</code></pre>"},{"location":"binding_models/#multi-instance-model-inference","title":"Multi-instance model inference","text":"<p>Multi-instance model inference is a mechanism for loading multiple instances of the same model and calling them alternately (to hide transfer overhead).</p> <p>With the <code>Triton</code> class, it can be realized by providing the list of multiple inference callables to <code>Triton.bind</code> in the <code>infer_func</code> parameter.</p> <p>The example presents multiple instances of the Linear PyTorch model loaded on separate devices.</p> <p>First, define the wrapper class for the inference handler. The class initialization receives a model and device as arguments. The inference handling is done by method <code>__call__</code> where the <code>model</code> instance is called:</p> <pre><code>import torch\nfrom pytriton.decorators import batch\n\n\nclass _InferFuncWrapper:\n    def __init__(self, model: torch.nn.Module, device: str):\n        self._model = model\n        self._device = device\n\n    @batch\n    def __call__(self, **inputs):\n        (input1_batch,) = inputs.values()\n        input1_batch_tensor = torch.from_numpy(input1_batch).to(self._device)\n        output1_batch_tensor = self._model(input1_batch_tensor)\n        output1_batch = output1_batch_tensor.cpu().detach().numpy()\n        return [output1_batch]\n</code></pre> <p>Next, create a factory function where a model and instances of <code>_InferFuncWrapper</code> are created - one per each device:</p> <pre><code>def _infer_function_factory(devices):\n    infer_fns = []\n    for device in devices:\n        model = torch.nn.Linear(20, 30).to(device).eval()\n        infer_fns.append(_InferFuncWrapper(model=model, device=device))\n\n    return infer_fns\n</code></pre> <p>Finally, the list of callable objects is passed to <code>infer_func</code> parameter of the <code>Triton.bind</code> function:</p> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\n\nwith Triton() as triton:\n  triton.bind(\n      model_name=\"Linear\",\n      infer_func=_infer_function_factory(devices=[\"cuda\", \"cpu\"]),\n      inputs=[\n          Tensor(dtype=np.float32, shape=(-1,)),\n      ],\n      outputs=[\n          Tensor(dtype=np.float32, shape=(-1,)),\n      ],\n      config=ModelConfig(max_batch_size=16),\n  )\n  ...\n</code></pre> <p>Once the multiple callable objects are passed to <code>infer_func</code>, the Triton server gets information that multiple instances of the same model have been created. The incoming requests are distributed among created instances. In our case executing two instances of a <code>Linear</code> model loaded on CPU and GPU devices.</p>"},{"location":"binding_models/#defining-inputs-and-outputs","title":"Defining Inputs and Outputs","text":"<p>The integration of the Python model requires the inputs and outputs types of the model. This is required to correctly map the input and output data passed through the Triton Inference Server.</p> <p>The simplest definition of model inputs and outputs expects providing the type of data and the shape per input:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\n\ninputs = [\n    Tensor(dtype=np.float32, shape=(-1,)),\n]\noutput = [\n    Tensor(dtype=np.float32, shape=(-1,)),\n    Tensor(dtype=np.int32, shape=(-1,)),\n]\n</code></pre> <p>The provided configuration creates the following tensors:</p> <ul> <li>Single input:</li> <li>name: INPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>Two outputs:</li> <li>name: OUTPUT_1, data type: FLOAT32, shape=(-1,)</li> <li>name: OUTPUT_2, data type: INT32, shape=(-1,)</li> </ul> <p>The <code>-1</code> means a dynamic shape of the input or output.</p> <p>To define the name of the input and its exact shape, the following definition can be used:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\n\ninputs = [\n    Tensor(name=\"image\", dtype=np.float32, shape=(224, 224, 3)),\n]\noutputs = [\n    Tensor(name=\"class\", dtype=np.int32, shape=(1000,)),\n]\n</code></pre> <p>This definition describes that the model has:</p> <ul> <li>a single input named <code>image</code> of size 224x224x3 and 32-bit floating-point data type</li> <li>a single output named <code>class</code> of size 1000 and 32-bit integer data type.</li> </ul> <p>The <code>dtype</code> parameter can be either <code>numpy.dtype</code>, <code>numpy.dtype.type</code>, or <code>str</code>. For example:</p> <pre><code>import numpy as np\nfrom pytriton.model_config import Tensor\n\ntensor1 = Tensor(name=\"tensor1\", shape=(-1,), dtype=np.float32),\ntensor2 = Tensor(name=\"tensor2\", shape=(-1,), dtype=np.float32().dtype),\ntensor3 = Tensor(name=\"tensor3\", shape=(-1,), dtype=\"float32\"),\n</code></pre> <p>dtype for bytes and string inputs/outputs</p> <p>When using the <code>bytes</code> dtype, NumPy removes trailing <code>\\x00</code> bytes. Therefore, for arbitrary bytes, it is required to use <code>object</code> dtype.</p> <pre><code>&gt; np.array([b\"\\xff\\x00\"])\narray([b'\\xff'], dtype='|S2')\n\n&gt; np.array([b\"\\xff\\x00\"], dtype=object)\narray([b'\\xff\\x00'], dtype=object)\n</code></pre> <p>For ease of use, for encoded string values, users might use <code>bytes</code> dtype.</p>"},{"location":"binding_models/#throwing-unrecoverable-errors","title":"Throwing Unrecoverable errors","text":"<p>When the model gets into a state where further inference is impossible, you can throw PyTritonUnrecoverableError from the inference callable. This will cause NVIDIA Triton Inference Server to shut down. This might be useful when the model is deployed on a cluster in a multi-node setup. In that case to recover the model you need to restart all \"workers\" on the cluster.</p> <p>When the model gets into a state where further inference is impossible, you can throw the PyTritonUnrecoverableError from the inference callable. This will cause the NVIDIA Triton Inference Server to shut down. This might be useful when the model is deployed on a cluster in a multi-node setup. In that case, to recover the model, you need to restart all \"workers\" on the cluster.</p> <pre><code>from typing import Dict\nimport numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.exceptions import PyTritonUnrecoverableError\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    ...\n\n    try:\n        outputs = model(**inputs)\n    except Exception as e:\n        raise PyTritonUnrecoverableError(\n            \"Some unrecoverable error occurred, \"\n            \"thus no further inferences are possible.\"\n        ) from e\n\n    ...\n    return outputs\n</code></pre>"},{"location":"building/","title":"Building binary package","text":""},{"location":"building/#building-binary-package-from-source","title":"Building binary package from source","text":"<p>This guide provides an outline of the process for building the PyTriton binary package from source. It offers the flexibility to modify the PyTriton code and integrate it with various versions of the Triton Inference Server, including custom builds. Additionally, it allows you to incorporate hotfixes that have not yet been officially released.</p>"},{"location":"building/#prerequisites","title":"Prerequisites","text":"<p>Before building the PyTriton binary package, ensure the following:</p> <ul> <li>Docker with buildx plugin is installed on the system. For more information, refer to the Docker documentation.</li> <li>Access to the Docker daemon is available from the system or container.</li> </ul> <p>If you plan to build <code>arm64</code> wheel on <code>amd64</code> machine we suggest to use QUEMU for emulation. To enable QUEMU on Ubuntu you need to: - Install the QEMU packages on your x86 machine: <pre><code>sudo apt-get install qemu binfmt-support qemu-user-static\n</code></pre> - Register the QEMU emulators for ARM architectures: <pre><code>docker run --rm --privileged multiarch/qemu-user-static --reset -p yes\n</code></pre></p>"},{"location":"building/#building-pytriton-binary-package","title":"Building PyTriton binary package","text":"<p>To build the wheel binary package, follow these steps from the root directory of the project:</p> <pre><code>make install-dev\nmake dist\n</code></pre> <p>Note: The default build create wheel for <code>x86_64</code> architecture. If you would like to build the wheel for <code>aarch64</code> use <pre><code>make dist -e PLATFORM=linux/arm64\n</code></pre> We use Docker convention name for platforms. The supported options are <code>linux/amd64</code> and <code>linux/arm64</code>.</p> <p>The wheel package will be located in the <code>dist</code> directory. To install the library, run the following <code>pip</code> command:</p> <pre><code>pip install dist/nvidia_pytriton-*-py3-none-*.whl\n</code></pre> <p>Note: The wheel name would have <code>x86_64</code> or <code>aarch64</code> in name based on selected platform.</p>"},{"location":"building/#building-for-a-specific-triton-inference-server-version","title":"Building for a specific Triton Inference Server version","text":"<p>Building for an unsupported OS or hardware platform is possible. PyTriton requires a Python backend and either an HTTP or gRPC endpoint. The build can be CPU-only, as inference is performed on Inference Handlers.</p> <p>For more information on the Triton Inference Server build process, refer to the building section of Triton Inference Server documentation.</p> <p>Untested Build</p> <p>The Triton Inference Server has only been rigorously tested on Ubuntu 20.04. Other OS and hardware platforms are not officially supported. You can test the build by following the steps outlined in the Triton Inference Server testing guide.</p> <p>By the following docker method steps you can create a <code>tritonserver:latest</code> Docker image that can be used to build PyTriton with the following command:</p> <pre><code>make dist -e TRITONSERVER_IMAGE_VERSION=latest -e TRITONSERVER_IMAGE_NAME=tritonserver:latest\n</code></pre>"},{"location":"chunking_guide/","title":"Chunking and batching","text":""},{"location":"chunking_guide/#how-to-use-pytriton-client-to-split-a-large-input-into-smaller-batches-and-send-them-to-the-server-in-parallel","title":"How to use PyTriton client to split a large input into smaller batches and send them to the server in parallel","text":"<p>In this article, you will learn how to use PyTriton clients to create a chunking client that can handle inputs that are larger than the maximum batch size of your model.</p> <p>First, you need to create a model that can process a batch of inputs and produce a batch of outputs. For simplicity, let's assume that your model can only handle two inputs at a time. We will call this model \"Batch2\" and run it on a local Triton server.</p> <p>Next, you need to create a client that can send requests to your model. In this article, we will use the FuturesModelClient, which returns a Future object for each request. A Future object is a placeholder that can be used to get the result or check the status of the request later.</p> <p>However, there is a problem with using the FuturesModelClient directly. If you try to send an input that is larger than the maximum batch size of your model, you will get an error. For example, the following code tries to send an input of size 4 to the \"Batch2\" model, which has a maximum batch size of 2:</p> <pre><code>import numpy as np\nfrom pytriton.client import FuturesModelClient\n\nwith FuturesModelClient(f\"localhost\", \"Batch2\") as client:\n    input_tensor = np.zeros((4, 1), dtype=np.int32)\n    print(client.infer_batch(input_tensor).result())\n</code></pre> <p>This code will raise an exception like this:</p> <pre><code>PyTritonClientInferenceServerError: Error occurred during inference request. Message: [request id: 0] inference request batch-size must be &lt;= 2 for 'Batch2'\n</code></pre> <p>To solve this problem, we can use a ChunkingClient class that inherits from FuturesModelClient and overrides the infer_batch method. The ChunkingClient class takes a chunking strategy as an argument, which is a function that takes the input dictionary and the maximum batch size as parameters and yields smaller dictionaries of inputs. The default chunking strategy simply splits the input along the first dimension according to the maximum batch size. For example, if the input is <code>{\"INPUT_1\": np.zeros((5, 1), dtype=np.int32)}</code> and the maximum batch size is 2, then the default chunking strategy will yield:</p> <pre><code>{\"INPUT_1\": np.zeros((2, 1), dtype=np.int32)}\n{\"INPUT_1\": np.zeros((2, 1), dtype=np.int32)}\n{\"INPUT_1\": np.zeros((1, 1), dtype=np.int32)}\n</code></pre> <p>You can also define your own chunking strategy if you have more complex logic for splitting your input.</p> <p><pre><code># Define a ChunkingClient class that inherits from FuturesModelClient and splits the input into smaller batches\nimport concurrent.futures\nfrom pytriton.client import FuturesModelClient\n\nclass ChunkingClient(FuturesModelClient):\n    def __init__(self, host, model_name, chunking_strategy=None, max_workers=None):\n        super().__init__(host, model_name, max_workers=max_workers)\n        self.chunking_strategy = chunking_strategy or self.default_chunking_strategy\n\n    def default_chunking_strategy(self, kwargs, max_batch_size):\n        # Split the input by the first dimension according to the max batch size\n        size_of_dimention_0 = self.find_size_0(kwargs)\n        for i in range(0, size_of_dimention_0, max_batch_size):\n            yield {key: value[i:i+max_batch_size] for key, value in kwargs.items()}\n\n    def find_size_0(self, kwargs):\n        # Check the size of the first dimension of each tensor and raise errors if they are not consistent or valid\n        size_of_dimention_0 = None\n        for key, value in kwargs.items():\n            if isinstance(value, np.ndarray):\n                if value.ndim &gt; 0:\n                    size = value.shape[0]\n                    if size_of_dimention_0 is None or size_of_dimention_0 == size:\n                        size_of_dimention_0 = size\n                    else:\n                        raise ValueError(\"The tensors have different sizes at the first dimension\")\n                else:\n                    raise ValueError(\"The tensor has no first dimension\")\n            else:\n                raise TypeError(\"The value is not a numpy tensor\")\n        return size_of_dimention_0\n\n    def infer_batch(self, *args, **kwargs):\n        max_batch_size = self.model_config().result().max_batch_size\n        # Send the smaller batches to the server in parallel and yield the futures with results\n        futures = [super(ChunkingClient, self).infer_batch(*args, **chunk) for chunk in self.chunking_strategy(kwargs, max_batch_size)]\n        for future in futures:\n            yield future\n</code></pre> To use the ChunkingClient class, you can create an instance of it and use it in a context manager. For example:</p> <pre><code># Use the ChunkingClient class with the default strategy to send an input of size 5 to the \"Batch2\" model\nimport numpy as np\nfrom pytriton.client import FuturesModelClient\nchunker_client = ChunkingClient(\"localhost\", \"Batch2\")\nresults = []\nwith chunker_client as client:\n    input_tensor = np.zeros((5, 1), dtype=np.int32)\n    # Print the results of each future without concatenating them\n    for future in client.infer_batch(INPUT_1=input_tensor):\n        results.append(future.result())\nprint(results)\n</code></pre> <p>This code will print:</p> <pre><code>{'OUTPUT_1': array([[0],\n       [0]], dtype=int32)}\n{'OUTPUT_1': array([[0],\n       [0]], dtype=int32)}\n{'OUTPUT_1': array([[0]], dtype=int32)}\n</code></pre> <p>You can see that the input is split into three batches of sizes 2, 2, and 1, and each batch is sent to the server in parallel. The results are returned as futures that can be accessed individually without concatenating them.</p>"},{"location":"clients/","title":"Clients","text":""},{"location":"clients/#triton-clients","title":"Triton clients","text":"<p>The prerequisite for this page is to install PyTriton. You also need <code>Linear</code> model described in quick_start. You should run it so client can connect to it.</p> <p>The clients section presents how to send requests to the Triton Inference Server using the PyTriton library.</p>"},{"location":"clients/#modelclient","title":"ModelClient","text":"<p>ModelClient is a simple client that can perform inference requests synchronously. You can use ModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the ModelClient object.</p> <p>For example, you can use ModelClient to send requests to a PyTorch model that performs linear regression:</p> <pre><code>import torch\nfrom pytriton.client import ModelClient\n\n# Create some input data as a numpy array\ninput1_data = torch.randn(128, 2).cpu().detach().numpy()\n\n# Create a ModelClient object with the server address and model name\nwith ModelClient(\"localhost:8000\", \"Linear\") as client:\n    # Call the infer_batch method with the input data\n    result_dict = client.infer_batch(input1_data)\n\n# Print the result dictionary\nprint(result_dict)\n</code></pre> <p>You can also use ModelClient to send requests to a model that performs image classification. The example assumes that a model takes in an image and returns the top 5 predicted classes. This model is not included in the PyTriton library.</p> <p>You need to convert the image to a numpy array and resize it to the expected input shape. You can use Pillow package to do this.</p> <pre><code>import numpy as np\nfrom PIL import Image\nfrom pytriton.client import ModelClient\n\n# Create some input data as a numpy array of an image\nimg = Image.open(\"cat.jpg\")\nimg = img.resize((224, 224))\ninput_data = np.array(img)\n\n# Create a ModelClient object with the server address and model name\nwith ModelClient(\"localhost:8000\", \"ImageNet\") as client:\n    # Call the infer_batch method with the input data\n    result_dict = client.infer_sample(input_data)\n\n# Print the result dictionary\nprint(result_dict)\n</code></pre> <p>You need to install Pillow package to run the above example: <pre><code>pip install Pillow\n</code></pre></p>"},{"location":"clients/#futuresmodelclient","title":"FuturesModelClient","text":"<p>FuturesModelClient is a concurrent.futures based client that can perform inference requests in a parallel way. You can use FuturesModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the FuturesModelClient object.</p> <p>For example, you can use FuturesModelClient to send multiple requests to a text generation model that takes in text prompts and returns generated texts. The TextGen model is not included in the PyTriton library. The example assumes that the model returns a single output tensor with the generated text. The example also assumes that the model takes in a list of text prompts and returns a list of generated texts.</p> <p>You need to convert the text prompts to numpy arrays of bytes using a tokenizer from transformers. You also need to detokenize the output texts using the same tokenizer:</p> <pre><code>import numpy as np\nfrom pytriton.client import FuturesModelClient\nfrom transformers import AutoTokenizer\n\n# Create some input data as a list of text prompts\ninput_data_list_text = [\"Write a haiku about winter.\", \"Summarize the article below in one sentence.\", \"Generate a catchy slogan for PyTriton.\"]\n\n# Create a tokenizer from transformers\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Convert the text prompts to numpy arrays of bytes using the tokenizer\ninput_data_list = [np.array(tokenizer.encode(prompt)) for prompt in input_data_list_text]\n\n# Create a FuturesModelClient object with the server address and model name\nwith FuturesModelClient(\"localhost:8000\", \"TextGen\") as client:\n    # Call the infer_sample method for each input data in the list and store the returned futures\n    output_data_futures = [client.infer_sample(input_data) for input_data in input_data_list]\n    # Wait for all the futures to complete and get the results\n    output_data_list = [output_data_future.result() for output_data_future in output_data_futures]\n\n# Print tokens\nprint(output_data_list)\n\n# Detokenize the output texts using the tokenizer and print them\noutput_texts = [tokenizer.decode(output_data[\"OUTPUT_1\"]) for output_data in output_data_list]\nfor output_text in output_texts:\n    print(output_text)\n</code></pre> <p>You need to install transformers package to run the above example: <pre><code>pip install transformers\n</code></pre></p> <p>You can also use FuturesModelClient to send multiple requests to an image classification model that takes in image data and returns class labels or probabilities. The ImageNet model is described above.</p> <p>In this case, you can use the infer_batch method to send a batch of images as input and get a batch of outputs. You need to stack the images along the first dimension to form a batch. You can also print the class names corresponding to the output labels:</p> <pre><code>import numpy as np\nfrom PIL import Image\nfrom pytriton.client import FuturesModelClient\n\n# Create some input data as a list of lists of image arrays\ninput_data_list = []\nfor batch in [[\"cat.jpg\", \"dog.jpg\", \"bird.jpg\"], [\"car.jpg\", \"bike.jpg\", \"bus.jpg\"], [\"apple.jpg\", \"banana.jpg\", \"orange.jpg\"]]:\n  batch_data = []\n  for filename in batch:\n    img = Image.open(filename)\n    img = img.resize((224, 224))\n    img = np.array(img)\n    batch_data.append(img)\n  # Stack the images along the first dimension to form a batch\n  batch_data = np.stack(batch_data, axis=0)\n  input_data_list.append(batch_data)\n\n# Create a list of class names for ImageNet\nclass_names = [\"tench\", \"goldfish\", \"great white shark\", ...]\n\n# Create a FuturesModelClient object with the server address and model name\nwith FuturesModelClient(\"localhost:8000\", \"ImageNet\") as client:\n    # Call the infer_batch method for each input data in the list and store the returned futures\n    output_data_futures = [client.infer_batch(input_data) for input_data in input_data_list]\n    # Wait for all the futures to complete and get the results\n    output_data_list = [output_data_future.result() for output_data_future in output_data_futures]\n\n# Print the list of result dictionaries\nprint(output_data_list)\n\n# Print the class names corresponding to the output labels for each batch\nfor output_data in output_data_list:\n  output_labels = output_data[\"OUTPUT_1\"]\n  for output_label in output_labels:\n    class_name = class_names[output_label]\n    print(f\"The image is classified as {class_name}.\")\n</code></pre>"},{"location":"clients/#asynciomodelclient","title":"AsyncioModelClient","text":"<p>AsyncioModelClient is an asynchronous client that can perform inference requests using the asyncio library. You can use AsyncioModelClient to communicate with the deployed model using HTTP or gRPC protocol. You can specify the protocol when creating the AsyncioModelClient object.</p> <p>For example, you can use AsyncioModelClient to send requests to a PyTorch model that performs linear regression:</p> <pre><code>import torch\nfrom pytriton.client import AsyncioModelClient\n\n# Create some input data as a numpy array\ninput1_data = torch.randn(2).cpu().detach().numpy()\n\n# Create an AsyncioModelClient object with the server address and model name\nasync with AsyncioModelClient(\"localhost:8000\", \"Linear\") as client:\n    # Call the infer_sample method with the input data\n    result_dict = await client.infer_sample(input1_data)\n\n# Print the result dictionary\nprint(result_dict)\n</code></pre> <p>You can also use FastAPI to create a web application that exposes the results of inference at an HTTP endpoint. FastAPI is a modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.</p> <p>To use FastAPI, you need to install it with:</p> <pre><code>pip install fastapi\n</code></pre> <p>You also need an ASGI server, for production such as Uvicorn or Hypercorn.</p> <p>To install Uvicorn, run:</p> <pre><code>pip install uvicorn[standard]\n</code></pre> <p>The <code>uvicorn</code> uses port <code>8000</code> as default for web server. Triton server default port is also <code>8000</code> for HTTP protocol. You can change uvicorn port by using <code>--port</code> option. PyTriton also supports custom ports configuration for Triton server. The class <code>TritonConfig</code> contains parameters for ports configuration. You can pass it to <code>Triton</code> during initialization:</p> <pre><code>config = TritonConfig(http_port=8015)\ntriton_server = Triton(config=config)\n</code></pre> <p>You can use this <code>triton_server</code> object to bind your inference model and run HTTP endpoint from Triton Inference Server at port <code>8015</code>.</p> <p>Then you can create a FastAPI app that uses the AsyncioModelClient to perform inference and return the results as JSON:</p> <pre><code>from fastapi import FastAPI\nimport torch\nfrom pytriton.client import AsyncioModelClient\n\napp = FastAPI()\n\n@app.get(\"/predict\")\nasync def predict():\n    # Create some input data as a numpy array\n    input1_data = torch.randn(2).cpu().detach().numpy()\n\n    # Create an AsyncioModelClient object with the server address and model name\n    async with AsyncioModelClient(\"localhost:8000\", \"Linear\") as client:\n        # Call the infer_sample method with the input data\n        result_dict = await client.infer_sample(input1_data)\n\n    # Return the result dictionary as JSON\n    return result_dict\n</code></pre> <p>Save this file as <code>main.py</code>.</p> <p>To run the app, use the command:</p> <pre><code>uvicorn main:app --reload --port 8015\n</code></pre> <p>You can then access the endpoint at <code>http://127.0.0.1:8015/predict</code> and see the JSON response.</p> <p>You can also check the interactive API documentation at <code>http://127.0.0.1:8015/docs</code>.</p> <p>You can test your server using curl:</p> <pre><code>curl -X 'GET' \\\n  'http://127.0.0.1:8015/predict' \\\n  -H 'accept: application/json'\n</code></pre> <p>Command will print three random numbers:</p> <pre><code>[-0.2608422636985779,-0.6435106992721558,-0.3492531180381775]\n</code></pre> <p>For more information about FastAPI and Uvicorn, check out these links:</p> <ul> <li>FastAPI documentation</li> <li>Uvicorn documentation</li> </ul>"},{"location":"clients/#client-timeouts","title":"Client timeouts","text":"<p>When creating a ModelClient or FuturesModelClient object, you can specify the timeout for waiting until the server and model are ready using the <code>init_timeout_s</code> parameter. By default, the timeout is set to 5 minutes (300 seconds).</p> <p>Example usage:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\n\ninput1_data = np.random.randn(128, 2)\nwith ModelClient(\"localhost\", \"MyModel\", init_timeout_s=120) as client:\n    # Raises PyTritonClientTimeoutError if the server or model is not ready within the specified timeout\n    result_dict = client.infer_batch(input1_data)\n\n\nwith FuturesModelClient(\"localhost\", \"MyModel\", init_timeout_s=120) as client:\n    future = client.infer_batch(input1_data)\n    ...\n    # It will raise `PyTritonClientTimeoutError` if the server is not ready and the model is not loaded within 120 seconds\n    # from the time `infer_batch` was called by a thread from `ThreadPoolExecutor`\n    result_dict = future.result()\n</code></pre> <p>You can disable the default behavior of waiting for the server and model to be ready during first inference request by setting <code>lazy_init</code> to <code>False</code>:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\n\ninput1_data = np.random.randn(128, 2)\n\n# will raise PyTritonClientTimeoutError if server is not ready and model loaded\n# within 120 seconds during intialization of client\nwith ModelClient(\"localhost\", \"MyModel\", init_timeout_s=120, lazy_init=False) as client:\n    result_dict = client.infer_batch(input1_data)\n</code></pre> <p>You can specify the timeout for the client to wait for the inference response from the server. The default timeout is 60 seconds. You can specify the timeout when creating the ModelClient or FuturesModelClient object:</p> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient, FuturesModelClient\n\ninput1_data = np.random.randn(128, 2)\nwith ModelClient(\"localhost\", \"MyModel\", inference_timeout_s=240) as client:\n    # Raises `PyTritonClientTimeoutError` if the server does not respond to inference request within 240 seconds\n    result_dict = client.infer_batch(input1_data)\n\n\nwith FuturesModelClient(\"localhost\", \"MyModel\", inference_timeout_s=240) as client:\n    future = client.infer_batch(input1_data)\n    ...\n    # Raises `PyTritonClientTimeoutError` if the server does not respond within 240 seconds\n    # from the time `infer_batch` was called by a thread from `ThreadPoolExecutor`\n    result_dict = future.result()\n</code></pre> <p>gRPC client timeout not fully supported</p> <p>There are some missing features in the gRPC client that prevent it from working correctly with timeouts used during the wait for the server and model to be ready. This may cause the client to hang if the server doesn't respond with the current server or model state.</p> <p>Server side timeout not implemented</p> <p>Currently, there is no support for server-side timeout. The server will continue to process the request even if the client timeout is reached.</p>"},{"location":"custom_params/","title":"Custom parameters/headers","text":""},{"location":"custom_params/#custom-httpgrpc-headers-and-parameters","title":"Custom HTTP/gRPC headers and parameters","text":"<p>This document provides guidelines for using custom HTTP/gRPC headers and parameters with PyTriton. Original Triton documentation related to parameters can be found here. Now, undecorated inference function accepts list of Request instances. Request class contains following fields: - data - for inputs (stored as dictionary, but can be also accessed with request dict interface e.g. request[\"input_name\"]) - parameters - for combined parameters and HTTP/gRPC headers</p> <p>Parameters/headers usage limitations</p> <p>Currently, custom parameters and headers can be only accessed in undecorated inference function (they don't work with decorators). There is separate example how to use parameters/headers in preprocessing step (see here)</p>"},{"location":"custom_params/#parameters","title":"Parameters","text":"<p>Parameters are passed to the inference callable as a dictionary. The dictionary is stored in HTTP/gRPC request body payload.</p>"},{"location":"custom_params/#httpgrpc-headers","title":"HTTP/gRPC headers","text":"<p>Custom HTTP/gRPC headers are passed to the inference callable in the same dictionary as parameters, but they are stored in HTTP/gRPC request headers instead of the request body payload. For the headers it is also necessary to specify the header prefix in Triton config, which is used to distinguish  the custom headers from standard ones (only headers with specified prefix are passed to the inference callable).</p>"},{"location":"custom_params/#usage","title":"Usage","text":"<ol> <li>Define inference callable (that one uses one parameter and one header):</li> </ol> <pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\n\ndef _infer_with_params_and_headers(requests):\n    responses = []\n    for req in requests:\n        a_batch, b_batch = req.values()\n        scaled_add_batch = (a_batch + b_batch) / float(req.parameters[\"header_divisor\"])\n        scaled_sub_batch = (a_batch - b_batch) * float(req.parameters[\"parameter_multiplier\"])\n        responses.append({\"scaled_add\": scaled_add_batch, \"scaled_sub\": scaled_sub_batch})\n    return responses\n</code></pre> <ol> <li>Bind inference callable to Triton (\"header\" is the prefix for custom headers):</li> </ol> <pre><code>with Triton(config=TritonConfig(http_header_forward_pattern=\"header.*\")) as triton:\n    triton.bind(\n        model_name=\"ParamsAndHeaders\",\n        infer_func=_infer_with_params_and_headers,\n        inputs=[\n            Tensor(dtype=np.float32, shape=(-1,)),\n            Tensor(dtype=np.float32, shape=(-1,)),\n        ],\n        outputs=[\n            Tensor(name=\"scaled_add\", dtype=np.float32, shape=(-1,)),\n            Tensor(name=\"scaled_sub\", dtype=np.float32, shape=(-1,)),\n        ],\n        config=ModelConfig(max_batch_size=128),\n    )\n\n    triton.serve()\n</code></pre> <ol> <li>Call the model using ModelClient:</li> </ol> <pre><code>import numpy as np\nfrom pytriton.client import ModelClient\n\nbatch_size = 2\na_batch = np.ones((batch_size, 1), dtype=np.float32) * 2\nb_batch = np.ones((batch_size, 1), dtype=np.float32)\n</code></pre> <pre><code>with ModelClient(\"localhost\", \"ParamsAndHeaders\") as client:\n    result_batch = client.infer_batch(a_batch, b_batch, parameters={\"parameter_multiplier\": 2}, headers={\"header_divisor\": 3})\n</code></pre>"},{"location":"decorators/","title":"Decorators","text":""},{"location":"decorators/#decorators","title":"Decorators","text":"<p>The PyTriton provide decorators for operations on input requests to simplify passing the requests to the model inputs. We have prepared several useful decorators for converting generic request input into common user needs. You can create custom decorators tailored to your requirements and chain them with other decorators.</p>"},{"location":"decorators/#batch","title":"Batch","text":"<p>In many cases, it is more convenient to receive input already batched in the form of a NumPy array instead of a list of separate requests. For such cases, we have prepared the <code>@batch</code> decorator that adapts generic input into a batched form. It passes kwargs to the inference function where each named input contains a NumPy array with a batch of requests received by the Triton server.</p> <p>Below, we show the difference between decorated and undecorated functions bound with Triton:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\nfrom pytriton.proxy.types import Request\n\n# Sample input data with 2 requests - each with 2 inputs\ninput_data = [\n     Request({'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])}),\n     Request({'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])})\n]\n\n\ndef undecorated_identity_fn(requests):\n    print(requests)\n    # As expected, requests = [\n    #     Request({'in1': np.array([[1, 1]]), 'in2': np.array([[2, 2]])}),\n    #     Request({'in1': np.array([[1, 2]]), 'in2': np.array([[2, 3]])}),\n    # ]\n    results = requests\n    return results\n\n\n@batch\ndef decorated_identity_fn(in1, in2):\n    print(in1, in2)\n    # in1 = np.array([[1, 1], [1, 2]])\n    # in2 = np.array([[2, 2], [2, 3]])\n    # Inputs are batched by `@batch` decorator and passed to the function as kwargs, so they can be automatically mapped\n    # with in1, in2 function parameters\n    # Of course, we could get these params explicitly with **kwargs like this:\n    # def decorated_infer_fn(**kwargs):\n    return {\"out1\": in1, \"out2\": in2}\n\n\nundecorated_identity_fn(input_data)\ndecorated_identity_fn(input_data)\n</code></pre> <p>More examples using the <code>@batch</code> decorator with different frameworks are shown below.</p> <p>Example implementation for TensorFlow model:</p> <pre><code>import numpy as np\nimport tensorflow as tf\n\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_tf_fn(**inputs: np.ndarray):\n    (images_batch,) = inputs.values()\n    images_batch_tensor = tf.convert_to_tensor(images_batch)\n    output1_batch = model.predict(images_batch_tensor)\n    return [output1_batch]\n</code></pre> <p>Example implementation for PyTorch model:</p> <pre><code>import numpy as np\nimport torch\n\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_pt_fn(**inputs: np.ndarray):\n    (input1_batch,) = inputs.values()\n    input1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\n    output1_batch_tensor = model(input1_batch_tensor)\n    output1_batch = output1_batch_tensor.cpu().detach().numpy()\n    return [output1_batch]\n</code></pre> <p>Example implementation with named inputs and outputs:</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch\n\n\n@batch\ndef add_subtract_fn(a: np.ndarray, b: np.ndarray):\n    return {\"add\": a + b, \"sub\": a - b}\n\n\n@batch\ndef multiply_fn(**inputs: np.ndarray):\n    a = inputs[\"a\"]\n    b = inputs[\"b\"]\n    return [a * b]\n</code></pre> <p>Example implementation with strings:</p> <pre><code>import numpy as np\nfrom transformers import pipeline\nfrom pytriton.decorators import batch\n\nCLASSIFIER = pipeline(\"zero-shot-classification\", model=\"facebook/bart-base\", device=0)\n\n\n@batch\ndef classify_text_fn(text_array: np.ndarray):\n    text = text_array[0]  # text_array contains one string at index 0\n    text = text.decode(\"utf-8\")  # string is stored in byte array encoded in utf-8\n    result = CLASSIFIER(text)\n    return [np.array(result)]  # return statistics generated by classifier\n</code></pre>"},{"location":"decorators/#sample","title":"Sample","text":"<p><code>@sample</code> - takes the first request and converts it into named inputs. This decorator is useful with non-batching models. Instead of a one-element list of requests, we get named inputs - <code>kwargs</code>.</p> <pre><code>from pytriton.decorators import sample\n\n\n@sample\ndef infer_fn(sequence):\n    pass\n</code></pre>"},{"location":"decorators/#group-by-keys","title":"Group by keys","text":"<p><code>@group_by_keys</code> - groups requests with the same set of keys and calls the wrapped function for each group separately. This decorator is convenient to use before batching because the batching decorator requires a consistent set of inputs as it stacks them into batches.</p> <pre><code>from pytriton.decorators import batch, group_by_keys\n\n\n@group_by_keys\n@batch\ndef infer_fn(mandatory_input, optional_input=None):\n    # perform inference\n    pass\n</code></pre>"},{"location":"decorators/#group-by-values","title":"Group by values","text":"<p><code>@group_by_values(*keys)</code> - groups requests with the same input value (for selected keys) and calls the wrapped function for each group separately. This decorator is particularly useful with models requiring dynamic parameters sent by users, such as temperature. In this case, we want to run the model only for requests with the same temperature value.</p> <pre><code>from pytriton.decorators import batch, group_by_values\n\n\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n    # perform inference\n    pass\n</code></pre>"},{"location":"decorators/#fill-optionals","title":"Fill optionals","text":"<p><code>@fill_optionals(**defaults)</code> - fills missing inputs in requests with default values provided by the user. If model owners have default values for some optional parameters, it's a good idea to provide them at the beginning, so other decorators can create larger consistent groups and send them to the inference callable.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, group_by_values\n\n\n@fill_optionals(temperature=np.array([0.7]))\n@batch\n@group_by_values('temperature')\ndef infer_fn(mandatory_input, temperature):\n    # perform inference\n    pass\n</code></pre> <p>``</p>"},{"location":"decorators/#pad-batch","title":"Pad batch","text":"<p><code>@pad_batch</code> - appends the last row to the input multiple times to achieve the desired batch size (preferred batch size or max batch size from the model config, whichever is closer to the current input size).</p> <pre><code>from pytriton.decorators import batch, pad_batch\n\n@batch\n@pad_batch\ndef infer_fn(mandatory_input):\n    # this model requires mandatory_input batch to be the size provided in the model config\n    pass\n</code></pre>"},{"location":"decorators/#first-value","title":"First value","text":"<p><code>@first_value</code> - this decorator takes the first elements from batches for selected inputs specified by the <code>keys</code> parameter. If the value is a one-element array, it is converted to a scalar value. This decorator is convenient to use with dynamic model parameters that users send in requests. You can use <code>@group_by_values</code> before to have batches with the same values in each batch.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_values\n\n@fill_optionals(temperature=np.array([0.7]))\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\ndef infer_fn(mandatory_input, temperature):\n    # perform inference with scalar temperature=10\n    pass\n</code></pre>"},{"location":"decorators/#triton-context","title":"Triton context","text":"<p>The <code>@triton_context</code> decorator provides an additional argument called <code>triton_context</code>, from which you can read the model config.</p> <p>```python  from pytriton.decorators import triton_context</p> <p>@triton_context def infer_fn(input_list, **kwargs):     model_config = kwargs['triton_context'].model_config     # perform inference using some information from model_config     pass  ```</p>"},{"location":"decorators/#stacking-multiple-decorators","title":"Stacking multiple decorators","text":"<p>Here is an example of stacking multiple decorators together. We recommend starting with type 1 decorators, followed by types 2 and 3. Place the <code>@triton_context</code> decorator last in the chain.</p> <pre><code>import numpy as np\nfrom pytriton.decorators import batch, fill_optionals, first_value, group_by_keys, group_by_values, triton_context\n\n\n@fill_optionals(temperature=np.array([0.7]))\n@group_by_keys\n@batch\n@group_by_values('temperature')\n@first_value('temperature')\n@triton_context\ndef infer(triton_context, mandatory_input, temperature, opt1=None, opt2=None):\n    model_config = triton_context.model_config\n    # perform inference using:\n    #   - some information from model_config\n    #   - scalar temperature value\n    #   - optional parameters opt1 and/or opt2\n</code></pre>"},{"location":"deploying_in_clusters/","title":"Deploying in Clusters","text":""},{"location":"deploying_in_clusters/#deploying-in-cluster","title":"Deploying in Cluster","text":"<p>The library can be used inside containers and deployed on Kubernetes clusters. There are certain prerequisites and information that would help deploy the library in your cluster.</p>"},{"location":"deploying_in_clusters/#health-checks","title":"Health checks","text":"<p>The library uses the Triton Inference Server to handle HTTP/gRPC requests. Triton Server provides endpoints to validate if the server is ready and in a healthy state. The following API endpoints can be used in your orchestrator to control the application ready and live states:</p> <ul> <li>Ready: <code>/v2/health/ready</code></li> <li>Live: <code>/v2/health/live</code></li> </ul>"},{"location":"deploying_in_clusters/#exposing-ports","title":"Exposing ports","text":"<p>The library uses the Triton Inference Server, which exposes the HTTP, gRPC, and metrics ports for communication. In the default configuration, the following ports have to be exposed:</p> <ul> <li>8000 for HTTP</li> <li>8001 for gRPC</li> <li>8002 for metrics</li> </ul> <p>If the library is inside a Docker container, the ports can be exposed by passing an extra argument to the <code>docker run</code> command. An example of passing ports configuration:</p> <pre><code>docker run -p 8000:8000 -p 8001:8001 -p 8002:8002 {image}\n</code></pre> <p>To deploy a container in Kubernetes, add a ports definition for the container in YAML deployment configuration:</p> <pre><code>containers:\n  - name: pytriton\n    ...\n    ports:\n      - containerPort: 8000\n        name: http\n      - containerPort: 8001\n        name: grpc\n      - containerPort: 8002\n        name: metrics\n</code></pre>"},{"location":"deploying_in_clusters/#configuring-shared-memory","title":"Configuring shared memory","text":"<p>The connection between Python callbacks and the Triton Inference Server uses shared memory to pass data between the processes. In the Docker container, the default amount of shared memory is <code>64MB</code>, which may not be enough to pass input and output data of the model. The PyTriton initialize <code>16MB</code> of shared memory for <code>Proxy Backend</code> at start to pass input/output tensors between processes. The additional memory is allocated dynamically. In case of failure, the size of available shared memory might need to be increased.</p> <p>To increase the available shared memory size, pass an additional flag to the <code>docker run</code> command. An example of increasing the shared memory size to 8GB:</p> <p><pre><code>docker run --shm-size 8GB {image}\n</code></pre> To increase the shared memory size for Kubernetes, the following configuration can be used:</p> <pre><code>spec:\n  volumes:\n    - name: shared-memory\n      emptyDir:\n        medium: Memory\n  containers:\n    - name: pytriton\n      ...\n      volumeMounts:\n        - mountPath: /dev/shm\n          name: shared-memory\n</code></pre>"},{"location":"deploying_in_clusters/#specify-container-init-process","title":"Specify container init process","text":"<p>You can use the <code>--init</code> flag of the <code>docker run</code> command to indicate that an init process should be used as the PID 1 in the container. Specifying an init process ensures that reaping zombie processes are performed inside the container. The reaping zombie processes functionality is important in case of an unexpected error occurrence in scripts hosting PyTriton.</p>"},{"location":"downloaded_input_data/","title":"Example with downloaded input data","text":""},{"location":"downloaded_input_data/#example-with-downloaded-input-data","title":"Example with downloaded input data","text":"<p>In the following example, we will demonstrate how to effectively utilize PyTriton with downloaded input data. While the model itself does not possess any inputs, it utilize custom parameters or headers to extract a URL and download data from an external source, such as an S3 bucket.</p> <p>The corresponding function can leverage the batch decorator since it does not rely on any parameters or headers.</p>"},{"location":"downloaded_input_data/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton, TritonConfig\n\n@batch\ndef model_infer_function(**inputs):\n    ...\n\ndef request_infer_function(requests):\n    for request in requests:\n        image_url = request.parameters[\"custom_url\"]\n        image_jpeg = download(image_url)\n        image_data = decompress(image_jpeg)\n        request['images_data'] = image_data\n    outputs = model_infer_function(requests)\n    return outputs\n\nwith Triton(config=TritonConfig(http_header_forward_pattern=\"custom.*\")) as triton:\n    triton.bind(\n        model_name=\"ImgModel\",\n        infer_func=request_infer_function,\n        inputs=[],\n        outputs=[Tensor(name=\"out\", dtype=np.float32, shape=(-1,))],\n        config=ModelConfig(max_batch_size=128),\n    )\n    triton.serve()\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide simple examples on how to integrate PyTorch, TensorFlow2, JAX, and simple Python models with the Triton Inference Server using PyTriton. The examples are available in the GitHub repository.</p>"},{"location":"examples/#samples-models-deployment","title":"Samples Models Deployment","text":"<p>The list of example models deployments:</p> <ul> <li>Add-Sub Python model</li> <li>Add-Sub Python model Jupyter Notebook</li> <li>BART PyTorch from HuggingFace</li> <li>BERT JAX from HuggingFace</li> <li>Identity Python model</li> <li>Linear RAPIDS/CuPy model</li> <li>Linear RAPIDS/CuPy model Jupyter Notebook</li> <li>Linear PyTorch model</li> <li>Multi-Layer TensorFlow2</li> <li>Multi Instance deployment for ResNet50 PyTorch model</li> <li>Multi Model deployment for Python models</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> <li>Stable Diffusion 1.5 from HuggingFace</li> <li>Using custom HTTP/gRPC headers and parameters</li> </ul>"},{"location":"examples/#profiling-models","title":"Profiling models","text":"<p>The Perf Analyzer can be used to profile the models served through PyTriton. We have prepared an example of using Perf Analyzer to profile BART PyTorch. See the example code in the GitHub repository.</p>"},{"location":"examples/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>The following examples contain a guide on how to deploy them on a Kubernetes cluster:</p> <ul> <li>BART PyTorch from HuggingFace</li> <li>OPT JAX from HuggingFace with multi-node support</li> <li>NeMo Megatron GPT model with multi-node support</li> <li>ResNet50 PyTorch from HuggingFace</li> <li>Stable Diffusion 1.5 from HuggingFace</li> </ul>"},{"location":"inference_callable/","title":"Inference Callable","text":""},{"location":"inference_callable/#inference-callable","title":"Inference Callable","text":"<p>This document provides guidelines for creating an inference callable for PyTriton, which serves as the entry point for handling inference requests.</p> <p>The inference callable is an entry point for handling inference requests. The interface of the inference callable assumes it receives a list of requests with input dictionaries, where each dictionary represents one request mapping model input names to NumPy ndarrays. Requests contain also custom HTTP/gRPC headers and parameters in parameters dictionary.</p>"},{"location":"inference_callable/#function","title":"Function","text":"<p>The simples inference callable is a function that implement the interface to handle request and responses. Request class contains following fields: - data - for inputs (stored as dictionary, but can be also accessed with request dict interface e.g. request[\"input_name\"]) - parameters - for combined parameters and HTTP/gRPC headers For more information about parameters and headers see here.</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nfrom pytriton.proxy.types import Request\n\ndef infer_fn(requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n    ...\n</code></pre>"},{"location":"inference_callable/#class","title":"Class","text":"<p>In many cases is worth to use an object of given class as callable. This is especially useful when you want to have a control over the order of initialized objects or models.</p> <pre><code>import numpy as np\nfrom typing import Dict, List\nfrom pytriton.proxy.types import Request\n\nclass InferCallable:\n\n    def __call__(self, requests: List[Request]) -&gt; List[Dict[str, np.ndarray]]:\n       ...\n</code></pre>"},{"location":"inference_callable/#binding-to-triton","title":"Binding to Triton","text":"<p>To use the inference callable with PyTriton, it must be bound to a Triton server instance using the <code>bind</code> method:</p> <pre><code>import numpy as np\nfrom pytriton.triton import Triton\nfrom pytriton.model_config import ModelConfig, Tensor\n\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"MyInferenceFn\",\n        infer_func=infer_fn,\n        inputs=[Tensor(shape=(1,), dtype=np.float32)],\n        outputs=[Tensor(shape=(1,), dtype=np.float32)],\n        config=ModelConfig(max_batch_size=8)\n    )\n\n    infer_callable = InferCallable()\n    triton.bind(\n        model_name=\"MyInferenceCallable\",\n        infer_func=infer_callable,\n        inputs=[Tensor(shape=(1,), dtype=np.float32)],\n        outputs=[Tensor(shape=(1,), dtype=np.float32)],\n        config=ModelConfig(max_batch_size=8)\n    )\n</code></pre> <p>For more information on serving the inference callable, refer to the Loading models section on Deploying Models page.</p>"},{"location":"initialization/","title":"Triton Initialization","text":""},{"location":"initialization/#initialization","title":"Initialization","text":"<p>The following page provides more details about possible options for configuring the Triton Inference Server and working with block and non-blocking mode for tests and deployment.</p>"},{"location":"initialization/#configuring-triton","title":"Configuring Triton","text":"<p>Connecting Python models with Triton Inference Server working in the current environment requires creating a Triton object. This can be done by creating a context:</p> <pre><code>from pytriton.triton import Triton\n\nwith Triton() as triton:\n    ...\n</code></pre> <p>or simply creating an object:</p> <pre><code>from pytriton.triton import Triton\n\ntriton = Triton()\n</code></pre> <p>The Triton Inference Server behavior can be configured by passing config parameter:</p> <pre><code>import pathlib\nfrom pytriton.triton import Triton, TritonConfig\n\ntriton_config = TritonConfig(log_file=pathlib.Path(\"/tmp/triton.log\"))\nwith Triton(config=triton_config) as triton:\n    ...\n</code></pre> <p>and through environment variables, for example, set as in the command below:</p> <pre><code>PYTRITON_TRITON_CONFIG_LOG_VERBOSITY=4 python my_script.py\n</code></pre> <p>The order of precedence of configuration methods is:</p> <ul> <li>config defined through <code>config</code> parameter of Triton class <code>__init__</code> method</li> <li>config defined in environment variables</li> <li>default TritonConfig values</li> </ul>"},{"location":"initialization/#blocking-mode","title":"Blocking mode","text":"<p>The blocking mode will stop the execution of the current thread and wait for incoming HTTP/gRPC requests for inference execution. This mode makes your application behave as a pure server. The example of using blocking mode:</p> <pre><code>from pytriton.triton import Triton\n\nwith Triton() as triton:\n    ...  # Load models here\n    triton.serve()\n</code></pre>"},{"location":"initialization/#background-mode","title":"Background mode","text":"<p>The background mode runs Triton as a subprocess and does not block the execution of the current thread. In this mode, you can run Triton Inference Server and interact with it from the current context. The example of using background mode:</p> <pre><code>from pytriton.triton import Triton\n\ntriton = Triton()\n...  # Load models here\ntriton.run()  # Triton Server started\nprint(\"This print will appear\")\ntriton.stop()  # Triton Server stopped\n</code></pre>"},{"location":"initialization/#filesystem-usage","title":"Filesystem usage","text":"<p>PyTriton needs to access the filesystem for two purposes:</p> <ul> <li>to communicate with the Triton backend using file sockets,</li> <li>storing copy of Triton backend and its binary dependencies.</li> </ul> <p>PyTriton creates temporary folders called Workspaces, where it stores the file descriptors for these operations. By default, these folders are located in <code>$HOME/.cache/pytriton</code> directory. However, you can change this location by setting the <code>PYTRITON_HOME</code> environment variable.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This page explains how to install the library. We assume that you have a basic understanding of the Python programming language and are familiar with machine learning models. Using Docker is optional but not required.</p> <p>You should be comfortable with the Python programming language and know how to work with Machine Learning models. Using Docker is optional and not necessary.</p> <p>The library can be installed in any of the following ways:</p> <ul> <li>system environment</li> <li>virtualenv</li> <li>Docker image</li> </ul> <p>If you opt for using Docker, you can get NVIDIA optimized Docker images for Python frameworks from the NVIDIA NGC Catalog.</p> <p>To run model inference on NVIDIA GPU using the Docker runtime, we recommend that you install the NVIDIA Container Toolkit, which enables GPU acceleration for containers.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the library, ensure that you meet the following requirements:</p> <ul> <li>An operating system with glibc &gt;= <code>2.35</code>. Triton Inference Server and PyTriton have only been rigorously tested on Ubuntu 22.04.   Other supported operating systems include Ubuntu Debian 11+, Rocky Linux 9+, and Red Hat Universal Base Image 9+.</li> <li>to check your glibc version, run <code>ldd --version</code></li> <li>Python version &gt;= <code>3.8</code></li> <li><code>pip &gt;=</code>20.3`</li> <li><code>libpython3.*.so</code> available in the operating system (appropriate for Python version).</li> </ul>"},{"location":"installation/#install-from-pypi","title":"Install from <code>pypi</code>","text":"<p>You can install the package from pypi.org by running the following command:</p> <pre><code>pip install -U nvidia-pytriton\n</code></pre> <p>Triton Inference Server binaries</p> <p>The Triton Inference Server binaries are installed as part of the PyTriton package.</p>"},{"location":"installation/#setting-up-python-environment","title":"Setting Up Python Environment","text":"<p>The Triton Inference Server is automatically run with your Python interpreter version. To use Triton binary you need to make sure that <code>libpython3.*.so</code> library can be linked during PyTriton start. Install and provide location to <code>libpython3.*.so</code> library in LD_LIBRARY_PATH before you will run PyTriton. Below we presented some options on how to prepare your Python environment to run PyTriton with common tools.</p>"},{"location":"installation/#upgrading-pip-version","title":"Upgrading <code>pip</code> version","text":"<p>You need to have <code>pip</code> version 20.3 or higher. To upgrade an older version of pip, run this command:</p> <pre><code>pip install -U pip\n</code></pre>"},{"location":"installation/#using-system-interpreter","title":"Using system interpreter","text":"<p>When you are running PyTriton on Ubuntu 22.04 install the desired Python interpreter and <code>libpython3*so.</code> library. <pre><code># Install necessary packages\napt update -y\napt install -y software-properties-common\n\n# Add repository with various Python versions\nadd-apt-repository ppa:deadsnakes/ppa -y\n\n# Install Python 3.8\napt install -y python3.8 libpython3.8 python3.8-distutils python3-pip \\\n     build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev \\\n     libffi-dev curl libbz2-dev pkg-config make\n\n# Install library for interpreter\npython3.8 -m pip install nvidia-pytriton\n</code></pre></p>"},{"location":"installation/#creating-virtualenv-using-pyenv","title":"Creating virtualenv using <code>pyenv</code>","text":"<p>In order to install different version replace the <code>3.8</code> with desired Python version in the example below:</p> <pre><code># Install necessary packages\napt update -y\napt install -y python3 python3-distutils python-is-python3 git \\\n    build-essential libssl-dev zlib1g-dev \\\n    libbz2-dev libreadline-dev libsqlite3-dev curl \\\n    libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev\n\n# Install pyenv\ncurl https://pyenv.run | bash\n\n# Configure pyenv in current environment\nexport PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\neval \"$(pyenv virtualenv-init -)\"\n\n# Install Python 3.8 with shared library support\nenv PYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install 3.8\n\n# Create and activate virtualenv\npyenv virtualenv 3.8 venv\npyenv activate venv\n\n# export the library path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pyenv virtualenv-prefix)/lib\n\n# Install library for interpreter\npip install nvidia-pytriton\n</code></pre>"},{"location":"installation/#creating-virtualenv-using-venv","title":"Creating virtualenv using <code>venv</code>","text":"<p>In order to install different version replace the <code>3.8</code> with desired Python version in the example below:</p> <pre><code># Install necessary packages\napt update -y\napt install -y software-properties-common\n\n# Add repository with various Python versions\nadd-apt-repository ppa:deadsnakes/ppa -y\n\n# Install Python 3.8\napt install -y python3.8 libpython3.8 python3.8-distutils python3.8-venv python3.8-pip python-is-python3 \\\n     build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev \\\n     libffi-dev curl libbz2-dev pkg-config make\n\n# Create and activate virtualenv\npython3.8 -m venv /opt/venv\nsource /opt/venv/bin/activate\n\n# Install library for interpreter\npip install nvidia-pytriton\n</code></pre>"},{"location":"installation/#creating-virtualenv-using-miniconda","title":"Creating virtualenv using <code>miniconda</code>","text":"<p>In order to install different version replace the <code>3.8</code> with desired Python version in the example below:</p> <pre><code># Install necessary packages\napt update -y\napt install -y python3 python3-distutils python-is-python3 curl\n\n# Download miniconda\nCONDA_VERSION=latest\nTARGET_MACHINE=x86_64\ncurl \"https://repo.anaconda.com/miniconda/Miniconda3-${CONDA_VERSION}-Linux-${TARGET_MACHINE}.sh\" --output miniconda.sh\n\n# Install miniconda and add to PATH\nbash miniconda.sh\nexport PATH=~/miniconda3/bin/:$PATH\n\n# Initialize bash\nconda init bash\nbash\n\n# Create and activate virtualenv\nconda create -c conda-forge -n venv python=3.8\nconda activate venv\n\n# Export the library path\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib\n\n# Install library for interpreter\npip install nvidia-pytriton\n</code></pre>"},{"location":"installation/#building-binaries-from-source","title":"Building binaries from source","text":"<p>The binary package can be built from the source, allowing access to unreleased hotfixes, the ability to modify the PyTriton code, and compatibility with various Triton Inference Server versions, including custom server builds. For further information on building the PyTriton binary, refer to the Building page.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>There is no one-to-one match between our solution and Triton Inference Server features, especially in terms of supporting a user model store.</li> <li>Support is currently limited to the x86-64 instruction set architecture.</li> <li>Running multiple scripts hosting PyTriton on the same machine or container is not feasible.</li> <li>Deadlocks may occur in some models when employing the NCCL communication library and multiple Inference Callables are triggered concurrently. This issue can be observed when deploying multiple instances of the same model or multiple models within a single server script. Additional information about this issue can be found here.</li> <li>Enabling verbose logging may cause a significant performance drop in model inference.</li> <li>Creation of Triton object leaks single intance of ModelClient object. This is a known issue and will be fixed in future releases. This may cause gevent warnings to be printed to the console.</li> <li>GRPC ModelClient doesn't support timeouts for model configuration and model metadata requests due to a limitation in the underlying tritonclient library.</li> </ul>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>The prerequisite for this page is to install PyTriton, which can be found in the installation page.</p> <p>The Quick Start presents how to run a Python model in the Triton Inference Server without needing to change the current working environment. In this example, we are using a simple <code>Linear</code> PyTorch model.</p> <p>The integration of the model requires providing the following elements:</p> <ul> <li>The model - a framework or Python model or function that handles inference requests</li> <li>Inference Callable - function or class with <code>__call__</code> method, that handles the input data coming from Triton and returns the result</li> <li>Python function connection with Triton Inference Server - a binding for communication between Triton and the Inference Callable</li> </ul> <p>The requirement for the example is to have PyTorch installed in your environment. You can do this by running:</p> <pre><code>pip install torch\n</code></pre> <p>In the next step, define the <code>Linear</code> model:</p> <pre><code>import torch\n\nmodel = torch.nn.Linear(2, 3).to(\"cuda\").eval()\n</code></pre> <p>In the second step, create an inference callable as a function. The function obtains the HTTP/gRPC request data as an argument, which should be in the form of a NumPy array. The expected return object should also be a NumPy array. You can define an inference callable as a function that uses the <code>@batch</code> decorator from PyTriton. This decorator converts the input request into a more suitable format that can be directly passed to the model. You can read more about decorators here.</p> <p>Example implementation:</p> <pre><code>import numpy as np\nimport torch\n\nfrom pytriton.decorators import batch\n\n\n@batch\ndef infer_fn(**inputs: np.ndarray):\n    (input1_batch,) = inputs.values()\n    input1_batch_tensor = torch.from_numpy(input1_batch).to(\"cuda\")\n    output1_batch_tensor = model(input1_batch_tensor) # Calling the Python model inference\n    output1_batch = output1_batch_tensor.cpu().detach().numpy()\n    return [output1_batch]\n</code></pre> <p>In the next step, you can create the binding between the inference callable and Triton Inference Server using the <code>bind</code> method from PyTriton. This method takes the model name, the inference callable, the inputs and outputs tensors, and an optional model configuration object.</p> <pre><code>from pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import Triton\n\n# Connecting inference callable with Triton Inference Server\nwith Triton() as triton:\n    triton.bind(\n        model_name=\"Linear\",\n        infer_func=infer_fn,\n        inputs=[\n            Tensor(dtype=np.float32, shape=(-1,)),\n        ],\n        outputs=[\n            Tensor(dtype=np.float32, shape=(-1,)),\n        ],\n        config=ModelConfig(max_batch_size=128)\n    )\n    ...\n</code></pre> <p>Finally, serve the model with the Triton Inference Server:</p> <pre><code>from pytriton.triton import Triton\n\nwith Triton() as triton:\n    ...  # Load models here\n    triton.serve()\n</code></pre> <p>The <code>bind</code> method creates a connection between the Triton Inference Server and the <code>infer_fn</code>, which handles the inference queries. The <code>inputs</code> and <code>outputs</code> describe the model inputs and outputs that are exposed in Triton. The config field allows more parameters for model deployment.</p> <p>The <code>serve</code> method is blocking, and at this point, the application waits for incoming HTTP/gRPC requests. From that moment, the model is available under the name <code>Linear</code> in the Triton server. The inference queries can be sent to <code>localhost:8000/v2/models/Linear/infer</code>, which are passed to the <code>infer_fn</code> function.</p> <p>If you would like to use Triton in the background mode, use <code>run</code>. More about that can be found in the Deploying Models page.</p> <p>Once the <code>serve</code> or <code>run</code> method is called on the <code>Triton</code> object, the server status can be obtained using:</p> <pre><code>curl -v localhost:8000/v2/health/live\n</code></pre> <p>The model is loaded right after the server starts, and its status can be queried using:</p> <pre><code>curl -v localhost:8000/v2/models/Linear/ready\n</code></pre> <p>Finally, you can send an inference query to the model:</p> <pre><code>curl -X POST \\\n  -H \"Content-Type: application/json\"  \\\n  -d @input.json \\\n  localhost:8000/v2/models/Linear/infer\n</code></pre> <p>The <code>input.json</code> with sample query:</p> <pre><code>{\n  \"id\": \"0\",\n  \"inputs\": [\n    {\n      \"name\": \"INPUT_1\",\n      \"shape\": [1, 2],\n      \"datatype\": \"FP32\",\n      \"parameters\": {},\n      \"data\": [[-0.04281254857778549, 0.6738349795341492]]\n    }\n  ]\n}\n</code></pre> <p>Read more about the HTTP/gRPC interface in the Triton Inference Server documentation.</p> <p>You can also validate the deployed model using a simple client that can perform inference requests:</p> <pre><code>import torch\nfrom pytriton.client import ModelClient\n\ninput1_data = torch.randn(128, 2).cpu().detach().numpy()\n\nwith ModelClient(\"localhost:8000\", \"Linear\") as client:\n    result_dict = client.infer_batch(input1_data)\n\nprint(result_dict)\n</code></pre> <p>The full example code can be found in examples/linear_random_pytorch.</p> <p>More information about running the server and models can be found in Deploying Models page.</p>"},{"location":"remote_triton/","title":"Triton Remote Mode","text":""},{"location":"remote_triton/#pytriton-remote-mode","title":"PyTriton remote mode","text":"<p>Remote mode is a way to use the PyTriton with the Triton Inference Server running remotely (at this moment it must be deployed on the same machine, but may be launched in a different container).</p> <p>To bind the model in remote mode, it is required to use the <code>RemoteTriton</code> class instead of <code>Triton</code>. Only difference of using <code>RemoteTriton</code> is that it requires the triton <code>url</code> argument in the constructor.</p>"},{"location":"remote_triton/#example-of-binding-a-model-in-remote-mode","title":"Example of binding a model in remote mode","text":"<p>Example below assumes that the Triton Inference Server is running on the same machine (launched with PyTriton in separate python script).</p> <p>RemoteTriton binds remote model to existing Triton Inference Server.</p> <pre><code>import numpy as np\n\nfrom pytriton.decorators import batch\nfrom pytriton.model_config import ModelConfig, Tensor\nfrom pytriton.triton import RemoteTriton, TritonConfig\n\ntriton_config = TritonConfig(\n    cache_config=[f\"local,size={1024 * 1024}\"],  # 1MB\n)\n\n@batch\ndef _add_sub(**inputs):\n    a_batch, b_batch = inputs.values()\n    add_batch = a_batch + b_batch\n    sub_batch = a_batch - b_batch\n    return {\"add\": add_batch, \"sub\": sub_batch}\n\nwith RemoteTriton(url='localhost') as triton:\n    triton.bind(\n        model_name=\"AddSub\",\n        infer_func=_add_sub,\n        inputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        outputs=[Tensor(shape=(1,), dtype=np.float32), Tensor(shape=(1,), dtype=np.float32)],\n        config=ModelConfig(max_batch_size=8, response_cache=True)\n    )\n    triton.serve()\n</code></pre>"}]}